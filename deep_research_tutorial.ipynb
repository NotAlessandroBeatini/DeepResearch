{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c524bb10",
   "metadata": {},
   "source": [
    "# scratchpad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f68e91",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Define the Agent's State\n",
    "This is the central \"memory\" of our agent. All information the agent needs to operate will be stored here. In LangGraph, the state is mutable, meaning it can be modified by any node in the graph, and it's passed from one node to the next.\n",
    "\n",
    "Why we use it: The state allows us to build a stateful system instead of a simple, stateless chain of events. A stateless system would have to re-evaluate the entire problem from scratch at every step. By passing a state object, we can maintain context and accumulate information over multiple steps, which is crucial for a complex task like research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1dd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our multi-step research agent.\n",
    "    This is basically a schema\n",
    "\n",
    "    Attributes:\n",
    "        query: The user's original research query.\n",
    "        research_plan: A list of search queries or sub-tasks to execute.\n",
    "        research_results: A list of raw results gathered from the tools.\n",
    "        report: The final, synthesized report.\n",
    "    \"\"\"\n",
    "    query: str\n",
    "    research_plan: List[str]\n",
    "    research_results: List[str]\n",
    "    report: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f08ae",
   "metadata": {},
   "source": [
    "## Step 3: Implement the Tools\n",
    "Tools are the \"hands\" of our agent. They are functions that the agent's LLM can call to interact with external systems, such as search engines, databases, or other APIs.\n",
    "\n",
    "Why we use it: Agents, by themselves, are limited to the knowledge they were trained on. By giving them access to tools, we enable them to overcome this limitation, allowing them to perform real-time searches, access up-to-date information, and perform complex calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ce4bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool name: duckduckgo_search\n",
      "tool description: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "tool arg schema: <class 'langchain_community.tools.ddg_search.tool.DDGInput'>\n",
      "{'query': FieldInfo(annotation=str, required=True, description='search query to look up')}\n",
      "########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_695944/1189855989.py:14: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result = search.invoke(search_input.dict())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prime minister of Italy , officially the president of the Council of Ministers , is the head of government of the Italian Republic. The office of president of the Council of Ministers is established by articles 92–96 of the Constitution of Italy ... Giorgia Meloni is an Italian politician who has served as Prime Minister of Italy since 2022. She is the first woman to hold the office. A member of the Chamber of Deputies since 2006, she has been president of the right-wing to far-right Brothers of ... Prime Minister Giorgia Meloni and European MP Alessandra Moretti were among the high-profile Italian women whose images were subject to lewd and violent content. Prime Minister Giorgia Meloni during the Rimini Meeting 2025 in Rimini, northeast Italy on August 27, 2025. Italian Prime Minister Georgia Meloni, pictured in March, has voiced outrage over a porn site that allegedly featured doctored images of herself and other high-profile women. ITALIAN PRIME MINISTER Giorgia Meloni has paid an unofficial surprise visit to US President-elect Donald Trump at his Mar-a-Lago Florida residence.Trump told the crowd: “This is very exciting.” “I’m here with a fantastic woman, the prime minister of Italy .\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()\n",
    "print(f\"tool name: {search.name}\")\n",
    "print(f\"tool description: {search.description}\")\n",
    "print(f\"tool arg schema: {search.args_schema}\")\n",
    "\n",
    "\n",
    "from langchain_community.tools.ddg_search.tool import DDGInput\n",
    "print(DDGInput.model_fields)\n",
    "print(\"#\"*40)\n",
    "\n",
    "query = \"Who is the prime minister of Italy?\"\n",
    "search_input = DDGInput(query=query)\n",
    "result = search.invoke(search_input.dict())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be4ad64",
   "metadata": {},
   "source": [
    "# Deep research !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0adc5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment is set up and keys are loaded.\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Optional: Configure LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Deep Research Notebook\"\n",
    "\n",
    "print(\"Environment is set up and keys are loaded.\")\n",
    "print(os.getenv(\"TAVILY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89c4f0",
   "metadata": {},
   "source": [
    "## we need to define tools and state of the agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b5503",
   "metadata": {},
   "source": [
    "### TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tavily tool schema: {\n",
      "  \"description\": \"Input for the Tavily tool.\",\n",
      "  \"properties\": {\n",
      "    \"query\": {\n",
      "      \"description\": \"search query to look up\",\n",
      "      \"title\": \"Query\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"query\"\n",
      "  ],\n",
      "  \"title\": \"TavilyInput\",\n",
      "  \"type\": \"object\"\n",
      "}\n",
      "\n",
      "##############################\n",
      "arxiv tool schema: {\n",
      "  \"properties\": {\n",
      "    \"query\": {\n",
      "      \"description\": \"The search query for scientific papers on ArXiv.\",\n",
      "      \"title\": \"Query\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"query\"\n",
      "  ],\n",
      "  \"title\": \"ArxivSearchInput\",\n",
      "  \"type\": \"object\"\n",
      "}\n",
      "\n",
      "wikipedia tool schema: {\n",
      "  \"properties\": {\n",
      "    \"query\": {\n",
      "      \"description\": \"The search query for general knowledge on Wikipedia.\",\n",
      "      \"title\": \"Query\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"query\"\n",
      "  ],\n",
      "  \"title\": \"WikipediaSearchInput\",\n",
      "  \"type\": \"object\"\n",
      "}\n",
      "\n",
      "Tools created: ['tavily_search_results_json', 'arxiv_search', 'wikipedia_search']\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List, Optional # for state types\n",
    "from typing_extensions import Annotated # for reducers\n",
    "from langgraph.graph import add_messages #reducer\n",
    "import operator\n",
    "from langchain.tools import Tool #base class that will use to wrap the utilies\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults #already a tool\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain_core.tools import StructuredTool\n",
    "from langchain_core.messages import ToolMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "#already subclass BaseModel and provide args_schema\n",
    "tavily_tool = TavilySearchResults(max_results = 5)\n",
    "print(f\"tavily tool schema: {json.dumps(tavily_tool.args_schema.model_json_schema(), indent=2)}\",end=\"\\n\\n\")\n",
    "print(\"#\"*30)\n",
    "\n",
    "#--------CUSTOM TOOLS-----------#\n",
    "#define schema for the wrapper tools.\n",
    "class ArxivSearchInput(BaseModel):\n",
    "    query: str = Field(description=\"The search query for scientific papers on ArXiv.\") #since i have not provided default, it treats it as required\n",
    "\n",
    "# define the function WITH a config parameter\n",
    "def arxiv_search_fn(query: str, config: RunnableConfig) -> str:\n",
    "    # Read runtime knobs that you DON'T want the LLM to fill\n",
    "    cfg = (config or {}).get(\"configurable\", {})\n",
    "    top_k: int = int(cfg.get(\"arxiv_top_k\", 3))\n",
    "    date_from: Optional[str] = cfg.get(\"arxiv_date_from\")  # example extra knob\n",
    "\n",
    "    wrapper = ArxivAPIWrapper(top_k_results=top_k, date_from=date_from)\n",
    "    \n",
    "    return wrapper.run(query)\n",
    "\n",
    "arxiv_tool = StructuredTool.from_function(\n",
    "    name=\"arxiv_search\",\n",
    "    description=\"Search ArXiv for scientific papers.\",\n",
    "    args_schema=ArxivSearchInput,\n",
    "    func=arxiv_search_fn\n",
    ")\n",
    "\n",
    "print(f\"arxiv tool schema: {json.dumps(arxiv_tool.args_schema.model_json_schema(),indent=2)}\",end=\"\\n\\n\")\n",
    "\n",
    "class WikipediaSearchInput(BaseModel):\n",
    "    query: str = Field(description=\"The search query for general knowledge on Wikipedia.\")\n",
    "\n",
    "\n",
    "def wikipedia_search_fn(query: str, config: RunnableConfig) -> str:\n",
    "    cfg = (config or {}).get(\"configurable\", {})\n",
    "    top_k: int = int(cfg.get(\"wikipedia_top_k\", 3))\n",
    "    lang: Optional[str] = cfg.get(\"wikipedia_lang\",\"en\")\n",
    "    wrapper = WikipediaAPIWrapper(top_k_results=top_k,lang=lang)\n",
    "\n",
    "    return wrapper.run(query)\n",
    "\n",
    "wikipedia_tool = StructuredTool.from_function(\n",
    "    name=\"wikipedia_search\",\n",
    "    description=\"Search Wikipedia for general knowledge.\",\n",
    "    args_schema=WikipediaSearchInput,\n",
    "    func=wikipedia_search_fn\n",
    ")\n",
    "\n",
    "print(f\"wikipedia tool schema: {json.dumps(wikipedia_tool.args_schema.model_json_schema(),indent=2)}\",end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "research_tools = [tavily_tool, arxiv_tool, wikipedia_tool]\n",
    "print(f\"Tools created: {[tool.name for tool in research_tools]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "376f145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "dict_keys(['title', 'url', 'content', 'score'])\n",
      "Follow the latest news on the Israel Gaza war. Get live updates, maps, verified video reports and expert analysis from BBC journalists on the ground.\n",
      "\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_vde9wqnqd7v4bDRTfpRIkKdp', 'function': {'arguments': '{\"query\":\"Prime Minister of Italy\"}', 'name': 'wikipedia_search'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 61, 'total_tokens': 78, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'id': 'chatcmpl-CB3Szo2m8JBh0Kg96kMlAhh89Gyty', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--66fdda58-0422-4f45-8ccf-36506360655a-0' tool_calls=[{'name': 'wikipedia_search', 'args': {'query': 'Prime Minister of Italy'}, 'id': 'call_vde9wqnqd7v4bDRTfpRIkKdp', 'type': 'tool_call'}] usage_metadata={'input_tokens': 61, 'output_tokens': 17, 'total_tokens': 78, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "wiki_res TOOL CALL: [{'name': 'wikipedia_search', 'args': {'query': 'Prime Minister of Italy'}, 'id': 'call_vde9wqnqd7v4bDRTfpRIkKdp', 'type': 'tool_call'}]\n",
      "\n",
      "--- Executing Tool: wikipedia_search ---\n",
      "\n",
      "--- Tool Output (first 200 chars): ---\n",
      "Page: Prime Minister of Italy\n",
      "Summary: The prime minister of Italy,  officially the president of the Council of Ministers (Italian: Presidente del Consiglio dei ministri), is the head of government of...\n",
      "Page: Presidente del Consiglio dei ministri della Repubblica Italiana\n",
      "Summary: Il presidente del Consiglio dei ministri in Italia è l'organo di vertice politico-amministrativo della presidenza del Con\n",
      "\n",
      "\n",
      "Published: 2024-09-05\n",
      "Title: Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation\n",
      "Authors: Yu Wang, Shiwan Zhao, Zhihu Wang, Heyuan Huang, Ming Fan, Yubo Zhang, Zhixing Wang, Haijun Wang, Ting Liu\n",
      "Summary: The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for\n",
      "enhancing the reasoning capabilities of large language models (LLMs). However,\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "# ------ TAVILY -------\n",
    "#invocation test\n",
    "tavily_result = tavily_tool.invoke({\"query\": \"Is Israel currently at war?\"}) #invoke expects dict matching schema\n",
    "print(len(tavily_result)) #list of dicts\n",
    "print(tavily_result[0].keys())\n",
    "print(tavily_result[0][\"content\"][:200],end=\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "# ---- WIKPEDIA -----\n",
    "\n",
    "#custom tooling can be either used in this way:\n",
    "llm = llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_completion_tokens=256)\n",
    "llm_with_tools = llm.bind_tools([wikipedia_tool])\n",
    "wiki_res = llm_with_tools.invoke(\"Who is the Prime Minister of Italy?\")\n",
    "print(wiki_res)\n",
    "print(f\"wiki_res TOOL CALL: {wiki_res.tool_calls}\")\n",
    "tool_calls = wiki_res.tool_calls\n",
    "tool_outputs = []\n",
    "for tool_call in tool_calls:\n",
    "    print(f\"\\n--- Executing Tool: {tool_call['name']} ---\")\n",
    "    \n",
    "    # Get the result from your actual tool function\n",
    "    tool_output = wikipedia_tool.invoke(tool_call['args'])\n",
    "    \n",
    "    # Package the result in a ToolMessage with the correct ID\n",
    "    tool_outputs.append(\n",
    "        ToolMessage(content=str(tool_output), tool_call_id=tool_call[\"id\"])\n",
    "    )\n",
    "\n",
    "print(f\"\\n--- Tool Output (first 200 chars): ---\\n{tool_outputs[0].content[:200]}...\")\n",
    "\n",
    "#or this way\n",
    "wikipedia_result = wikipedia_tool.invoke(\"Chi è il primo ministro italiano?\",config={\"configurable\": {\"wikipedia_top_k\": 5, \"wikipedia_lang\": \"it\"}}\n",
    ")\n",
    "print(wikipedia_result[:200],end=\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "#----- ARXIV ------\n",
    "\n",
    "arxiv_res = arxiv_tool.invoke(\"What is Chain of Thought?\")\n",
    "print(arxiv_res[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51155ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc527bd1",
   "metadata": {},
   "source": [
    "### AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577c61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['task'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['task'], input_types={}, partial_variables={}, template=\"You are an expert research director. Your goal is to create a detailed, step-by-step research plan \\n    to answer the user's task.\\n\\n    Task: {task}\\n\\n    Generate a research plan with a main search query and a list of steps.\"), additional_kwargs={})]\n",
      "--- Testing the Planner Agent ---\n",
      "Generated Main Query: What is the future trajectory and key drivers of Retrieval-Augmented Generation (RAG) in AI systems, including architectures, data strategies, evaluation metrics, and deployment considerations.\n",
      "Generated Steps:\n",
      "1. 1) Define scope, success criteria, and time horizons (short-term: 1-2 years; mid-term: 3-5 years; long-term: 5-10+ years) for the RAG future study; specify target domains (open-domain QA, code, scientific literature, multi-modal), and outcomes (theory, benchmarks, roadmap).\n",
      "2. 2) Develop a compact theory of change: identify the core components of RAG (retriever, reader/generator, interface, and tooling) and the principal levers that will shape the future (data scale, indexing, retrieval quality, prompting, alignment, latency, cost, privacy).\n",
      "3. 3) Compile a master literature map: collect and categorize key papers, reports, and whitepapers on RAG, dense vs sparse retrieval, vector databases, multi-hop retrieval, guidance from industry labs (OpenAI, DeepMind, Meta, Google), and relevant benchmarks (KILT, MRQA, etc.).\n",
      "4. 4) Design comprehensive search queries and sources: arXiv, ACL Anthology, IEEE/ACM, major ML conferences (NAACL/ACL/ICML/NeurIPS/ICLR), industry blogs, standards bodies, and whitepapers; include terms like 'retrieval augmented generation', 'RAG', 'hybrid retriever-reader', 'latent retrieval', 'RLHF for retrieval', 'end-to-end differentiable retrieval'.\n",
      "5. 5) Inventory current architectures and systems: document architectures (pipeline RAG vs end-to-end retriever-tuner, tool-augmented retrieval, memory-based models), retrievers (BM25, dense vectors, FAISS, Milvus, ColBERT, DPR, FiD, RAG family), readers (T5/OPT/LLMs), and orchestration patterns.\n",
      "6. 6) Identify data and retrieval backends trends: scale of knowledge bases (Wikipedia, KILT datasets, scientific corpora), indexing strategies, update frequency, gossip/refresh mechanisms, data provenance and licensing considerations.\n",
      "7. 7) Establish an evaluation framework: define metrics for retrieval quality (recall@k, MRR, nDCG), generation quality and factuality (BLEU/ROUGE/ROUGE-L, BERTScore, BLiP, QUEST, FEQA), factual consistency (QGu), latency, throughput, cost, and user satisfaction; include safety and robustness tests.\n",
      "8. 8) Plan an empirical benchmarking plan: select representative tasks and datasets (open-domain QA, code retrieval, factual question answering, scientific literature summarization), build baselines (BM25+generator, dense retrieval variants, current SOTA RAG), and define ablations.\n",
      "9. 9) Map potential future architectures and research directions: end-to-end differentiable retrieval with learned indexing; hybrid retrieval combining dense and sparse signals; retrieval-conditioned prompting; retrieval-aware training loops; cross-domain and multi-modal retrieval; memory graphs and knowledge bases; privacy-preserving retrieval.\n",
      "10. 10) Conduct horizon scanning for drivers and risks: compute cost, data freshness, regulatory constraints, privacy, data leakage, misinformation, model misalignment, hallucination mitigation, latency constraints, energy usage, and environmental impact.\n",
      "11. 11) Design a practical RAG research experiments plan: data collection and curation, reproducible experimental protocol, baselines, ablation strategy, hyperparameter search plan, and hardware/resource plan.\n",
      "12. 12) Create scenario narratives: build 3-4 plausible futures (e.g., rapid adoption with strong privacy controls; gradual advancement with high data-safety concerns; multi-modal and tool-augmented ecosystems; stagnation due to regulation) with probabilities and implications for research priorities.\n",
      "13. 13) Address ethics, governance, and safety: data provenance, licensing, consent, bias, fairness, misinformation risk, explainability, and auditability of retrieval results.\n",
      "14. 14) Outline deployment and productization considerations: MLOps, monitoring for data drift, retrieval degradation, system observability, governance, risk controls, rollback plans, and regulatory compliance.\n",
      "15. 15) Propose standards, benchmarks, and open science policies: propose or adapt benchmarks for RAG evaluation, data sharing rules, reproducibility guidelines, and encourage open datasets/code; identify potential standard bodies or consortia to engage.\n",
      "16. 16) Develop a roadmap with milestones: near-term (1-2y) research questions and experiments; mid-term (3-5y) integration with industry pipelines and multi-domain data; long-term (5-10y) ambitious goals (fully adaptive, self-updating knowledge bases, privacy-preserving retrieval).\n",
      "17. 17) Create a validation plan: expert interviews, surveys, and community feedback to validate assumptions and refine priorities; identify key stakeholders and plan for outreach.\n",
      "18. 18) Build a reproducible methodology and artifacts: document search strategies, data sources, inclusion/exclusion criteria, versioned code, and a living literature digest; set up a lightweight dashboard to track progress.\n",
      "19. 19) Deliverables and communication: executive summary, detailed literature digest, drivers-and-risks map, scenario analysis, benchmarking plan, synthesis of findings, and a final roadmap.\n",
      "20. 20) Timeline, resource estimates, and risk mitigation: provide a Gantt-like timeline, resource requirements (person-hours, compute), potential bottlenecks, and mitigation strategies.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "#define structure of planner output\n",
    "class ResearchPlan(BaseModel):\n",
    "    main_query: str = Field(description=\"A concise, focused query for search engines based on the initial task.\")\n",
    "    steps: List[str] = Field(description=\"A list of detailed, sequential steps to execute for the research.\")\n",
    "\n",
    "\n",
    "\n",
    "planner_llm = ChatOpenAI(model=\"gpt-5-nano\",temperature=0,)\n",
    "planner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an expert research director. Your goal is to create a detailed, step-by-step research plan \n",
    "    to answer the user's task.\n",
    "\n",
    "    Task: {task}\n",
    "    \n",
    "    Generate a research plan with a main search query and a list of steps.\"\"\")\n",
    "\n",
    "print(planner_prompt)\n",
    "\n",
    "planner_chain = planner_prompt | planner_llm.with_structured_output(schema=ResearchPlan)\n",
    "\n",
    "print(\"--- Testing the Planner Agent ---\")\n",
    "test_plan = planner_chain.invoke({\"task\": \"What is the future of Retrieval-Augmented Generation?\"})\n",
    "print(f\"Generated Main Query: {test_plan.main_query}\")\n",
    "print(\"Generated Steps:\")\n",
    "for i, step in enumerate(test_plan.steps, 1):\n",
    "    print(f\"{i}. {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c88a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'} template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'\n",
      "\n",
      "--- Testing the Execution Agent ---\n",
      "\n",
      "Executor's response to the test step:\n",
      "Here’s a concise, up-to-date overview of the latest developments in multimodal retrieval-augmented generation (multimodal RAG) as of 2025–2025. I’ve pulled from recent workshops, surveys, blog posts, and implementations.\n",
      "\n",
      "Key trends and directions\n",
      "- Unified multimodal RAG architectures\n",
      "  - Moving beyond text+image to jointly handle text, images, tables, and video (and audio in some lines). The goal is a single pipeline that can retrieve across modalities and reason over heterogeneous content.\n",
      "  - Notable work: Multimodal Retrieval-Augmented Generation: Unified Information Processing Across Text, Image, Table, and Video Modalities (MAGMaR 2025). This work emphasizes integrated processing and cross-modal reasoning in a single framework.\n",
      "\n",
      "- Multimodal RAG for richer data types\n",
      "  - Extending retrieval to tables, diagrams, charts, long-form videos, and audio transcripts. This makes RAG suitable for document understanding, education, and enterprise knowledge bases.\n",
      "  - Examples: articles and demos on multimodal video/audio RAG; CLIP-RAG and related approaches that bridge vision-language spaces to retrieve non-textual content in response to textual queries.\n",
      "\n",
      "- Retrieval infrastructure and pipelines\n",
      "  - Hybrid retrieval is popular: combining dense vector search with symbolic or structured constraints to improve precision on complex visuals and diagrams.\n",
      "  - Frameworks and tooling maturing:\n",
      "    - LlamaIndex, Haystack, Weaviate, Milvus, and similar vector DBs now support multimodal retrieval workflows and pipelines.\n",
      "    - NVIDIA NIM/Nemo-based pipelines for multimodal document retrieval and extraction (e.g., faster extraction from PDFs and multimodal data).\n",
      "  - The emphasis is on real-time or near-real-time retrieval with multi-hop reasoning across modalities.\n",
      "\n",
      "- Evaluation, benchmarking, and automated assessment\n",
      "  - There’s growing attention to robust evaluation of multimodal RAG systems, not just textual QA. Automated evaluation suites (e.g., RAGAs) are being proposed to assess retrieval relevance, factuality, and alignment across modalities.\n",
      "  - Community resources (surveys and demos) are helping standardize how to test multimodal RAG in education, healthcare, and enterprise contexts.\n",
      "\n",
      "- Practical models and systems\n",
      "  - State-of-the-art LLMs and multimodal LLMs (e.g., GPT-4o, Claude 3 family) are increasingly paired with RAG backends to provide multimodal-aware generation.\n",
      "  - Industry/tooling workflows highlight practical implementations: MagMaR papers, tutorials, and blog posts showing how to design, train, and deploy multimodal RAG pipelines.\n",
      "  - Company and open-source efforts (e.g., NeMo Retriever, LiDAR-like retrieval for documents, and “NIM” style agents) demonstrate viable enterprise-grade pipelines.\n",
      "\n",
      "Key recent works and resources to look at\n",
      "- MAGMaR 2025 workshop paper\n",
      "  - Multimodal Retrieval-Augmented Generation: Unified Information Processing Across Text, Image, Table, and Video Modalities\n",
      "  - Authors: Nazarii Drushchak, Nataliya Polyakovska, Maryna Bautina, Taras Semenchenko, Jakub Koscielecki, Wojciech Sykala, Michal Wegrzynowski, et al.\n",
      "  - Focus: unified processing across 4 modalities; cross-modal retrieval and reasoning.\n",
      "\n",
      "- RAGAs: Automated evaluation of retrieval augmented generation (2024)\n",
      "  - Focus: evaluation methodology for RAG systems, including multimodal contexts.\n",
      "\n",
      "- An Easy Introduction to Multimodal Retrieval-Augmented Generation for Video and Audio (Dec 2024)\n",
      "  - Practical introduction to building multimodal RAG pipelines that include video and audio data sources.\n",
      "\n",
      "- A comprehensive survey preprint: Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation (arXiv: 2025)\n",
      "  - Broad survey of modalities, architectures, tasks, prompts, and evaluation.\n",
      "\n",
      "- GitHub: Multimodal RAG survey and educational materials (2025)\n",
      "  - Repository with slides, PDFs, and tutorials summarizing approaches and experiments.\n",
      "\n",
      "- Blog and explainer: Multimodal Retrieval-Augmented Generation: Advances and remaining challenges (denemlabs, Apr 2025)\n",
      "  - Overview of advances (e.g., CLIP-RAG, FaRL) and remaining challenges like alignment, memory, and cross-modal reasoning.\n",
      "\n",
      "- Related system and benchmark notes\n",
      "  - R^2AG (2022) describes augmenting retrieval with retrieval-aware prompting; foundational but still relevant for understanding alignment between retrievers and LLMs.\n",
      "  - GPT-4o and other system cards provide context on how modern multimodal models can be complemented with RAG-style retrieval.\n",
      "\n",
      "What this means for practitioners\n",
      "- When to use multimodal RAG\n",
      "  - Your questions require evidence or content from multiple modalities (text, images, diagrams, tables, videos, audio).\n",
      "  - You need cross-modal reasoning (e.g., answer requires both textual description and a figure/diagram).\n",
      "  - You want up-to-date information beyond what the model was trained on, sourced from curated multimodal corpora.\n",
      "\n",
      "- Architecture tips\n",
      "  - Start with a heterogeneous retriever: combine dense cross-modal embeddings with modality-specific constraints (e.g., image embeddings for visuals, structured data for tables).\n",
      "  - Use hybrid retrieval pipelines: dense + symbolic constraints to control scope and precision.\n",
      "  - Consider retrieval-aware prompting or RAG-aware generation to help the LLM better utilize retrieved multimodal content.\n",
      "  - Leverage tooling ecosystems: LlamaIndex, Haystack, Weaviate, Milvus, and NVIDIA/NEM stacks for end-to-end pipelines.\n",
      "\n",
      "- Evaluation and governance\n",
      "  - Include multimodal-specific evaluation metrics: factual accuracy across modalities, grounding in visuals, answer completeness, and the quality of image/table sources.\n",
      "  - Use automated evaluation where possible, but plan for human-in-the-loop checks for complex multimodal reasoning and critical-domain tasks (health, law, finance).\n",
      "\n",
      "- Challenges to anticipate\n",
      "  - Alignment gaps between retriever signals and LLM reasoning across modalities.\n",
      "  - Computational and memory demands of processing large multimodal corpora.\n",
      "  - Data quality, copyright, and licensing issues when pulling content from diverse modalities.\n",
      "  - Hallucinations or misalignment when visual content is misinterpreted by the model.\n",
      "\n",
      "If you tell me your exact use case (e.g., multimodal QA over enterprise documents, educational content with diagrams and videos, or clinical decision support with charts and reports), I can:\n",
      "- Recommend a concrete architecture (which retrievers, vector DB, and LLMs to pair).\n",
      "- Point to specific papers and tutorials most relevant to your domain.\n",
      "- Provide a starter implementation plan and a shortlist of datasets or sample pipelines to experiment with.\n",
      "\n",
      "Would you like a deeper dive into any of the papers (MAGMaR 2025, RAGAs, or the multimodal RAG survey), or a practical starter guide for building a multimodal RAG pipeline in your target domain?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain import hub\n",
    "\n",
    "executor_llm = ChatOpenAI(model=\"gpt-5-nano\",temperature=0,)\n",
    "\n",
    "#careful if using custom prompt, might mess up stuff\n",
    "execution_agent = create_react_agent(model=executor_llm,tools= research_tools,)#prompt=execution_prompt)\n",
    "\n",
    "print(\"\\n--- Testing the Execution Agent ---\")\n",
    "# Let's test it on a single, simple task.\n",
    "test_step = \"Search for the latest developments in multi-modal RAG systems.\"\n",
    "# The agent expects its input in a specific format: a dictionary with a 'messages' key.\n",
    "executor_output = execution_agent.invoke({\"messages\": [(\"user\", test_step)]})\n",
    "\n",
    "# The agent's final answer is always in the last message of the output.\n",
    "final_answer = executor_output['messages'][-1].content\n",
    "print(f\"\\nExecutor's response to the test step:\\n{final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv_search (call_apHMzcBCznQoD06E9nXM3vg9)\n",
      " Call ID: call_apHMzcBCznQoD06E9nXM3vg9\n",
      "  Args:\n",
      "    query: multimodal retrieval augmented generation\n",
      "  arxiv_search (call_qG4tcaCnizJIpmi1sCjluQGL)\n",
      " Call ID: call_qG4tcaCnizJIpmi1sCjluQGL\n",
      "  Args:\n",
      "    query: multimodal RAG 2023 2024 2025\n",
      "  tavily_search_results_json (call_7p98qmn914B7pRaEbsr4vtKT)\n",
      " Call ID: call_7p98qmn914B7pRaEbsr4vtKT\n",
      "  Args:\n",
      "    query: multimodal RAG latest developments 2024 2025\n",
      "  wikipedia_search (call_36iCysIIuTQ8JePOx0jYcUzL)\n",
      " Call ID: call_36iCysIIuTQ8JePOx0jYcUzL\n",
      "  Args:\n",
      "    query: Retrieval-augmented generation multimodal\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2025-03-03\n",
      "Title: MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models\n",
      "Authors: Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao\n",
      "Summary: Artificial Intelligence (AI) has demonstrated significant potential in\n",
      "healthcare, particularly in disease diagnosis and treatment planning. Recent\n",
      "progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new\n",
      "possibilities for interactive diagnostic tools. However, these models often\n",
      "suffer from factual hallucination, which can lead to incorrect diagnoses.\n",
      "Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to\n",
      "address these issues. However, the amount of high-quality data and distribution\n",
      "shifts between training data and deployment data limit the application of\n",
      "fine-tuning methods. Although RAG is lightweight and effective, existing\n",
      "RAG-based approaches are not sufficiently general to different medical domains\n",
      "and can potentially cause misalignment issues, both between modalities and\n",
      "between the model and the ground truth. In this paper, we propose a versatile\n",
      "multimodal RAG system, MMed-RAG, designed to enhance the factuality of\n",
      "Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an\n",
      "adaptive retrieved contexts selection method, and a provable RAG-based\n",
      "preference fine-tuning strategy. These innovations make the RAG process\n",
      "sufficiently general and reliable, significantly improving alignment when\n",
      "introducing retrieved contexts. Experimental results across five medical\n",
      "datasets (involving radiology, ophthalmology, pathology) on medical VQA and\n",
      "report generation demonstrate that MMed-RAG can achieve an average improvement\n",
      "of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available\n",
      "in https://github.com/richard-peng-xia/MMed-RAG.\n",
      "\n",
      "Published: 2025-08-16\n",
      "Title: OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation\n",
      "Authors: Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying Li, Ka-Ho Chow, Conghui He, Wentao Zhang\n",
      "Summary: Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by\n",
      "integrating external knowledge to reduce hallucinations and incorporate\n",
      "up-to-date information without retraining. As an essential part of RAG,\n",
      "external knowledge bases are commonly built by extracting structured data from\n",
      "unstructured PDF documents using Optical Character Recognition (OCR). However,\n",
      "given the imperfect prediction of OCR and the inherent non-uniform\n",
      "representation of structured data, knowledge bases inevitably contain various\n",
      "OCR noises. In this paper, we introduce OHRBench, the first benchmark for\n",
      "understanding the cascading impact of OCR on RAG systems. OHRBench includes\n",
      "8,561 carefully selected unstructured document images from seven real-world RAG\n",
      "application domains, along with 8,498 Q&A pairs derived from multimodal\n",
      "elements in documents, challenging existing OCR solutions used for RAG. To\n",
      "better understand OCR's impact on RAG systems, we identify two primary types of\n",
      "OCR noise: Semantic Noise and Formatting Noise and apply perturbation to\n",
      "generate a set of structured data with varying degrees of each OCR noise. Using\n",
      "OHRBench, we first conduct a comprehensive evaluation of current OCR solutions\n",
      "and reveal that none is competent for constructing high-quality knowledge bases\n",
      "for RAG systems. We then systematically evaluate the impact of these two noise\n",
      "types and demonstrate the trend relationship between the degree of OCR noise\n",
      "and RAG performance. Our OHRBench, including PDF documents, Q&As, and the\n",
      "ground truth structured data are released at:\n",
      "https://github.com/opendatalab/OHR-Bench\n",
      "\n",
      "Published: 2025-02-24\n",
      "Title: FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research\n",
      "Authors: Jiajie Jin, Yutao Zhu, Guanting Dong, Yuyao Zhang, Xinyu Yang, Chenghao Zhang, Tong Zhao, Zhao Yang, Zhicheng Dou, Ji-Rong Wen\n",
      "Summary: With the advent of\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2025-02-06\n",
      "Title: Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation\n",
      "Authors: Liwen Sun, James Zhao, Megan Han, Chenyan Xiong\n",
      "Summary: Multimodal foundation models hold significant potential for automating\n",
      "radiology report generation, thereby assisting clinicians in diagnosing cardiac\n",
      "diseases. However, generated reports often suffer from serious factual\n",
      "inaccuracy. In this paper, we introduce a fact-aware multimodal\n",
      "retrieval-augmented pipeline in generating accurate radiology reports\n",
      "(FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then\n",
      "integrate factual knowledge to train a universal multimodal retriever. Given a\n",
      "radiology image, our retriever can identify high-quality reference reports to\n",
      "augment multimodal foundation models, thus enhancing the factual completeness\n",
      "and correctness of report generation. Experiments on two benchmark datasets\n",
      "show that our multimodal retriever outperforms state-of-the-art retrievers on\n",
      "both language generation and radiology-specific metrics, up to 6.5% and 2%\n",
      "score in F1CheXbert and F1RadGraph. Further analysis indicates that employing\n",
      "our factually-informed training strategy imposes an effective supervision\n",
      "signal, without relying on explicit diagnostic label guidance, and successfully\n",
      "propagates fact-aware capabilities from the multimodal retriever to the\n",
      "multimodal foundation model in radiology report generation.\n",
      "\n",
      "Published: 2025-08-08\n",
      "Title: M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation\n",
      "Authors: Zhiyou Xiao, Qinhan Yu, Binghui Li, Geng Chen, Chong Chen, Wentao Zhang\n",
      "Summary: Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables\n",
      "diverse multimodal inputs but remains limited to single-modality outputs,\n",
      "restricting expressive capacity and practical utility. In contrast, real-world\n",
      "applications often demand both multimodal inputs and multimodal outputs for\n",
      "effective communication and grounded reasoning. Motivated by the recent success\n",
      "of Reinforcement Learning (RL) in complex reasoning tasks for Large Language\n",
      "Models (LLMs), we adopt RL as a principled and effective paradigm to address\n",
      "the multi-step, outcome-driven challenges inherent in multimodal output\n",
      "generation. Here, we introduce M2IO-R1, a novel framework for Multimodal\n",
      "Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal\n",
      "inputs and outputs. Central to our framework is an RL-based inserter,\n",
      "Inserter-R1-3B, trained with Group Relative Policy Optimization to guide image\n",
      "selection and placement in a controllable and semantically aligned manner.\n",
      "Empirical results show that our lightweight 3B inserter achieves strong\n",
      "reasoning capabilities with significantly reduced latency, outperforming\n",
      "baselines in both quality and efficiency.\n",
      "\n",
      "Published: 2025-04-21\n",
      "Title: MRAMG-Bench: A Comprehensive Benchmark for Advancing Multimodal Retrieval-Augmented Multimodal Generation\n",
      "Authors: Qinhan Yu, Zhiyou Xiao, Binghui Li, Zhengren Wang, Chong Chen, Wentao Zhang\n",
      "Summary: Recent advances in Retrieval-Augmented Generation (RAG) have significantly\n",
      "improved response accuracy and relevance by incorporating external knowledge\n",
      "into Large Language Models (LLMs). However, existing RAG methods primarily\n",
      "focus on generating text-only answers, even in Multimodal Retrieval-Augmented\n",
      "Generation (MRAG) scenarios, where multimodal elements are retrieved to assist\n",
      "in generating text answers. To address this, we introduce the Multimodal\n",
      "Retrieval-Augmented Multimodal Generation (MRAMG) task, in which we aim to\n",
      "generate multimodal answers that combine both text and images, fully leveraging\n",
      "the multimodal data within a corpus. Despite growing attention to this\n",
      "challenging task, a notable lack of a comprehensive benchmark persists for\n",
      "effectively evaluating its performance. To bridge this gap, we provide\n",
      "MRAMG-Bench, a meticulously curated, human-annotated benchmark comprising 4,346\n",
      "docum\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia_search\n",
      "\n",
      "Page: Large language model\n",
      "Summary: A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\n",
      "The largest and most capable LLMs are generative pretrained transformers (GPTs), based on a transformer architecture, which are largely used in generative chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\n",
      "\n",
      "Page: Multimodal learning\n",
      "Summary: Multimodal learning is a type of deep learning that integrates and processes multiple types of data, referred to as  modalities, such as text, audio, images, or video. This integration allows for a more holistic understanding of complex data, improving model performance in tasks like visual question answering, cross-modal retrieval, text-to-image generation, aesthetic ranking, and image captioning.\n",
      "Large multimodal models, such as Google Gemini and GPT-4o, have become increasingly popular since 2023, enabling increased versatility and a broader understanding of real-world phenomena.\n",
      "\n",
      "Page: Music and artificial intelligence\n",
      "Summary: Music and artificial intelligence (music and AI) is the development of music software programs which use AI to generate music. As with applications in other fields, AI in music also simulates mental tasks. A prominent feature is the capability of an AI algorithm to learn based on past data, such as in computer accompaniment technology, wherein the AI is capable of listening to a human performer and performing accompaniment. Artificial intelligence also drives interactive composition technology, wherein a computer composes music in response to a live performance. There are other AI applications in music that cover not only music composition, production, and performance but also how music is marketed and consumed. Several music player programs have also been developed to use voice recognition and natural language processing technology for music voice control. Current research includes the application of AI in music composition, performance, theory and digital sound processing. Composers/artists like Jennifer Walshe or Holly Herndon have been exploring aspects of music AI for years in their performances and musical works. Another original approach of humans “imitating AI” can be found in the 43-hour sound installation String Quartet(s) by Georges Lentz (see interview with ChatGPT-4 on music and AI).\n",
      "20th century art historian Erwin Panofsky proposed that in all art, there existed three levels of meaning: primary meaning, or the natural subject; secondary meaning, or the conventional subject; and tertiary meaning, the intrinsic content of the subject. AI music explores the foremost of these, creating music without the \"intention\" which is usually behind it, leaving composers who listen to machine-generated pieces feeling unsettled by the lack of apparent meaning.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"What Are the Future Trends in RAG for 2025 and Beyond? - Chitika\", \"url\": \"https://www.chitika.com/future-trends-in-retrieval-augmented-generation-what-to-expect-in-2025-and-beyond/\", \"content\": \"Key advancements driving Retrieval-Augmented Generation in 2025 include the integration of multimodal systems, enabling seamless processing of diverse data types, and adaptive algorithms that evolve with user needs. Additionally, dynamic indexing ensures real-time updates, enhancing accuracy and relevance.\\n\\n### How will multimodal RAG systems impact various industries? [...] Actionable insight: Combine neural re-ranking with adaptive feedback loops to refine retrieval precision dynamically, ensuring relevance in evolving datasets.\\n\\n## Emerging Trends in RAG for 2025 and Beyond\\n\\nMultimodal RAG integrates text, images, and audio, enabling personalized education platforms that adapt to diverse learning styles.\\n\\nUnexpected connection: Cross-lingual retrieval bridges global communication gaps, fostering real-time multilingual collaboration. [...] Unexpectedly, multimodal RAG—integrating text, images, and audio—has emerged as a transformative innovation. For instance, educational platforms now combine textual explanations with visual aids, enhancing learning outcomes.\\n\\nExpert perspectives emphasize the importance of curated datasets and adaptive retrieval algorithms. These ensure relevance, reduce bias, and improve system trustworthiness, particularly in high-stakes fields like finance and governance.\", \"score\": 0.87139857}, {\"title\": \"The Rise and Evolution of RAG in 2024 A Year in Review - RAGFlow\", \"url\": \"https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review\", \"content\": \"In the upcoming year of 2025, research based on Encoder-Decoder architectures is expected to advance further. We can anticipate the potential development of a unified multi-modal document parsing model capable of accurately converting various unstructured documents into text content. [...] promote multimodal RAG. It is anticipated that with more models becoming available, tensor-based reranking will see widespread application in 2025. [...] Whether for multimodal or textual data, the results of chunking significantly impact final outcomes. In 2025, we can expect more high-quality work in this area that will ultimately resolve issues related to data entry quality.\\n\\n## Hybrid Search​\", \"score\": 0.8649622}, {\"title\": \"The future of multimodal RAG systems: transforming AI's capabilities\", \"url\": \"https://superlinear.eu/insights/articles/the-future-of-multimodal-rag-systems-transforming-ai-capabilities\", \"content\": \"# The future of multimodal RAG systems: transforming AIâs capabilities\\n\\n# The future of multimodal RAG systems: transforming AIâs capabilities\\n\\nLast updated on:\\n\\n30 Apr 2025\\n\\nPublished on:\\n\\n21 Feb 2025\\n\\nExplore the next evolution of Retrieval-Augmented Generation (RAG), where AI goes beyond text to integrate images, video, and audio. Multimodal RAG unlocks richer, more precise insights, but merging diverse data comes with challenges. [...] Cantersteen 47â¨â¨\\n\\n1000 Brussels\\n\\nGhent\\n\\nPlanet Group Arena  \\nOttergemsesteenweg-Zuid 808 b300   \\n9000 Gent\\n\\nÂ© 2024 Superlinear. All rights reserved.\\n\\nPrivacy Policy\\n\\nTerms of Service [...] Explore the next evolution of Retrieval-Augmented Generation (RAG), where AI goes beyond text to integrate images, video, and audio. Multimodal RAG unlocks richer, more precise insights, but merging diverse data comes with challenges.\\n\\nExplore the next evolution of Retrieval-Augmented Generation (RAG), where AI goes beyond text to integrate images, video, and audio. Multimodal RAG unlocks richer, more precise insights, but merging diverse data comes with challenges.\", \"score\": 0.8585411}, {\"title\": \"RAG at the Crossroads - Mid-2025 Reflections on AI's Incremental ...\", \"url\": \"https://ragflow.io/blog/rag-at-the-crossroads-mid-2025-reflections-on-ai-evolution\", \"content\": \"Turning to the second aspect: multimodal data. In our year-end review, we highlighted Multimodal RAG (MM-RAG) as a key trend for 2025. Yet, by mid-year, this trend has failed to gain momentum. The primary obstacle remains the immaturity of the supporting infrastructure. As noted, late interaction models continue to dominate MM-RAG pipelines, meaning embedding models produce Tensors, or multi-vectors. For instance, a single image may be represented by 1,024 vectors, each comprising [...] Throughout our events from late 2024 to early 2025, we consistently highlighted three key features for RAG in 2025: reasoning, memory, and multimodality. The first two are inherently linked to Agents. In our initial blog this year, we offered a comprehensive overview of implementation of reasoning. A recent survey [Ref 3] further synthesizes reasoning and RAG, and we have adapted and condensed its framework as shown below: [...] Having examined the relationship between RAG and Agents, let us now refocus on RAG itself. Although RAG-related papers continued to be published steadily in 2025, genuine innovation in concepts and systems was notably scarce. Has RAG technology reached a critical plateau? At its core, RAG relies on information retrieval (IR), a well-established field. Yet RAG presents new challenges beyond traditional IR, including query diversity and multimodal data.\", \"score\": 0.850382}, {\"title\": \"Guide to Multimodal RAG for Images and Text (in 2025) - Medium\", \"url\": \"https://medium.com/kx-systems/guide-to-multimodal-rag-for-images-and-text-10dab36e3117\", \"content\": \"Multimodal AI stands at the forefront of the next wave of AI advancements. In fact, one of my generative AI predictions for 2025 is that multimodal models will move into the mainstream and become the norm by the end of the year. Traditional AI systems can typically only process a single data type, such as text, but take a look around you, the world is not unimodal. Humans interact with their environment using multiple senses, multimodal AI aims to emulate this perception in machines. Ideally, [...] This article has been refreshed to incorporate Voyage AI’s multimodal embedding model, replacing the previously used Meta ImageBind. This update offers improved performance and streamlined implementation, while ensuring the content remains current, as ImageBind hasn’t received recent updates.\\n\\n\\\\\\\\\\\\\\n\\nFirst, let’s pip install and import necessary packages (Sign-Up for VoyageAI to get an API key, it comes with 200,000 free tokens): [...] With both text and image summaries embedded and stored in the vector database, multimodal similarity search is enabled. When the most relevant embeddings are retrieved, we can pass them to an LLM for RAG.\\n\\nNow that we have a high level understanding of this method, it’s time to dive into some code! View the full notebook here for the details.\", \"score\": 0.7715858}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here’s a concise synthesis of the latest developments in multimodal retrieval-augmented generation (MRAG) and related multimodal RAG systems, focusing on 2024–2025 work and emerging trends.\n",
      "\n",
      "Key themes and advances\n",
      "\n",
      "- Fact-grounded multimodal RAG\n",
      "  - What’s new: Moving beyond generic retrieval to factual grounding in multimodal outputs, especially in high-stakes domains like medicine.\n",
      "  - Notable work:\n",
      "    - Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation (FactMM-RAG) — uses RadGraph to mine fact-rich report pairs and train a universal multimodal retriever. Improves factual accuracy (e.g., F1CheXbert, F1RadGraph) in radiology report generation.\n",
      "  - Impact: Reduces hallucinations by anchoring generated content to fact-consistent references retrieved from a domain-specific corpus.\n",
      "\n",
      "- Multimodal outputs and RL-driven reasoning\n",
      "  - What’s new: Systems that generate not just text but multimodal outputs (e.g., text plus images) and use reinforcement learning to optimize the multimodal reasoning and output assembly.\n",
      "  - Notable work:\n",
      "    - M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation (2025-04-21)\n",
      "      - Introduces Inserter-R1-3B, an RL-based module that selects and places images in a controllable, semantically aligned way.\n",
      "      - Emphasizes reduced latency with a small inserter model while maintaining reasoning quality.\n",
      "    - MRAMG-Bench (2025-04-21)\n",
      "      - A benchmark for evaluating Multimodal Retrieval-Augmented Multimodal Generation (MRAMG), focusing on generating multimodal answers (text + images) and providing a standardized evaluation suite.\n",
      "  - Impact: Enables practical multimodal outputs with better control over how retrieved content is integrated into generated answers.\n",
      "\n",
      "- Domain-specific MRAG for medicine and biology\n",
      "  - What’s new: Tailored retrieval and alignment mechanisms to medical domains, with stronger factuality guarantees and domain-aware retrieval strategies.\n",
      "  - Notable work:\n",
      "    - MMed-RAG: Versatile Multimodal RAG System for Medical Vision-Language Models (2025-08-16)\n",
      "      - Proposes domain-aware retrieval, adaptive context selection, and provable RAG-based fine-tuning to improve factual alignment across multiple medical datasets (radiology, ophthalmology, pathology).\n",
      "      - Reports substantial factual accuracy gains across five medical datasets.\n",
      "  - Impact: Demonstrates that carefully designed retrieval and alignment strategies can generalize across multiple medical modalities and tasks.\n",
      "\n",
      "- OCR reliability and data quality in RAG\n",
      "  - What’s new: Systematic study of how OCR noise and document extraction quality affect RAG systems, with benchmarks to quantify cascading errors.\n",
      "  - Notable work:\n",
      "    - OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation (OHRBench) (2025-02-24)\n",
      "      - Introduces OHRBench (8,561 unstructured document images; 8,498 Q&As) and analyzes semantic vs. formatting OCR noise.\n",
      "  - Impact: Highlights a major bottleneck in real-world RAG pipelines and motivates better OCR preprocessing, noise-robust representations, and OCR-aware retrieval.\n",
      "\n",
      "- Efficient tooling and modular architectures\n",
      "  - What’s new: Modular toolkits and efficient retriever-inserter architectures to accelerate development and deployment of MRAG systems.\n",
      "  - Notable work:\n",
      "    - FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research (2024–2025 window; tooling paper)\n",
      "    - Inserter-R1-3B (as part of M2IO-R1) for lightweight, fast multimodal content insertion\n",
      "  - Impact: Lowers the barrier to building and benchmarking MRAG systems, enabling more rapid experimentation and deployment.\n",
      "\n",
      "- Multimodal RAG trend analyses and industry perspectives\n",
      "  - What’s new: Industry and research blogs/panels discuss the trajectory of multimodal RAG, emphasizing reasoning, memory, multimodality, and the maturity of infrastructure.\n",
      "  - Notable perspectives:\n",
      "    - “The future of multimodal RAG systems” and “RAG at the Crossroads” (mid-2025) discuss industry momentum, challenges in infrastructure, and the potential role of agents and memory in RAG systems.\n",
      "  - Impact: Provides context for prioritizing research directions (e.g., better multimodal encoders, dynamic indexing, and agent-based reasoning).\n",
      "\n",
      "Representative papers, systems, and benchmarks (2024–2025)\n",
      "\n",
      "- Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation (FactMM-RAG)\n",
      "  - Date: 2025-02-06\n",
      "  - Key idea: RadGraph-driven factual retriever to augment radiology report generation with factual references.\n",
      "  - Outcome: Improved factual metrics on radiology benchmarks.\n",
      "\n",
      "- MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models\n",
      "  - Date: 2025-08-16\n",
      "  - Key idea: Domain-aware retrieval, adaptive context selection, and provable RAG fine-tuning for medical LVLMs.\n",
      "  - Outcome: ~large gains in factual accuracy across multiple medical datasets.\n",
      "\n",
      "- MRAMG-Bench: A Comprehensive Benchmark for Advancing Multimodal Retrieval-Augmented Multimodal Generation\n",
      "  - Date: 2025-04-21\n",
      "  - Key idea: Benchmark suite for MRAMG with multimodal answers (text + images).\n",
      "  - Outcome: Standardized evaluation framework for MRAMG tasks.\n",
      "\n",
      "- M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval-Augmented Multimodal Generation\n",
      "  - Date: 2025-04-21\n",
      "  - Key idea: RL-based Inserter-R1-3B to place retrieved images in generated outputs; focuses on efficiency and alignment.\n",
      "\n",
      "- OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation (OHRBench)\n",
      "  - Date: 2025-02-24\n",
      "  - Key idea: Benchmark to quantify how OCR noise propagates through RAG pipelines; identifies semantic vs. formatting noise types.\n",
      "\n",
      "- FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research\n",
      "  - Date: 2024–2025 window\n",
      "  - Key idea: Modular tooling to simplify MRAG development and experimentation.\n",
      "\n",
      "- Additional perspectives and trend analyses\n",
      "  - Various industry/blog posts (2024–2025) discuss trends like multimodal integration, dynamic indexing, cross-lingual retrieval, and the potential for memory and agent-based approaches in MRAG.\n",
      "\n",
      "Implications and open challenges\n",
      "\n",
      "- Data quality and grounding\n",
      "  - OCR noise and document extraction quality (OHRBench) remain a major bottleneck for reliable MRAG, especially in real-world pipelines using PDFs and scanned docs.\n",
      "  - Fact grounding (e.g., RadGraph-based fact mining) and domain-specific knowledge graphs are increasingly used to improve factual accuracy.\n",
      "\n",
      "- Multimodal outputs and evaluation\n",
      "  - Generating accurate multimodal outputs (text + images) adds complexity to evaluation. Benchmarks like MRAMG-Bench help standardize assessment, but richer, human-aligned evaluation remains a challenge.\n",
      "\n",
      "- Efficiency and deployment\n",
      "  - Lightweight retrievers and modular toolkits (e.g., Inserter-R1-3B, FlashRAG) aim to make MRAG systems practical at scale with lower latency.\n",
      "\n",
      "- Cross-domain generalization\n",
      "  - Domain-aware retrieval and fine-tuning approaches (e.g., MMed-RAG) show promise for cross-domain generalization, but domain shifts and data scarcity remain issues in many fields.\n",
      "\n",
      "- Future directions\n",
      "  - Real-time and dynamic indexing to keep knowledge up to date.\n",
      "  - Multimodal reasoning and memory-enabled agents that can plan and justify multimodal outputs.\n",
      "  - Cross-lingual and cross-modal retrieval to support global, multilingual applications.\n",
      "  - More robust multimodal evaluation protocols that account for both factuality and alignment across modalities.\n",
      "\n",
      "Would you like me to:\n",
      "- Summarize a specific paper in more detail (methods, experiments, and results)?\n",
      "- Pull PDFs or arXiv links for these papers?\n",
      "- Create a quick glossary of terms (MRAG, MRAMG, MMed-RAG, etc.) and a reading list ordered by relevance to your interests (medicine, general MRAG, benchmarks)?\n"
     ]
    }
   ],
   "source": [
    "# if you want to inspect, use stream. \n",
    "query = \"Latest developments in multimodal RAG systems.\"\n",
    "\n",
    "for event in execution_agent.stream({\"messages\": [(\"user\", query)]}):\n",
    "    # Each event is a *delta* keyed by node name, e.g. \"agent\" or \"tools\"\n",
    "    update = event.get(\"agent\", event.get(\"tools\", {}))\n",
    "    for msg in update.get(\"messages\", []):\n",
    "        msg.pretty_print()        # nice formatting of AI/Human/Tool messages\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
