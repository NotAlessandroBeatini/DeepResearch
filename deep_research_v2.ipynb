{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc987c1",
   "metadata": {},
   "source": [
    "# Deep Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a0fd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, List, Optional, Annotated\n",
    "from langchain_core.messages import BaseMessage,HumanMessage, AIMessage\n",
    "from langgraph.graph import START, END, StateGraph, add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a3297",
   "metadata": {},
   "source": [
    "### (deprecated) Define state and schema of blackboard (shared among agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10784d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     general_reference = \u001b[33m\"\u001b[39m\u001b[33mgeneral_reference\u001b[39m\u001b[33m\"\u001b[39m      \u001b[38;5;66;03m# blog/vendor docs/etc.\u001b[39;00m\n\u001b[32m     12\u001b[39m     note = \u001b[33m\"\u001b[39m\u001b[33mnote\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mCitation\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPublication title.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mHttpUrl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCanonical/stable link.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mCitation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCitation\u001b[39;00m(BaseModel):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     title: \u001b[43mOptional\u001b[49m[\u001b[38;5;28mstr\u001b[39m] = Field(\u001b[38;5;28;01mNone\u001b[39;00m, description=\u001b[33m\"\u001b[39m\u001b[33mPublication title.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     url: Optional[HttpUrl] = Field(\u001b[38;5;28;01mNone\u001b[39;00m, description=\u001b[33m\"\u001b[39m\u001b[33mCanonical/stable link.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     authors: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = Field(\u001b[38;5;28;01mNone\u001b[39;00m, description=\u001b[33m\"\u001b[39m\u001b[33mAuthor list.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "class FactKind(str, Enum):\n",
    "    finding = \"finding\"\n",
    "    draft = \"draft\"\n",
    "    critique = \"critique\"\n",
    "    plan_step = \"plan_step\"\n",
    "    scholar_reference = \"scholar_reference\"        # formal academic item\n",
    "    general_reference = \"general_reference\"      # blog/vendor docs/etc.\n",
    "    note = \"note\"\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    title: Optional[str] = Field(None, description=\"Publication title.\")\n",
    "    url: Optional[HttpUrl] = Field(None, description=\"Canonical/stable link.\")\n",
    "    authors: Optional[List[str]] = Field(None, description=\"Author list.\")\n",
    "    venue: Optional[str] = Field(None, description=\"Journal/conference or 'arXiv'.\")\n",
    "    year: Optional[int] = Field(None, ge=1900, le=2100, description=\"Publication year.\")\n",
    "    doi: Optional[str] = Field(None, description=\"DOI if available.\")\n",
    "    arxiv_id: Optional[str] = Field(None, description=\"arXiv ID like 2405.06211. Leave null if unknown.\")\n",
    "    snippet: Optional[str] = None\n",
    "    contributions: Optional[List[str]] = Field(\n",
    "        None, description=\"2–5 bullets of concrete contributions/novelty, if any.\"\n",
    "    )\n",
    "\n",
    "    results: Optional[List[str]] = Field(\n",
    "        None, description=\"1–4 concrete metrics/ original claims, if applicable.\",\n",
    "        min_length=1,max_length=4\n",
    "    )\n",
    "\n",
    "    extended_summary: str = Field(\n",
    "        None,\n",
    "        description=\"~50–300 words giving an accurate and detailed overview of the article findings\"\n",
    "        )\n",
    "    \n",
    "    tags: Optional[List[str]] = Field(None, description=\"Short tags.\")   \n",
    "\n",
    "\n",
    "class Reference(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of blog/doc/tutorial/case study.\")\n",
    "    url: HttpUrl = Field(..., description=\"Link to the source.\")\n",
    "    publisher: Optional[str] = Field(None, description=\"Blog/company site.\")\n",
    "    author: Optional[str] = Field(None, description=\"Main author if available.\")\n",
    "    year: Optional[int] = Field(None, description=\"Year.\")\n",
    "    snippet: Optional[str] = Field(None, description=\"50–300 words accurately summarizing the 'source' findings, and their relevance to the research question.\")\n",
    "\n",
    "\n",
    "#maybe this only for elaborating the stuff i found and synthesize insight\n",
    "class Finding(BaseModel):\n",
    "    statement: str = Field(description=\"1–2 sentence synthesized insight.\")                              # 1–2 sentences\n",
    "    supporting_sources: Optional[List[HttpUrl]] = Field(\n",
    "        None, description=\"sources backing this finding.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    # id/created_at are best filled server-side; keep them optional in schema\n",
    "    id: Optional[str] = Field(None, description=\"Server-generated unique identifier.\")\n",
    "    kind: FactKind\n",
    "    #content: str\n",
    "    created_at: Optional[datetime] = Field(\n",
    "        None, description=\"Server timestamp; use datetime.now(timezone.utc).\"\n",
    "    )\n",
    "    #this probably best to be handled in \"reducer\" for the blackboard\n",
    "    #replaces: Optional[str] = Field(None, description=\"ID of a superseded fact.\")\n",
    "    scholar_references: Optional[List[Citation]] = Field(\n",
    "        None,\n",
    "        description=\"Formal bibliographic references (peer-reviewed papers, preprints,reports).\")\n",
    "    \n",
    "    general_references: Optional[List[Reference]] = Field(\n",
    "        None, description=\"Informal web sources (blogs, docs, tutorials).\"\n",
    "    )\n",
    "\n",
    "    confidence: Optional[float] = Field(\n",
    "        None, ge=0.0, le=1.0, description=\"Self-estimated confidence of relevance to research question 0–1\"\n",
    "    )\n",
    "\n",
    "class ResearchOutput(BaseModel):\n",
    "    topic: str\n",
    "    facts: List[Fact]\n",
    "\n",
    "    \n",
    "# State definition\n",
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    blackboard: List[Fact]\n",
    "    researcher_msgs: Annotated[list[BaseMessage],add_messages]\n",
    "    writer_msgs: Annotated[list[BaseMessage],add_messages]\n",
    "\n",
    "\n",
    "# === Test Schema ===\n",
    "sample_fact = Fact(\n",
    "    kind=\"finding\",\n",
    "    content=\"LangGraph allows for building cyclical agentic workflows.\",\n",
    "    source_agent=\"Researcher\"\n",
    ")\n",
    "\n",
    "# Pydantic models have a nice `model_dump_json` method for inspection\n",
    "print(sample_fact.model_dump_json(indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf0a61",
   "metadata": {},
   "source": [
    "### New approach\n",
    "IDEA:\n",
    "1) Planner -> plan\n",
    "2) Researcher -> gather resources and write to blackboard\n",
    "3) Synthesizer -> read one by one the blackboard and fill more detailed fields, also synthesize Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c30432d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from typing import Optional, List, TypedDict\n",
    "from pydantic import BaseModel, HttpUrl, Field\n",
    "from enum import Enum\n",
    "import operator\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "class FactKind(str, Enum):\n",
    "    finding = \"finding\"\n",
    "    draft = \"draft\"\n",
    "    critique = \"critique\"\n",
    "    plan_step = \"plan_step\"\n",
    "    scholar_reference = \"scholar_reference\"        # formal academic item\n",
    "    general_reference = \"general_reference\"      # blog/vendor docs/etc.\n",
    "    note = \"note\"\n",
    "\n",
    "        \n",
    "class Citation(BaseModel):\n",
    "    # --- Fields for the COLLECTOR agent (easy to find) ---\n",
    "    title: Optional[str] = Field(None, description=\"Publication title.\")\n",
    "    url: Optional[HttpUrl] = Field(None, description=\"Canonical/stable link.\")\n",
    "    authors: Optional[List[str]] = Field(None, description=\"Author list.\")\n",
    "    venue: Optional[str] = Field(None, description=\"Journal/conference or 'arXiv'.\")\n",
    "    year: Optional[int] = Field(None, description=\"Publication year.\")\n",
    "    doi: Optional[str] = Field(None, description=\"DOI if available.\")\n",
    "    arxiv_id: Optional[str] = Field(None, description=\"arXiv ID like 2405.06211.\")\n",
    "\n",
    "    # --- Fields for the ENRICHMENT agent (require analysis) ---\n",
    "    snippet: Optional[str] = Field(\n",
    "        None, description=\"A brief, quoted sentence or two from the source.\"\n",
    "    )\n",
    "    contributions: Optional[List[str]] = Field(\n",
    "        None, description=\"[ENRICHMENT] 2–5 bullets of concrete contributions/novelty.\"\n",
    "    )\n",
    "    results: Optional[List[str]] = Field(\n",
    "        None, description=\"[ENRICHMENT] 1–4 concrete metrics or original claims.\"\n",
    "    )\n",
    "    extended_summary: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"[ENRICHMENT] ~50–300 words giving an accurate overview.\"\n",
    "    )\n",
    "    tags: Optional[List[str]] = Field(None, description=\"Short tags.\")\n",
    "\n",
    "\n",
    "class Reference(BaseModel):\n",
    "    # --- Fields for the COLLECTOR agent ---\n",
    "    title: str = Field(..., description=\"Title of blog/doc/tutorial/case study.\")\n",
    "    url: HttpUrl = Field(..., description=\"Link to the source.\")\n",
    "    publisher: Optional[str] = Field(None, description=\"Blog/company site.\")\n",
    "    author: Optional[str] = Field(None, description=\"Main author if available.\")\n",
    "    year: Optional[int] = Field(None, description=\"Year.\")\n",
    "\n",
    "    # --- Field for the ENRICHMENT agent ---\n",
    "    snippet: Optional[str] = Field(\n",
    "        None, \n",
    "        description=\"[ENRICHMENT] 50–300 words accurately summarizing the source's findings.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Finding(BaseModel):\n",
    "    statement: str = Field(description=\"1–2 sentence synthesized insight.\")                              # 1–2 sentences\n",
    "    supporting_sources: Optional[List[HttpUrl]] = Field(\n",
    "        None, description=\"sources backing this finding.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    id: Optional[str] = Field(None, description=\"Server-generated unique identifier.\")\n",
    "    kind: FactKind\n",
    "    content: Union[Citation, Reference, Finding, str] # Use a Union for content\n",
    "    created_at: Optional[datetime] = Field(\n",
    "        None, description=\"Server timestamp.\"\n",
    "    )\n",
    "    confidence: Optional[float] = Field(\n",
    "        None, ge=0.0, le=1.0, description=\"Self-estimated confidence of relevance 0–1.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Step(BaseModel):\n",
    "    \"\"\"A single, atomic step in a research plan.\"\"\"\n",
    "    task: str = Field(..., description=\"The specific, imperative task for the researcher to perform.\")\n",
    "    expected_output: str = Field(..., description=\"A description of the expected output from this step.\")\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"A structured research plan.\"\"\"\n",
    "    main_query: str = Field(..., description=\"A single crisp query to kick off search, summarizing the topic.\")\n",
    "    steps: List[Step] = Field(..., description=\"The list of atomic, ordered steps to execute.\")\n",
    "    success_criteria: List[str] = Field(..., description=\"A checklist of criteria to verify the research is complete.\")\n",
    "\n",
    "\n",
    "class ResearchState(AgentState): #TypedDict AgentState\n",
    "    \"\"\"The live blackboard that is passed between agents.\"\"\"\n",
    "    topic: str\n",
    "    facts: Annotated[list[Fact], operator.add]\n",
    "    plan: Plan \n",
    "\n",
    "class ResearchOutput(BaseModel):\n",
    "    \"\"\"The final, clean output of the entire research process.\"\"\"\n",
    "    topic: str\n",
    "    facts: List[Fact]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 2}\n",
      "{'x': 4}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class MyState(TypedDict):\n",
    "    x: int\n",
    "\n",
    "def my_node(state: MyState):\n",
    "    # returning a key not in MyState, it gets ignored\n",
    "    return {\"x\": state[\"x\"] + 1, \"y\": 5}\n",
    "\n",
    "builder = StateGraph(MyState)\n",
    "builder.add_node(\"bad_node\", my_node)\n",
    "builder.add_edge(START, \"bad_node\")\n",
    "builder.add_edge(\"bad_node\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# This is where you'll see the error:\n",
    "out = graph.invoke({\"x\": 1})\n",
    "print(out)\n",
    "out2 = graph.invoke({\"x\": 3})\n",
    "print(out2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071cd51",
   "metadata": {},
   "source": [
    "### Tools "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b619c",
   "metadata": {},
   "source": [
    "#### Search Tools\n",
    "STRATEGY:\n",
    "- an agent for each type of search (arxiv, web, wikipedia)\n",
    "- a supervisor researcher agent that manages those\n",
    "- a record_fact tool to push to the blackboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "265737ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
    "from langchain_core.tools import StructuredTool\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "class ArxivSearchInput(BaseModel):\n",
    "    query: str = Field(description=\"The search query for scientific papers on ArXiv.\")\n",
    "\n",
    "def arxiv_search_fn(query: str, config: RunnableConfig) -> str:\n",
    "    cfg = (config or {}).get(\"configurable\", {})\n",
    "    top_k = int(cfg.get(\"arxiv_top_k\", 3))\n",
    "    wrapper = ArxivAPIWrapper(top_k_results=top_k)\n",
    "    return wrapper.run(query)\n",
    "\n",
    "arxiv_tool = StructuredTool.from_function(\n",
    "    name=\"arxiv_search\",\n",
    "    description=\"Search ArXiv for scientific papers.\",\n",
    "    args_schema=ArxivSearchInput,\n",
    "    func=arxiv_search_fn,\n",
    "    handle_tool_error=True\n",
    ")\n",
    "\n",
    "# EXPERIMENTAL WRAPPING TO GET ELABORATE ERROR HANDLING BEHAVIOR\n",
    "\n",
    "class WikipediaSearchInput(BaseModel):\n",
    "    query: str = Field(description=\"The search query for general knowledge on Wikipedia.\")\n",
    "\n",
    "def wikipedia_search_fn(query:str , config: RunnableConfig):\n",
    "\n",
    "    cfg = (config or {}).get(\"configurable\", {})\n",
    "    top_k: int = int(cfg.get(\"wikipedia_top_k\", 3))\n",
    "    lang = cfg.get(\"wikipedia_lang\", \"en\")\n",
    "    wrapper = WikipediaAPIWrapper(top_k_results=top_k, lang=lang)\n",
    "    return wrapper.run(query)\n",
    "\n",
    "runnable_wiki_search = RunnableLambda(wikipedia_search_fn).with_retry(wait_exponential_jitter=True,stop_after_attempt=3)\n",
    "\n",
    "def wikipedia_tool_fn(query:str , config: RunnableConfig):\n",
    "\n",
    "    return runnable_wiki_search.invoke(query, config=config)\n",
    "\n",
    "wikipedia_tool = StructuredTool.from_function(\n",
    "    func=wikipedia_tool_fn,\n",
    "    name=\"wikipedia_search\",\n",
    "    description=\"Search Wikipedia for general knowledge.\",\n",
    "    args_schema=WikipediaSearchInput,\n",
    "    handle_tool_error=True\n",
    ")\n",
    "\n",
    "research_tools = [tavily_tool, arxiv_tool, wikipedia_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd3040",
   "metadata": {},
   "source": [
    "#### Add reference tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f6b89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools defined successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "\n",
    "# --- New Tool Functions ---\n",
    "@tool\n",
    "def add_scholar_reference_tool(\n",
    "    citation: Citation,\n",
    "#    state: Annotated[ResearchState, InjectedState] = None,  # injected by LangGraph, only need it if i want to inspect state\n",
    " #   tool_call_id: Annotated[str, InjectedToolCallId] = None,      # optional: for threading            \n",
    "    ) -> Command:\n",
    "    \"\"\"\n",
    "    Adds a formal academic citation (paper, preprint, etc.) to the research blackboard.\n",
    "    Call this immediately after finding a relevant scholarly article.\n",
    "    \"\"\"\n",
    "    fact = Fact(kind=FactKind.scholar_reference, content=citation)\n",
    "\n",
    "    # Update the graph state; reducer will append to `blackboard`\n",
    "    return Command(update={\"facts\": [fact]})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def add_general_reference_tool(\n",
    "    reference: Reference,\n",
    "    #state: Annotated[ResearchState, InjectedState] = None,  # injected by LangGraph, only need it if i want to inspect state\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId] = None,      # optional: for threading    \n",
    "    ) -> Command:\n",
    "    \"\"\"\n",
    "    Adds an informal web source (blog, docs, tutorial, wikipedia...) to the research blackboard.\n",
    "    Call this immediately after finding a relevant blog post, documentation, or similar article.\n",
    "    \"\"\"\n",
    "    fact = Fact(kind=FactKind.general_reference, content=reference)\n",
    "    return Command(update={\"facts\": [fact]})\n",
    "\n",
    "\n",
    "@tool \n",
    "def request_stop(reason: str = \"\") -> Command:\n",
    "    \"Call when the task is complete or no further progress is likely.\"\n",
    "    # you can also append a final message/summary if you like\n",
    "    return Command(update={\"done\": True})\n",
    "\n",
    "\n",
    "#override transfer calls\n",
    "@tool\n",
    "def transfer_to_web_agent_tool(task: str = \"\") -> Command:\n",
    "    \"Call to transfer control to the web_agent\"\n",
    "    # Reset per-worker ReAct budget on entry\n",
    "    return Command(\n",
    "        update={\"remaining_steps\": 5, \"current_task\": task},\n",
    "        goto=\"web_agent\",           # jump into the worker graph\n",
    "        graph=Command.PARENT\n",
    "    )\n",
    "\n",
    "@tool\n",
    "def transfer_to_arxiv_agent_tool(task: str = \"\") -> Command:\n",
    "    \"Call to transfer control to the arxiv_agent\"\n",
    "    # Reset per-worker ReAct budget on entry\n",
    "    return Command(\n",
    "        update={\"remaining_steps\": 5, \"current_task\": task},\n",
    "        goto=\"arxiv_agent\",           # jump into the worker graph\n",
    "        graph=Command.PARENT\n",
    "    )\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_wikipedia_agent_tool(task: str = \"\") -> Command:\n",
    "    \"Call to transfer control to the wikipedia_agent\"\n",
    "    # Reset per-worker ReAct budget on entry\n",
    "    return Command(\n",
    "        update={\"remaining_steps\": 5, \"current_task\": task},\n",
    "        goto=\"wikipedia_agent\",           # jump into the worker graph\n",
    "        graph=Command.PARENT\n",
    "    )\n",
    "print(\"Tools defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab08ff1",
   "metadata": {},
   "source": [
    "### Create Planner Chain and react Agent\n",
    "- only the roles that actually use tool require a react agent, the rest can be chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b745fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Planner chain\n",
    "\n",
    "# class Step(BaseModel):\n",
    "#     \"\"\"A single, atomic step in a research plan.\"\"\"\n",
    "#     task: str = Field(..., description=\"The specific, imperative task for the researcher to perform.\")\n",
    "#     expected_output: str = Field(..., description=\"A description of the expected output from this step.\")\n",
    "\n",
    "# class Plan(BaseModel):\n",
    "#     \"\"\"A structured research plan.\"\"\"\n",
    "#     main_query: str = Field(..., description=\"A single crisp query to kick off search, summarizing the topic.\")\n",
    "#     steps: List[Step] = Field(..., description=\"The list of atomic, ordered steps to execute.\")\n",
    "#     success_criteria: List[str] = Field(..., description=\"A checklist of criteria to verify the research is complete.\")\n",
    "\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"You are a Research Planner. Your job is to take a broad research goal and turn it into a clear, step-by-step research plan.\n",
    "\n",
    "You do not execute the research yourself — instead, you design a sequence of tasks that other specialized researcher agents will carry out.\n",
    "\n",
    "---\n",
    "\n",
    "Guidelines:\n",
    "1. **Understand the Goal**: Restate the user’s task in your own words to ensure clarity.  \n",
    "2. **Break Down the Problem**: Identify the major sub-questions or aspects that must be researched.  \n",
    "3. **Order the Steps**: Arrange tasks in a logical order (general context → deeper details → synthesis).  \n",
    "4. **Output Format**: Produce a structured plan as JSON with the following schema:\n",
    "   - `main_query`: a single concise query that captures the essence of the research task.  \n",
    "   - `steps`: a numbered list of atomic tasks. Each step should be:\n",
    "     - **Task**: short description of what to find or do.  \n",
    "     - **Expected Output**: what kind of result is expected (e.g., list of sources, factual summary, comparison table).  \n",
    "\n",
    "Do not add any supreflous steps.\n",
    "\n",
    "---\n",
    "\n",
    "Example (for the goal: “What is the impact of AI on climate modeling?”):\n",
    "```json\n",
    "{{\n",
    "  \"main_query\": \"AI in climate modeling\",\n",
    "  \"steps\": [\n",
    "    {{\n",
    "      \"Task\": \"Collect recent academic and industry publications on AI in climate science\",\n",
    "      \"Expected Output\": \"Curated list of papers with metadata\",\n",
    "    }},\n",
    "    {{\n",
    "      \"Task\": \"Summarize methodologies used in these papers\",\n",
    "      \"Expected Output\": \"Comparative summary table of approaches\",\n",
    "    }},\n",
    "    {{\n",
    "      \"Task\": \"Identify gaps and controversies in the research\",\n",
    "      \"Expected Output\": \"List of research gaps and open questions\",\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "---\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Return ONLY the structured object.\"\"\"\n",
    ") \n",
    "\n",
    "planner_llm = ChatOpenAI(model=\"gpt-5-nano\",temperature=0)\n",
    "planner_llm = planner_llm.with_structured_output(Plan)\n",
    "\n",
    "planner_chain = planner_prompt | planner_llm\n",
    "\n",
    "plan_res = planner_chain.invoke({\"topic\":\"what are the latest developements in RAG?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c57e4",
   "metadata": {},
   "source": [
    "##### Try planner-researcher chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5846fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main_query': 'Latest developments in RAG (Retriever-Augmented Generation)',\n",
      " 'steps': [{'expected_output': 'Defined scope and time window document',\n",
      "            'task': \"Identify the time window and scope for 'latest' \"\n",
      "                    'developments in RAG (e.g., past 24–36 months; include '\n",
      "                    'academic, industry, and open-source releases).'},\n",
      "           {'expected_output': 'Curated set of sources with metadata (title, '\n",
      "                               'authors, venue, date, type)',\n",
      "            'task': 'Collect recent primary sources on RAG from major venues '\n",
      "                    'and sources (papers, blogs, repos, vendor tech reports).'},\n",
      "           {'expected_output': 'Taxonomy of techniques with brief descriptions',\n",
      "            'task': 'Extract and categorize key methodological advances in RAG '\n",
      "                    '(retriever types, indexing, fusion strategies, training '\n",
      "                    'regimes).'},\n",
      "           {'expected_output': 'Comparison table of performance metrics and '\n",
      "                               'benchmarks across sources',\n",
      "            'task': 'Document improvements in system performance and '\n",
      "                    'evaluation (datasets, benchmarks, reported metrics, '\n",
      "                    'latency).'},\n",
      "           {'expected_output': 'Inventory of tools with features and maturity '\n",
      "                               'levels',\n",
      "            'task': 'Survey open-source tools and platforms implementing RAG '\n",
      "                    '(e.g., Haystack, LangChain, LlamaIndex, etc.).'},\n",
      "           {'expected_output': 'List of challenges and open questions with '\n",
      "                               'brief rationale',\n",
      "            'task': 'Identify open challenges, limitations, and areas of '\n",
      "                    'active debate in RAG research and practice.'},\n",
      "           {'expected_output': 'Executive-style summary with key takeaways and '\n",
      "                               'suggested research/practice directions',\n",
      "            'task': 'Synthesize findings into a concise landscape summary '\n",
      "                    'highlighting trends and near-term future directions.'}],\n",
      " 'success_criteria': []}\n",
      "################################################################################\n",
      "Step(task=\"Identify the time window and scope for 'latest' developments in RAG (e.g., past 24–36 months; include academic, industry, and open-source releases).\", expected_output='Defined scope and time window document')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76536/270327561.py:3: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  pprint(plan_res.dict())\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(plan_res.dict())\n",
    "print(\"#\"*80)\n",
    "pprint(plan_res.steps[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fabc32",
   "metadata": {},
   "source": [
    "### Specialized Agents and Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba9c9233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "research_llm = ChatOpenAI(model=\"gpt-5-nano\",temperature=0) #for react agent must have \"free\" llm, no structured output\n",
    "\n",
    "\n",
    "\n",
    "ARXIV_PROMPT = (\n",
    "        \"You are an academic research agent.\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"- Assist ONLY with research-related tasks.\\n\"\n",
    "        \"- For each solid paper, CALL add_scholar_reference_tool with a proper Citation.\\n\"\n",
    "        \"- After you're done with your tasks, respond to the supervisor directly\\n\" \n",
    ")\n",
    "\n",
    "arxiv_agent = create_react_agent(\n",
    "    research_llm,\n",
    "    tools=[arxiv_tool, add_scholar_reference_tool],  # search + writer\n",
    "    prompt=ARXIV_PROMPT,\n",
    "    name=\"arxiv_agent\",\n",
    ")\n",
    "\n",
    "WIKI_PROMPT = (\n",
    "\"\"\"You are a Wikipedia researcher.\\n\\n\n",
    "INSTRUCTIONS:\\n\n",
    "- Use wikipedia_search for background.\\n\n",
    "- For strong facts with a clear page+section, CALL add_general_reference_tool.\\n\n",
    "- After you're done with your tasks, respond to the supervisor directly\\n\"\"\" \n",
    ")\n",
    "\n",
    "#-When you’ve CALL add_general_reference for  5 high-quality references or cannot make further progress, call transfer_back_to_supervisor (do not write a final summary yourself).\"\"\"\n",
    "wikipedia_agent = create_react_agent(\n",
    "    research_llm,\n",
    "    tools=[wikipedia_tool, add_general_reference_tool],\n",
    "    prompt=WIKI_PROMPT,\n",
    "    name=\"wikipedia_agent\",\n",
    ")\n",
    "\n",
    "WEB_PROMPT = (\"\"\"You are a web researcher.\\n\\n\n",
    "INSTRUCTIONS:\\n\n",
    "- Use web_search (Tavily). Prefer primary sources.\\n\n",
    "- For each trustworthy source, CALL add_general_reference_tool with url+snippet.\\n\n",
    "- After you're done with your tasks, respond to the supervisor directly\\n\"\"\"\n",
    ")\n",
    "#- When you’ve CALL add_general_reference_tool for 5 high-quality references, or cannot make further progress, call transfer_back_to_supervisor (do not write a final summary yourself).\"\"\"\n",
    "web_agent = create_react_agent(\n",
    "    research_llm,\n",
    "    tools=[tavily_tool, add_general_reference_tool],\n",
    "    prompt=WEB_PROMPT,\n",
    "    name=\"web_agent\",\n",
    ")\n",
    "\n",
    "SUPERVISOR_PROMPT = (\n",
    "    \"You are a supervisor managing three agents:\\n\"\n",
    "    \"- an academic researcher agent. Assign academic research-related tasks to this agent\\n\"\n",
    "    \"- a web search agent. Assign web-search-related tasks to this agent\\n\"\n",
    "    \"- a wikipedia search agent. Assign wikipedia search-realate tasks to this agent\\n\"\n",
    "    \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n",
    "    \"Do not do any work yourself.\"\n",
    ")\n",
    "\n",
    "\n",
    "supervisor = create_supervisor(\n",
    "    model=research_llm,\n",
    "    agents=[arxiv_agent, wikipedia_agent, web_agent],\n",
    "    prompt= SUPERVISOR_PROMPT,\n",
    "    # optional: include workers' inner histories vs final messages, etc.\n",
    "    add_handoff_back_messages=True,\n",
    "    add_handoff_messages = True,\n",
    "    output_mode=\"full_history\",\n",
    "    #tools=[request_stop,transfer_to_arxiv_agent_tool,transfer_to_web_agent_tool,transfer_to_wikipedia_agent_tool]\n",
    ").compile()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "336985a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "AgentState.__annotations__.keys()\n",
    "\n",
    "from IPython.display import display, Image\n",
    "display(Image(supervisor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIELDS: ['messages']\n",
      "BLACKBOARD SIZE: 0\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Deep research plan: Find SOTA RAG papers (2024–2025), collect canonical citations and key claims; add solid Wikipedia/overview links.\n",
      "FIELDS: ['messages']\n",
      "BLACKBOARD SIZE: 0\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Deep research plan: Find SOTA RAG papers (2024–2025), collect canonical citations and key claims; add solid Wikipedia/overview links.\n",
      "FIELDS: ['messages']\n",
      "BLACKBOARD SIZE: 0\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Deep research plan: Find SOTA RAG papers (2024–2025), collect canonical citations and key claims; add solid Wikipedia/overview links.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "Tool Calls:\n",
      "  transfer_to_web_agent (call_gMGtoV8KzGJhOIKOGDy8mQJI)\n",
      " Call ID: call_gMGtoV8KzGJhOIKOGDy8mQJI\n",
      "  Args:\n",
      "FIELDS: ['messages']\n",
      "BLACKBOARD SIZE: 0\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Deep research plan: Find SOTA RAG papers (2024–2025), collect canonical citations and key claims; add solid Wikipedia/overview links.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "Tool Calls:\n",
      "  transfer_to_web_agent (call_gMGtoV8KzGJhOIKOGDy8mQJI)\n",
      " Call ID: call_gMGtoV8KzGJhOIKOGDy8mQJI\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_web_agent\n",
      "\n",
      "Successfully transferred to web_agent\n",
      "FIELDS: ['messages']\n",
      "BLACKBOARD SIZE: 0\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Deep research plan: Find SOTA RAG papers (2024–2025), collect canonical citations and key claims; add solid Wikipedia/overview links.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "Tool Calls:\n",
      "  transfer_to_web_agent (call_gMGtoV8KzGJhOIKOGDy8mQJI)\n",
      " Call ID: call_gMGtoV8KzGJhOIKOGDy8mQJI\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_web_agent\n",
      "\n",
      "Successfully transferred to web_agent\n",
      "FIELDS: ['messages']\n",
      "BLACKBOARD SIZE: 0\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Deep research plan: Find SOTA RAG papers (2024–2025), collect canonical citations and key claims; add solid Wikipedia/overview links.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "Tool Calls:\n",
      "  transfer_to_web_agent (call_gMGtoV8KzGJhOIKOGDy8mQJI)\n",
      " Call ID: call_gMGtoV8KzGJhOIKOGDy8mQJI\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_web_agent\n",
      "\n",
      "Successfully transferred to web_agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: web_agent\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_YKgyjxydNNnkjUZ7lM9Xvp3f)\n",
      " Call ID: call_YKgyjxydNNnkjUZ7lM9Xvp3f\n",
      "  Args:\n",
      "    query: 2024 2025 state-of-the-art retrieval augmented generation RAG paper\n",
      "  tavily_search_results_json (call_DooSrXIHzSIxRh85cMILFav1)\n",
      " Call ID: call_DooSrXIHzSIxRh85cMILFav1\n",
      "  Args:\n",
      "    query: RAG paper 2024 2025 retrieval augmented generation\n",
      "  tavily_search_results_json (call_A9NThq52hj8oeiS6jyVRlZYy)\n",
      " Call ID: call_A9NThq52hj8oeiS6jyVRlZYy\n",
      "  Args:\n",
      "    query: KNN-LM 2024 RAG evolution review\n",
      "  tavily_search_results_json (call_GMzfABjcS1vxJBQxzYH8IB5b)\n",
      " Call ID: call_GMzfABjcS1vxJBQxzYH8IB5b\n",
      "  Args:\n",
      "    query: Wikipedia retrieval augmented generation overview\n",
      "FIELDS: ['messages']\n",
      "BLACKBOARD SIZE: 0\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Deep research plan: Find SOTA RAG papers (2024–2025), collect canonical citations and key claims; add solid Wikipedia/overview links.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "Tool Calls:\n",
      "  transfer_to_web_agent (call_gMGtoV8KzGJhOIKOGDy8mQJI)\n",
      " Call ID: call_gMGtoV8KzGJhOIKOGDy8mQJI\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_web_agent\n",
      "\n",
      "Successfully transferred to web_agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: web_agent\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_YKgyjxydNNnkjUZ7lM9Xvp3f)\n",
      " Call ID: call_YKgyjxydNNnkjUZ7lM9Xvp3f\n",
      "  Args:\n",
      "    query: 2024 2025 state-of-the-art retrieval augmented generation RAG paper\n",
      "  tavily_search_results_json (call_DooSrXIHzSIxRh85cMILFav1)\n",
      " Call ID: call_DooSrXIHzSIxRh85cMILFav1\n",
      "  Args:\n",
      "    query: RAG paper 2024 2025 retrieval augmented generation\n",
      "  tavily_search_results_json (call_A9NThq52hj8oeiS6jyVRlZYy)\n",
      " Call ID: call_A9NThq52hj8oeiS6jyVRlZYy\n",
      "  Args:\n",
      "    query: KNN-LM 2024 RAG evolution review\n",
      "  tavily_search_results_json (call_GMzfABjcS1vxJBQxzYH8IB5b)\n",
      " Call ID: call_GMzfABjcS1vxJBQxzYH8IB5b\n",
      "  Args:\n",
      "    query: Wikipedia retrieval augmented generation overview\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"The State of RAG in 2025: Bridging Knowledge and Generative AI\", \"url\": \"https://squirro.com/squirro-blog/state-of-rag-genai\", \"content\": \"In 2025, retrieval augmented generation (RAG) is not just a solution; it's the strategic imperative addressing these core enterprise challenges head-on. At its core, the RAG architecture bridges the gap between large language models (LLMs) and the ever-expanding corpus of organizational knowledge. How? By retrieving verified, contextually relevant data at the moment of generation, ensuring AI outputs are both informed and trustworthy. [...] What is Retrieval Augmented Generation (RAG)?\\n\\nRetrieval augmented generation (RAG) offers a powerful approach for deploying accurate, reliable, and up-to-date generative AI in dynamic, data-rich enterprise environments. By retrieving relevant information in real time, RAG enables LLMs to generate accurate, context-aware responses without constant retraining. [...] Unlike generative AI powered by pre-trained and fine-tuned LLMs, which generate answers based on static training data, retrieval augmented generation grounds responses in real-time, curated, proprietary information. This enables enterprises to build systems that are not only intelligent but also compliant, secure, and scalable. In a landscape where outdated knowledge or misinformation can carry significant operational and legal risks, RAG provides the confidence layer needed for businesses in\", \"score\": 0.8655088}, {\"title\": \"A Comprehensive Survey of Retrieval-Augmented Generation (RAG)\", \"url\": \"https://arxiv.org/abs/2410.12837\", \"content\": \"We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n [Submitted on 3 Oct 2024]\\n\\nTitle:A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions\\n\\nAuthors:Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh [...] Comments:4 Figures\\nSubjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)\\nCite as:arXiv:2410.12837 [cs.CL]\\n(or arXiv:2410.12837v1 [cs.CL] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\n\\nSubmission history\\n\\n From: Rajesh Ranjan [view email] \\n\\n[v1] Thu, 3 Oct 2024 22:29:47 UTC (511 KB)\\n\\n: Evolution, Current Landscape and Future Directions, by Shailja Gupta and 2 other authors\\n\\n   View PDF\\n   Other Formats [...] > Abstract:This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant\", \"score\": 0.8377398}, {\"title\": \"RAG In 2025: State Of The Art And The Road Forward - Data Council\", \"url\": \"https://www.datacouncil.ai/talks25/rag-in-2025-state-of-the-art-and-the-road-forward?hsLang=en\", \"content\": \"Enterprise RAG Systems: Building Robust LLM Knowledge Integration | Master advanced techniques in Retrieval-Augmented Generation (RAG) for enterprise-scale language models. Learn strategies to overcome common RAG pipeline challenges including brittle parsers, suboptimal chunking, and manual query tuning. Deep dive into cutting-edge embedding models and reranking systems that enable automated, scalable knowledge retrieval. Discover practical approaches to building production-ready RAG systems\", \"score\": 0.77873385}, {\"title\": \"The State of Retrieval-Augmented Generation (RAG) in 2025 and ...\", \"url\": \"https://www.ayadata.ai/the-state-of-retrieval-augmented-generation-rag-in-2025-and-beyond/\", \"content\": \"A recent study in npj Health Systems (2025) discusses how RAG-powered AI transforms healthcare by integrating real-time diagnostic data, drug interactions, and the latest clinical research, ensuring medical decisions are based on current information.\\n\\n### RAG in Finance\\n\\nFinancial markets change by the second, so static AI models are unreliable at best. [...] Book A Demo\\n\\n Home\\n Blog\\n Generative AI\\n The State of Retrieval-Augmented Generation (RAG) in 2025 and Beyond\\n\\nBy Aya Data\\n\\n \\n10 Feb 25 \\n\\nGenerative AI\\n\\nGenerative AI writes, summarises, and even debates – but it doesn’t actually “know” anything beyond the data it was trained on. It’s a perennial problem. [...] Traditional retail recommendation engines rely on historical user behaviour, but RAG allows AI to make better suggestions by analysing real-time inventory, user reviews, and dynamic pricing data. A Forbes (2025) report revealed that a leading online retailer saw a 25% increase in customer engagement after implementing RAG-driven search and product recommendations.\", \"score\": 0.7520312}, {\"title\": \"Prospects of Retrieval Augmented Generation (RAG) for Academic ...\", \"url\": \"https://ital.corejournals.org/index.php/ital/article/view/17361\", \"content\": \"Zhenrui Yue et al., \\\"Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation,” in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) (Association for Computational Linguistics, 2024), 5628–43, \\n\\nDownloads\\n\\n   PDF\\n\\nPublished\\n\\n2025-06-16\\n\\nHow to Cite [...] Shenglai Zeng et al., “The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG),” in Findings of the Association for Computational Linguistics ACL 2024 (Association for Computational Linguistics: 2024), 4505–24, \\n\\nShubham Agarwal et al., “Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented Generation,” arXiv preprint, arXiv:2502.15734 (2025). [...] Bevara, R. V. K., Lund, B. D., Mannuru, N. R., Karedla, S. P., Mohammed, Y., Kolapudi, S. T., & Mannuru, A. (2025). Prospects of Retrieval Augmented Generation (RAG) for Academic Library Search and Retrieval. _Information Technology and Libraries_, _44_(2). \\n\\n More Citation Formats \\n\\n   ACM\\n   ACS\\n   APA\\n   ABNT\\n   Chicago\\n   Harvard\\n   IEEE\\n   MLA\\n   Turabian\\n   Vancouver\\n\\n Download Citation \\n\\n   Endnote/Zotero/Mendeley (RIS)\\n   BibTeX\\n\\nIssue\\n\\nVol. 44 No. 2 (2025)\\n\\nSection\\n\\n Articles\", \"score\": 0.7467494}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"[2409.14924] Retrieval Augmented Generation (RAG) and Beyond\", \"url\": \"https://arxiv.org/abs/2409.14924\", \"content\": \"We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n [Submitted on 23 Sep 2024]\\n\\nTitle:Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely\\n\\nAuthors:Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K. Qiu, Lili Qiu [...] Subjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI)\\nCite as:arXiv:2409.14924 [cs.CL]\\n(or arXiv:2409.14924v1 [cs.CL] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\n\\nSubmission history\\n\\n From: Siyun Zhao [view email] \\n\\n[v1] Mon, 23 Sep 2024 11:20:20 UTC (2,086 KB)\\n\\nFull-text links:\\nAccess Paper: [...] View a PDF of the paper titled Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely, by Siyun Zhao and 5 other authors\\n\\n   View PDF\\n   HTML (experimental)\\n   TeX Source\\n   Other Formats\\n\\nview license\\n\\n Current browse context: \\n\\ncs.CL\\n\\n<prev\\\") | next>\\\")\\n\\nnew | recent | 2024-09\\n\\n Change to browse by: \\n\\ncs\\n\\ncs.AI\\n\\n### References & Citations\\n\\n   NASA ADS\\n   Google Scholar\\n   Semantic Scholar\", \"score\": 0.88822687}, {\"title\": \"Retrieval-Augmented Generation (RAG): The Definitive Guide [2025]\", \"url\": \"https://www.chitika.com/retrieval-augmented-generation-rag-the-definitive-guide-2025/\", \"content\": \"One pivotal advancement in RAG by 2025 is the integration of adaptive retrieval mechanisms that dynamically adjust based on user intent and query complexity. Unlike static retrieval pipelines, these mechanisms leverage reinforcement learning to optimize the selection of external data sources in real time. This ensures that the retrieved information aligns more closely with the nuanced demands of diverse applications. [...] Recent advancements in RAG techniques have focused on adaptive retrieval mechanisms that dynamically adjust based on query complexity. For example, multi-stage retrieval pipelines now incorporate contextual re-ranking, where initial results are refined using semantic filters. This approach has shown a 15% improvement in retrieval precision for legal document analysis, as demonstrated in a 2024 study by OpenAI Labs. [...] Retrieval-Augmented Generation (RAG) has unlocked transformative potential across diverse domains by bridging static knowledge with dynamic, real-time data. In healthcare, for example, RAG systems assist clinicians by synthesizing patient-specific insights from medical literature, enabling more informed diagnoses. A 2024 study demonstrated that RAG-powered tools reduced diagnostic errors by 15% when compared to traditional AI systems.\", \"score\": 0.86928266}, {\"title\": \"10 Must-Read RAG Papers from January 2025 - Reddit\", \"url\": \"https://www.reddit.com/r/Rag/comments/1ihbn2i/10_mustread_rag_papers_from_january_2025/\", \"content\": \"7.   Enhancing RAG: Best Practices: This study explores key design factors influencing RAG systems, including query expansion, retrieval strategies, and In-Context Learning.\\n\\n8.   Chain of Retrieval Augmented Generation: This paper covers the CoRG technique that improves RAG by iteratively retrieving and reasoning over the information before generating an answer. [...] 5.   Agentic RAG: This paper covers Agentic RAG, which is the fusion of RAG with agents, improving the retrieval process with decision-making and reasoning capabilities.\\n\\n6.   TrustRAG: This is another paper that covers a security-focused framework designed to protect Retrieval-Augmented Generation (RAG) systems from corpus poisoning attacks.\", \"score\": 0.8319231}, {\"title\": \"Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\", \"url\": \"https://arxiv.org/abs/2501.09136\", \"content\": \"Published Time: Thu, 31 Jul 2025 09:58:38 GMT\\n\\n[2501.09136] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\\n\\nSkip to main content\\n\\nImage 1: Cornell University Logo\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n Submitted on 15 Jan 2025 ([v1), last revised 4 Feb 2025 (this version, v3)]\\n\\nTitle:Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG [...] Subjects:Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)\\nCite as:arXiv:2501.09136 [cs.AI]\\n(or arXiv:2501.09136v3 [cs.AI] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\n\\nSubmission history\\n\\n From: Abul Ehtesham [view email] \\n\\n( Wed, 15 Jan 2025 20:40:25 UTC (20,962 KB)\\n\\n( Mon, 3 Feb 2025 04:01:36 UTC (22,453 KB)\\n\\n[v3] Tue, 4 Feb 2025 04:48:00 UTC (22,430 KB)\\n\\n\\n   TeX Source\\n   Other Formats\\n\\nview license [...] > Abstract:Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses.\", \"score\": 0.82048416}, {\"title\": \"Retrieval Augmented Generation or Long-Context LLMs? A ...\", \"url\": \"https://aclanthology.org/2024.emnlp-industry.66/\", \"content\": \"Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 881–893, Miami, Florida, US. Association for Computational Linguistics. [...] Cite (ACL):\\n:   Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 881–893, Miami, Florida, US. Association for Computational Linguistics. [...] ```\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<modsCollection xmlns=\\\"\\n<mods ID=\\\"li-etal-2024-retrieval\\\">\\n    <titleInfo>\\n        <title>Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach</title>\\n    </titleInfo>\\n    <name type=\\\"personal\\\">\\n        <namePart type=\\\"given\\\">Zhuowan</namePart>\\n        <namePart type=\\\"family\\\">Li</namePart>\\n        <role>\\n            <roleTerm authority=\\\"marcrelator\\\" type=\\\"text\\\">author</roleTerm>\\n        </role>\\n    </name>\", \"score\": 0.76554126}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"2024: The Year of RAG (Part 1) - Medium\", \"url\": \"https://medium.com/@yu-joshua/2024-the-year-of-rag-part-1-bdf8a05f818d\", \"content\": \"## Conclusion\\n\\nIn 2024, Retrieval-Augmented Generation (RAG) has undergone transformative advancements, solidifying its role as a cornerstone of modern AI. Researchers have pushed the boundaries of RAG, enabling it to handle increasingly complex tasks, scale to massive datasets, and deliver more natural, human-like interactions. [...] From pioneering methods to innovative applications across industries, the year’s progress demonstrates the immense potential of combining retrieval and generation. As challenges such as scalability, efficiency, and contextual understanding continue to be addressed, RAG is poised to redefine how we access and interact with information, marking 2024 as a landmark year in its evolution.\\n\\nLlm\\n\\nRetrieval Augmented Gen\\n\\nGenai\\n\\nNLP\\n\\nKnowledge Graph\\n\\n## Written by Fanghua (Joshua) Yu\\n\\n1K followers [...] Graph Retrieval-Augmented Generation: A Survey discusses the integration of graph-based data into RAG systems, highlighting the potential for multi-modal information processing .\\n A Comprehensive Survey of Retrieval-Augmented Generation (RAG) provides an overview of RAG evolution, including discussions on incorporating various data modalities to enhance generation capabilities .\\n\\n### 5. Standardization of Metrics\", \"score\": 0.81595254}, {\"title\": \"Innovative Retrieval-Augmented Generation (RAG) Solutions in 2024\", \"url\": \"https://www.linkedin.com/pulse/innovative-retrieval-augmented-generation-rag-2024-sokolnicki-qvhsf\", \"content\": \"Retrieval-Augmented Generation (RAG) is a technology that combines information retrieval with text generation using advanced language models. In 2024, we are witnessing a dynamic evolution in this field, with numerous new frameworks and techniques aimed at enhancing the performance, accuracy, and scalability of RAG systems. This article will present a classification of RAG solutions, examples of the latest frameworks, and suggestions for combining these solutions to create comprehensive RAG [...] Description: Combines Amazon Bedrock with AWS Cloud Development Kit (CDK) for automated deployment of RAG solutions. Uses Amazon OpenSearch Serverless for indexing and Amazon Bedrock language models for generating responses. Use Case: Automating RAG system deployment, integrating with Amazon S3 for document storage, and utilizing advanced NLP models.\\n\\n2. Azure AI Services + Azure OpenAI [...] KNN-Search: Implements the k-nearest neighbors algorithm for efficient similarity searches in large datasets, ideal for similarity searches and analyzing large datasets.Suggested Combinations of RAG Solutions\", \"score\": 0.7982658}, {\"title\": \"The Rise and Evolution of RAG in 2024 A Year in Review - RAGFlow\", \"url\": \"https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review\", \"content\": \"The following is a summary of the technological developments in RAG throughout 2024 from both industrial and academic perspectives. RAG has been a hot topic in this year's research. Since the beginning of the year, the frequency of preprints on the topic of RAG has reached over ten papers per week, with some weeks seeing as many as several dozen. These papers primarily focus on experiments related to the applications, tuning, and evaluation of RAG, leading to various conclusions. This article [...] Therefore, the following landmark events revolve around the technical challenges of RAG.\\n\\n### The rise of multimodal document parsing tools​\\n\\n### The emergence of BM25 and hybrid search​\\n\\nThe rise of BM25 and hybrid search renders pure vector databases unnecessary as a separate category. [...] Both of these major events involve upgrades to ranking models and require native tensor support at the database level. For the first event, adopting a late interaction model effectively provides capabilities similar to reranking models at the database level. For the second event, this approach unlocks greater commercial value for more complex documents (such as magazines and pie charts) within enterprises. Based on this observation, we fully implemented these capabilities in Infinity, our\", \"score\": 0.7623715}, {\"title\": \"A Survey on Knowledge-Oriented Retrieval-Augmented Generation\", \"url\": \"https://arxiv.org/html/2503.10677v2\", \"content\": \"context-aware responses. In addition to concentrating on research pertaining to RAG technology, recent scholarship has increasingly directed its focus towards the assessment of RAG systems. A survey by Yu et al.(Yu et al., 2024a) addresses the evaluation challenges of RAG systems, providing an integrated evaluation framework and examining existing benchmarks along with their limitations. Despite covering multiple aspects of RAG, existing surveys still lack a comprehensive review that includes [...] and memory components. Section 6: RAG Evaluation(Saad-Falcon et al., 2024; Es et al., 2023; Petroni et al., 2021) focuses on the benchmarks and datasets commonly used to assess RAG systems, offering a critical review of existing evaluation practices and metrics. Section 7: RAG Applications highlights key use cases and real-world applications where RAG has demonstrated improvements, followed by Section 8: Prospects and Future Directions, which discusses emerging trends and potential areas for [...] offers a fundamental overview of RAG and discusses specific applications within the realm of text generation tasks. Recent surveys by Gao et al. and Fan et al. (Gao et al., 2024; Fan et al., 2024; Huang and Huang, 2024) explore RAG in the context of large language models, with a specific emphasis on query-oriented RAG enhancement methods. Wu et al. (Wu et al., 2024) delves into key technologies of RAG in the areas of retrieval, while simultaneously introducing its broad applications in natural\", \"score\": 0.67037153}, {\"title\": \"Retrieval Augmented Generation (RAG) for LLMs\", \"url\": \"https://www.promptingguide.ai/research/rag\", \"content\": \"In conclusion, RAG systems have evolved rapidly including the development of more advanced paradigms that enable customization and further the performance and utility of RAG across a wide range of domains. There is a huge demand for RAG applications, which has accelerated the development of methods to improve the different components of a RAG system. From hybrid methodologies to self-retrieval, these are some of the currently explored research areas of modern RAG models. There is also [...] Over the past few years, RAG systems have evolved from Naive RAG to Advanced RAG and Modular RAG. This evolution has occurred to address certain limitations around performance, cost, and efficiency.\\n\\nFigure Source (opens in a new tab)\\n\\n### Naive RAG [...] Context length: LLMs continue to extend context window size which presents challenges to how RAG needs to be adapted to ensure highly relevant and important context is captured.\\n Robustness: Dealing with counterfactual and adversarial information is important to measure and improve in RAG.\\n Hybrid approaches: There is an ongoing research effort to better understand how to best optimize the use of both RAG and fine-tuned models.\", \"score\": 0.63098115}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Retrieval-augmented generation - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\", \"content\": \"Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data [...] Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. AWS states, \\\"RAG allows LLMs to retrieve relevant information from external data sources to generate more accurate and contextually relevant responses\\\" (\\\"indexing\\\"). This approach reduces reliance on static datasets, which can quickly become outdated. When a user submits a [...] 12. ^ Jump up to: a b c d Gao, Yunfan; Xiong, Yun; Gao, Xinyu; Jia, Kangxiang; Pan, Jinliu; Bi, Yuxi; Dai, Yi; Sun, Jiawei; Wang, Meng; Wang, Haofen (2023). \\\"Retrieval-Augmented Generation for Large Language Models: A Survey\\\". arXiv \\\"ArXiv (identifier)\\\"):2312.10997 [cs.CL].\\n13. ^ Jump up to: a b Sankar, Shrinivasan (Feb 13, 2024). \\\"Retrieval Augmented Generation(RAG) — A quick and comprehensive introduction\\\". ai-bites.net.\", \"score\": 0.84707206}, {\"title\": \"Building RAG-Enhanced LLM using Wikimedia Enterprise APIs\", \"url\": \"https://enterprise.wikimedia.com/blog/build-rag-llm-wikimedia-enterprise-api/\", \"content\": \"Working with massive amounts of data and Large Language Models (LLMs) has become commonplace these days and you’ve undoubtedly heard of all the buzzwords and acronyms associated with the topic. In this article we will focus on one that will help us explain a common use-case for using Wikimedia Enterprise APIs in the AI space. RAG, or Retrieval-Augmented Generation, is a machine learning approach that integrates real-time retrieval of relevant information into the response generation process. [...] Congratulations on building your own RAG-based LLM demo application! Through this hands-on demo, we’ve explored the power of Retrieval-Augmented Generation by leveraging the Wikimedia Enterprise On-demand API’s Structured Contents endpoint. By comparing responses with and without RAG, you can see how real-time retrieval of hyper-specific content can significantly enhance the contextual accuracy and relevance of generated responses. The Wikimedia Enterprise API can be an effective solution for\", \"score\": 0.81524754}, {\"title\": \"What is RAG? - Retrieval-Augmented Generation AI Explained - AWS\", \"url\": \"https://aws.amazon.com/what-is/retrieval-augmented-generation/\", \"content\": \"Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific\", \"score\": 0.78154784}, {\"title\": \"Retrieval Augmented Generation (RAG) - Prompt Engineering Guide\", \"url\": \"https://www.promptingguide.ai/techniques/rag\", \"content\": \"Lewis et al., (2021) proposed a general-purpose fine-tuning recipe for RAG. A pre-trained seq2seq model is used as the parametric memory and a dense vector index of Wikipedia is used as non-parametric memory (accessed using a neural pre-trained retriever). Below is a overview of how the approach works:\\n\\nImage Source: Lewis et el. (2021) (opens in a new tab) [...] Meta AI researchers introduced a method called Retrieval Augmented Generation (RAG) (opens in a new tab) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model. [...] 🚀 Master Prompt Engineering and building AI Agents in our NEW courses! Use PROMPTING20 for 20% off ➜ Enroll now\\n\\nPrompting Techniques\\n\\nRetrieval Augmented Generation\\n\\n# Retrieval Augmented Generation (RAG)\\n\\nGeneral-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don't require additional background knowledge.\", \"score\": 0.76152116}, {\"title\": \"[PDF] REVerSum: A Multi-staged Retrieval-Augmented Generation ...\", \"url\": \"https://aclanthology.org/2025.coling-industry.61.pdf\", \"content\": \"Generation and section enhancement: We use several state-of-the-art large language models (LLM) to perform text generation. This generated text can be appended to an existing Wikipedia sec-tion. First, we carefully design a prompt which consist of two inputs - (1) the existing content of the Wikipedia section, (2) retrieved context (top k chunks relevant to retrieval query) and an instruc-tion. The exact prompt can be found in Table 5 of Appendix B. [...] Jiebin Zhang, Eugene J. Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xi-aoguang Li, Qun Liu, and Sujian Li. 2024. Retrieval-based full-length wikipedia generation for emergent events. Preprint, arXiv:2402.18264.\\n741 Appendices A Details of personal narratives The details of the personal narratives collected and the corresponding statistics are provided in Table 4. [...] By integrating rich, first-hand information from personal narratives, we aim to provide more comprehensive and accurate con-tent. We presents a scalable solution for improv-ing Wikipedia content quality, directly benefiting industries that rely on accurate knowledge bases, such as education, media, and digital libraries. Our contributions are as follows: • We propose a novel multi-staged approach REVERSUM to incorporate personal narra-tives, such as autobiographies and biographies to enhance\", \"score\": 0.7334523}]\n",
      "FIELDS: ['messages']\n",
      "BLACKBOARD SIZE: 0\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Deep research plan: Find SOTA RAG papers (2024–2025), collect canonical citations and key claims; add solid Wikipedia/overview links.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: supervisor\n",
      "Tool Calls:\n",
      "  transfer_to_web_agent (call_gMGtoV8KzGJhOIKOGDy8mQJI)\n",
      " Call ID: call_gMGtoV8KzGJhOIKOGDy8mQJI\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: transfer_to_web_agent\n",
      "\n",
      "Successfully transferred to web_agent\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: web_agent\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_YKgyjxydNNnkjUZ7lM9Xvp3f)\n",
      " Call ID: call_YKgyjxydNNnkjUZ7lM9Xvp3f\n",
      "  Args:\n",
      "    query: 2024 2025 state-of-the-art retrieval augmented generation RAG paper\n",
      "  tavily_search_results_json (call_DooSrXIHzSIxRh85cMILFav1)\n",
      " Call ID: call_DooSrXIHzSIxRh85cMILFav1\n",
      "  Args:\n",
      "    query: RAG paper 2024 2025 retrieval augmented generation\n",
      "  tavily_search_results_json (call_A9NThq52hj8oeiS6jyVRlZYy)\n",
      " Call ID: call_A9NThq52hj8oeiS6jyVRlZYy\n",
      "  Args:\n",
      "    query: KNN-LM 2024 RAG evolution review\n",
      "  tavily_search_results_json (call_GMzfABjcS1vxJBQxzYH8IB5b)\n",
      " Call ID: call_GMzfABjcS1vxJBQxzYH8IB5b\n",
      "  Args:\n",
      "    query: Wikipedia retrieval augmented generation overview\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"The State of RAG in 2025: Bridging Knowledge and Generative AI\", \"url\": \"https://squirro.com/squirro-blog/state-of-rag-genai\", \"content\": \"In 2025, retrieval augmented generation (RAG) is not just a solution; it's the strategic imperative addressing these core enterprise challenges head-on. At its core, the RAG architecture bridges the gap between large language models (LLMs) and the ever-expanding corpus of organizational knowledge. How? By retrieving verified, contextually relevant data at the moment of generation, ensuring AI outputs are both informed and trustworthy. [...] What is Retrieval Augmented Generation (RAG)?\\n\\nRetrieval augmented generation (RAG) offers a powerful approach for deploying accurate, reliable, and up-to-date generative AI in dynamic, data-rich enterprise environments. By retrieving relevant information in real time, RAG enables LLMs to generate accurate, context-aware responses without constant retraining. [...] Unlike generative AI powered by pre-trained and fine-tuned LLMs, which generate answers based on static training data, retrieval augmented generation grounds responses in real-time, curated, proprietary information. This enables enterprises to build systems that are not only intelligent but also compliant, secure, and scalable. In a landscape where outdated knowledge or misinformation can carry significant operational and legal risks, RAG provides the confidence layer needed for businesses in\", \"score\": 0.8655088}, {\"title\": \"A Comprehensive Survey of Retrieval-Augmented Generation (RAG)\", \"url\": \"https://arxiv.org/abs/2410.12837\", \"content\": \"We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n [Submitted on 3 Oct 2024]\\n\\nTitle:A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions\\n\\nAuthors:Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh [...] Comments:4 Figures\\nSubjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)\\nCite as:arXiv:2410.12837 [cs.CL]\\n(or arXiv:2410.12837v1 [cs.CL] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\n\\nSubmission history\\n\\n From: Rajesh Ranjan [view email] \\n\\n[v1] Thu, 3 Oct 2024 22:29:47 UTC (511 KB)\\n\\n: Evolution, Current Landscape and Future Directions, by Shailja Gupta and 2 other authors\\n\\n   View PDF\\n   Other Formats [...] > Abstract:This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant\", \"score\": 0.8377398}, {\"title\": \"RAG In 2025: State Of The Art And The Road Forward - Data Council\", \"url\": \"https://www.datacouncil.ai/talks25/rag-in-2025-state-of-the-art-and-the-road-forward?hsLang=en\", \"content\": \"Enterprise RAG Systems: Building Robust LLM Knowledge Integration | Master advanced techniques in Retrieval-Augmented Generation (RAG) for enterprise-scale language models. Learn strategies to overcome common RAG pipeline challenges including brittle parsers, suboptimal chunking, and manual query tuning. Deep dive into cutting-edge embedding models and reranking systems that enable automated, scalable knowledge retrieval. Discover practical approaches to building production-ready RAG systems\", \"score\": 0.77873385}, {\"title\": \"The State of Retrieval-Augmented Generation (RAG) in 2025 and ...\", \"url\": \"https://www.ayadata.ai/the-state-of-retrieval-augmented-generation-rag-in-2025-and-beyond/\", \"content\": \"A recent study in npj Health Systems (2025) discusses how RAG-powered AI transforms healthcare by integrating real-time diagnostic data, drug interactions, and the latest clinical research, ensuring medical decisions are based on current information.\\n\\n### RAG in Finance\\n\\nFinancial markets change by the second, so static AI models are unreliable at best. [...] Book A Demo\\n\\n Home\\n Blog\\n Generative AI\\n The State of Retrieval-Augmented Generation (RAG) in 2025 and Beyond\\n\\nBy Aya Data\\n\\n \\n10 Feb 25 \\n\\nGenerative AI\\n\\nGenerative AI writes, summarises, and even debates – but it doesn’t actually “know” anything beyond the data it was trained on. It’s a perennial problem. [...] Traditional retail recommendation engines rely on historical user behaviour, but RAG allows AI to make better suggestions by analysing real-time inventory, user reviews, and dynamic pricing data. A Forbes (2025) report revealed that a leading online retailer saw a 25% increase in customer engagement after implementing RAG-driven search and product recommendations.\", \"score\": 0.7520312}, {\"title\": \"Prospects of Retrieval Augmented Generation (RAG) for Academic ...\", \"url\": \"https://ital.corejournals.org/index.php/ital/article/view/17361\", \"content\": \"Zhenrui Yue et al., \\\"Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation,” in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) (Association for Computational Linguistics, 2024), 5628–43, \\n\\nDownloads\\n\\n   PDF\\n\\nPublished\\n\\n2025-06-16\\n\\nHow to Cite [...] Shenglai Zeng et al., “The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG),” in Findings of the Association for Computational Linguistics ACL 2024 (Association for Computational Linguistics: 2024), 4505–24, \\n\\nShubham Agarwal et al., “Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented Generation,” arXiv preprint, arXiv:2502.15734 (2025). [...] Bevara, R. V. K., Lund, B. D., Mannuru, N. R., Karedla, S. P., Mohammed, Y., Kolapudi, S. T., & Mannuru, A. (2025). Prospects of Retrieval Augmented Generation (RAG) for Academic Library Search and Retrieval. _Information Technology and Libraries_, _44_(2). \\n\\n More Citation Formats \\n\\n   ACM\\n   ACS\\n   APA\\n   ABNT\\n   Chicago\\n   Harvard\\n   IEEE\\n   MLA\\n   Turabian\\n   Vancouver\\n\\n Download Citation \\n\\n   Endnote/Zotero/Mendeley (RIS)\\n   BibTeX\\n\\nIssue\\n\\nVol. 44 No. 2 (2025)\\n\\nSection\\n\\n Articles\", \"score\": 0.7467494}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"[2409.14924] Retrieval Augmented Generation (RAG) and Beyond\", \"url\": \"https://arxiv.org/abs/2409.14924\", \"content\": \"We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n [Submitted on 23 Sep 2024]\\n\\nTitle:Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely\\n\\nAuthors:Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K. Qiu, Lili Qiu [...] Subjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI)\\nCite as:arXiv:2409.14924 [cs.CL]\\n(or arXiv:2409.14924v1 [cs.CL] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\n\\nSubmission history\\n\\n From: Siyun Zhao [view email] \\n\\n[v1] Mon, 23 Sep 2024 11:20:20 UTC (2,086 KB)\\n\\nFull-text links:\\nAccess Paper: [...] View a PDF of the paper titled Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely, by Siyun Zhao and 5 other authors\\n\\n   View PDF\\n   HTML (experimental)\\n   TeX Source\\n   Other Formats\\n\\nview license\\n\\n Current browse context: \\n\\ncs.CL\\n\\n<prev\\\") | next>\\\")\\n\\nnew | recent | 2024-09\\n\\n Change to browse by: \\n\\ncs\\n\\ncs.AI\\n\\n### References & Citations\\n\\n   NASA ADS\\n   Google Scholar\\n   Semantic Scholar\", \"score\": 0.88822687}, {\"title\": \"Retrieval-Augmented Generation (RAG): The Definitive Guide [2025]\", \"url\": \"https://www.chitika.com/retrieval-augmented-generation-rag-the-definitive-guide-2025/\", \"content\": \"One pivotal advancement in RAG by 2025 is the integration of adaptive retrieval mechanisms that dynamically adjust based on user intent and query complexity. Unlike static retrieval pipelines, these mechanisms leverage reinforcement learning to optimize the selection of external data sources in real time. This ensures that the retrieved information aligns more closely with the nuanced demands of diverse applications. [...] Recent advancements in RAG techniques have focused on adaptive retrieval mechanisms that dynamically adjust based on query complexity. For example, multi-stage retrieval pipelines now incorporate contextual re-ranking, where initial results are refined using semantic filters. This approach has shown a 15% improvement in retrieval precision for legal document analysis, as demonstrated in a 2024 study by OpenAI Labs. [...] Retrieval-Augmented Generation (RAG) has unlocked transformative potential across diverse domains by bridging static knowledge with dynamic, real-time data. In healthcare, for example, RAG systems assist clinicians by synthesizing patient-specific insights from medical literature, enabling more informed diagnoses. A 2024 study demonstrated that RAG-powered tools reduced diagnostic errors by 15% when compared to traditional AI systems.\", \"score\": 0.86928266}, {\"title\": \"10 Must-Read RAG Papers from January 2025 - Reddit\", \"url\": \"https://www.reddit.com/r/Rag/comments/1ihbn2i/10_mustread_rag_papers_from_january_2025/\", \"content\": \"7.   Enhancing RAG: Best Practices: This study explores key design factors influencing RAG systems, including query expansion, retrieval strategies, and In-Context Learning.\\n\\n8.   Chain of Retrieval Augmented Generation: This paper covers the CoRG technique that improves RAG by iteratively retrieving and reasoning over the information before generating an answer. [...] 5.   Agentic RAG: This paper covers Agentic RAG, which is the fusion of RAG with agents, improving the retrieval process with decision-making and reasoning capabilities.\\n\\n6.   TrustRAG: This is another paper that covers a security-focused framework designed to protect Retrieval-Augmented Generation (RAG) systems from corpus poisoning attacks.\", \"score\": 0.8319231}, {\"title\": \"Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\", \"url\": \"https://arxiv.org/abs/2501.09136\", \"content\": \"Published Time: Thu, 31 Jul 2025 09:58:38 GMT\\n\\n[2501.09136] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\\n\\nSkip to main content\\n\\nImage 1: Cornell University Logo\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n Submitted on 15 Jan 2025 ([v1), last revised 4 Feb 2025 (this version, v3)]\\n\\nTitle:Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG [...] Subjects:Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)\\nCite as:arXiv:2501.09136 [cs.AI]\\n(or arXiv:2501.09136v3 [cs.AI] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\n\\nSubmission history\\n\\n From: Abul Ehtesham [view email] \\n\\n( Wed, 15 Jan 2025 20:40:25 UTC (20,962 KB)\\n\\n( Mon, 3 Feb 2025 04:01:36 UTC (22,453 KB)\\n\\n[v3] Tue, 4 Feb 2025 04:48:00 UTC (22,430 KB)\\n\\n\\n   TeX Source\\n   Other Formats\\n\\nview license [...] > Abstract:Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses.\", \"score\": 0.82048416}, {\"title\": \"Retrieval Augmented Generation or Long-Context LLMs? A ...\", \"url\": \"https://aclanthology.org/2024.emnlp-industry.66/\", \"content\": \"Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 881–893, Miami, Florida, US. Association for Computational Linguistics. [...] Cite (ACL):\\n:   Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 881–893, Miami, Florida, US. Association for Computational Linguistics. [...] ```\\n<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\\n<modsCollection xmlns=\\\"\\n<mods ID=\\\"li-etal-2024-retrieval\\\">\\n    <titleInfo>\\n        <title>Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach</title>\\n    </titleInfo>\\n    <name type=\\\"personal\\\">\\n        <namePart type=\\\"given\\\">Zhuowan</namePart>\\n        <namePart type=\\\"family\\\">Li</namePart>\\n        <role>\\n            <roleTerm authority=\\\"marcrelator\\\" type=\\\"text\\\">author</roleTerm>\\n        </role>\\n    </name>\", \"score\": 0.76554126}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"2024: The Year of RAG (Part 1) - Medium\", \"url\": \"https://medium.com/@yu-joshua/2024-the-year-of-rag-part-1-bdf8a05f818d\", \"content\": \"## Conclusion\\n\\nIn 2024, Retrieval-Augmented Generation (RAG) has undergone transformative advancements, solidifying its role as a cornerstone of modern AI. Researchers have pushed the boundaries of RAG, enabling it to handle increasingly complex tasks, scale to massive datasets, and deliver more natural, human-like interactions. [...] From pioneering methods to innovative applications across industries, the year’s progress demonstrates the immense potential of combining retrieval and generation. As challenges such as scalability, efficiency, and contextual understanding continue to be addressed, RAG is poised to redefine how we access and interact with information, marking 2024 as a landmark year in its evolution.\\n\\nLlm\\n\\nRetrieval Augmented Gen\\n\\nGenai\\n\\nNLP\\n\\nKnowledge Graph\\n\\n## Written by Fanghua (Joshua) Yu\\n\\n1K followers [...] Graph Retrieval-Augmented Generation: A Survey discusses the integration of graph-based data into RAG systems, highlighting the potential for multi-modal information processing .\\n A Comprehensive Survey of Retrieval-Augmented Generation (RAG) provides an overview of RAG evolution, including discussions on incorporating various data modalities to enhance generation capabilities .\\n\\n### 5. Standardization of Metrics\", \"score\": 0.81595254}, {\"title\": \"Innovative Retrieval-Augmented Generation (RAG) Solutions in 2024\", \"url\": \"https://www.linkedin.com/pulse/innovative-retrieval-augmented-generation-rag-2024-sokolnicki-qvhsf\", \"content\": \"Retrieval-Augmented Generation (RAG) is a technology that combines information retrieval with text generation using advanced language models. In 2024, we are witnessing a dynamic evolution in this field, with numerous new frameworks and techniques aimed at enhancing the performance, accuracy, and scalability of RAG systems. This article will present a classification of RAG solutions, examples of the latest frameworks, and suggestions for combining these solutions to create comprehensive RAG [...] Description: Combines Amazon Bedrock with AWS Cloud Development Kit (CDK) for automated deployment of RAG solutions. Uses Amazon OpenSearch Serverless for indexing and Amazon Bedrock language models for generating responses. Use Case: Automating RAG system deployment, integrating with Amazon S3 for document storage, and utilizing advanced NLP models.\\n\\n2. Azure AI Services + Azure OpenAI [...] KNN-Search: Implements the k-nearest neighbors algorithm for efficient similarity searches in large datasets, ideal for similarity searches and analyzing large datasets.Suggested Combinations of RAG Solutions\", \"score\": 0.7982658}, {\"title\": \"The Rise and Evolution of RAG in 2024 A Year in Review - RAGFlow\", \"url\": \"https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review\", \"content\": \"The following is a summary of the technological developments in RAG throughout 2024 from both industrial and academic perspectives. RAG has been a hot topic in this year's research. Since the beginning of the year, the frequency of preprints on the topic of RAG has reached over ten papers per week, with some weeks seeing as many as several dozen. These papers primarily focus on experiments related to the applications, tuning, and evaluation of RAG, leading to various conclusions. This article [...] Therefore, the following landmark events revolve around the technical challenges of RAG.\\n\\n### The rise of multimodal document parsing tools​\\n\\n### The emergence of BM25 and hybrid search​\\n\\nThe rise of BM25 and hybrid search renders pure vector databases unnecessary as a separate category. [...] Both of these major events involve upgrades to ranking models and require native tensor support at the database level. For the first event, adopting a late interaction model effectively provides capabilities similar to reranking models at the database level. For the second event, this approach unlocks greater commercial value for more complex documents (such as magazines and pie charts) within enterprises. Based on this observation, we fully implemented these capabilities in Infinity, our\", \"score\": 0.7623715}, {\"title\": \"A Survey on Knowledge-Oriented Retrieval-Augmented Generation\", \"url\": \"https://arxiv.org/html/2503.10677v2\", \"content\": \"context-aware responses. In addition to concentrating on research pertaining to RAG technology, recent scholarship has increasingly directed its focus towards the assessment of RAG systems. A survey by Yu et al.(Yu et al., 2024a) addresses the evaluation challenges of RAG systems, providing an integrated evaluation framework and examining existing benchmarks along with their limitations. Despite covering multiple aspects of RAG, existing surveys still lack a comprehensive review that includes [...] and memory components. Section 6: RAG Evaluation(Saad-Falcon et al., 2024; Es et al., 2023; Petroni et al., 2021) focuses on the benchmarks and datasets commonly used to assess RAG systems, offering a critical review of existing evaluation practices and metrics. Section 7: RAG Applications highlights key use cases and real-world applications where RAG has demonstrated improvements, followed by Section 8: Prospects and Future Directions, which discusses emerging trends and potential areas for [...] offers a fundamental overview of RAG and discusses specific applications within the realm of text generation tasks. Recent surveys by Gao et al. and Fan et al. (Gao et al., 2024; Fan et al., 2024; Huang and Huang, 2024) explore RAG in the context of large language models, with a specific emphasis on query-oriented RAG enhancement methods. Wu et al. (Wu et al., 2024) delves into key technologies of RAG in the areas of retrieval, while simultaneously introducing its broad applications in natural\", \"score\": 0.67037153}, {\"title\": \"Retrieval Augmented Generation (RAG) for LLMs\", \"url\": \"https://www.promptingguide.ai/research/rag\", \"content\": \"In conclusion, RAG systems have evolved rapidly including the development of more advanced paradigms that enable customization and further the performance and utility of RAG across a wide range of domains. There is a huge demand for RAG applications, which has accelerated the development of methods to improve the different components of a RAG system. From hybrid methodologies to self-retrieval, these are some of the currently explored research areas of modern RAG models. There is also [...] Over the past few years, RAG systems have evolved from Naive RAG to Advanced RAG and Modular RAG. This evolution has occurred to address certain limitations around performance, cost, and efficiency.\\n\\nFigure Source (opens in a new tab)\\n\\n### Naive RAG [...] Context length: LLMs continue to extend context window size which presents challenges to how RAG needs to be adapted to ensure highly relevant and important context is captured.\\n Robustness: Dealing with counterfactual and adversarial information is important to measure and improve in RAG.\\n Hybrid approaches: There is an ongoing research effort to better understand how to best optimize the use of both RAG and fine-tuned models.\", \"score\": 0.63098115}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Retrieval-augmented generation - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\", \"content\": \"Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data [...] Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. AWS states, \\\"RAG allows LLMs to retrieve relevant information from external data sources to generate more accurate and contextually relevant responses\\\" (\\\"indexing\\\"). This approach reduces reliance on static datasets, which can quickly become outdated. When a user submits a [...] 12. ^ Jump up to: a b c d Gao, Yunfan; Xiong, Yun; Gao, Xinyu; Jia, Kangxiang; Pan, Jinliu; Bi, Yuxi; Dai, Yi; Sun, Jiawei; Wang, Meng; Wang, Haofen (2023). \\\"Retrieval-Augmented Generation for Large Language Models: A Survey\\\". arXiv \\\"ArXiv (identifier)\\\"):2312.10997 [cs.CL].\\n13. ^ Jump up to: a b Sankar, Shrinivasan (Feb 13, 2024). \\\"Retrieval Augmented Generation(RAG) — A quick and comprehensive introduction\\\". ai-bites.net.\", \"score\": 0.84707206}, {\"title\": \"Building RAG-Enhanced LLM using Wikimedia Enterprise APIs\", \"url\": \"https://enterprise.wikimedia.com/blog/build-rag-llm-wikimedia-enterprise-api/\", \"content\": \"Working with massive amounts of data and Large Language Models (LLMs) has become commonplace these days and you’ve undoubtedly heard of all the buzzwords and acronyms associated with the topic. In this article we will focus on one that will help us explain a common use-case for using Wikimedia Enterprise APIs in the AI space. RAG, or Retrieval-Augmented Generation, is a machine learning approach that integrates real-time retrieval of relevant information into the response generation process. [...] Congratulations on building your own RAG-based LLM demo application! Through this hands-on demo, we’ve explored the power of Retrieval-Augmented Generation by leveraging the Wikimedia Enterprise On-demand API’s Structured Contents endpoint. By comparing responses with and without RAG, you can see how real-time retrieval of hyper-specific content can significantly enhance the contextual accuracy and relevance of generated responses. The Wikimedia Enterprise API can be an effective solution for\", \"score\": 0.81524754}, {\"title\": \"What is RAG? - Retrieval-Augmented Generation AI Explained - AWS\", \"url\": \"https://aws.amazon.com/what-is/retrieval-augmented-generation/\", \"content\": \"Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific\", \"score\": 0.78154784}, {\"title\": \"Retrieval Augmented Generation (RAG) - Prompt Engineering Guide\", \"url\": \"https://www.promptingguide.ai/techniques/rag\", \"content\": \"Lewis et al., (2021) proposed a general-purpose fine-tuning recipe for RAG. A pre-trained seq2seq model is used as the parametric memory and a dense vector index of Wikipedia is used as non-parametric memory (accessed using a neural pre-trained retriever). Below is a overview of how the approach works:\\n\\nImage Source: Lewis et el. (2021) (opens in a new tab) [...] Meta AI researchers introduced a method called Retrieval Augmented Generation (RAG) (opens in a new tab) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model. [...] 🚀 Master Prompt Engineering and building AI Agents in our NEW courses! Use PROMPTING20 for 20% off ➜ Enroll now\\n\\nPrompting Techniques\\n\\nRetrieval Augmented Generation\\n\\n# Retrieval Augmented Generation (RAG)\\n\\nGeneral-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don't require additional background knowledge.\", \"score\": 0.76152116}, {\"title\": \"[PDF] REVerSum: A Multi-staged Retrieval-Augmented Generation ...\", \"url\": \"https://aclanthology.org/2025.coling-industry.61.pdf\", \"content\": \"Generation and section enhancement: We use several state-of-the-art large language models (LLM) to perform text generation. This generated text can be appended to an existing Wikipedia sec-tion. First, we carefully design a prompt which consist of two inputs - (1) the existing content of the Wikipedia section, (2) retrieved context (top k chunks relevant to retrieval query) and an instruc-tion. The exact prompt can be found in Table 5 of Appendix B. [...] Jiebin Zhang, Eugene J. Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xi-aoguang Li, Qun Liu, and Sujian Li. 2024. Retrieval-based full-length wikipedia generation for emergent events. Preprint, arXiv:2402.18264.\\n741 Appendices A Details of personal narratives The details of the personal narratives collected and the corresponding statistics are provided in Table 4. [...] By integrating rich, first-hand information from personal narratives, we aim to provide more comprehensive and accurate con-tent. We presents a scalable solution for improv-ing Wikipedia content quality, directly benefiting industries that rely on accurate knowledge bases, such as education, media, and digital libraries. Our contributions are as follows: • We propose a novel multi-staged approach REVERSUM to incorporate personal narra-tives, such as autobiographies and biographies to enhance\", \"score\": 0.7334523}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: web_agent\n",
      "Tool Calls:\n",
      "  add_general_reference_tool (call_o39ejpGOo47FUV961p95kzUD)\n",
      " Call ID: call_o39ejpGOo47FUV961p95kzUD\n",
      "  Args:\n",
      "    reference: {'title': 'A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions', 'url': 'https://arxiv.org/abs/2410.12837', 'publisher': 'arXiv', 'author': 'Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh, et al.', 'year': 2024, 'snippet': 'This comprehensive survey traces Retrieval-Augmented Generation from foundational concepts to the state-of-the-art as of 2024, detailing RAG architectures (retrieve-then-generate, hybrid, memory-based variants), retrieval strategies (BM25, dense vector indices, reranking), data sources, evaluation benchmarks, and open challenges with directions for future research.'}\n",
      "  add_general_reference_tool (call_MFssTi07CiCfREz45aSQMaFK)\n",
      " Call ID: call_MFssTi07CiCfREz45aSQMaFK\n",
      "  Args:\n",
      "    reference: {'title': 'Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely', 'url': 'https://arxiv.org/abs/2409.14924', 'publisher': 'arXiv', 'author': 'Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K. Qiu, Lili Qiu', 'year': 2024, 'snippet': 'Surveys methods to use external data effectively in RAG, including retrieval architectures, data sources, evaluation, and practical design choices (retrieval scope, memory management, chunking).'}\n",
      "  add_general_reference_tool (call_zoufUJEfa31IRsvKqVH7Tqs9)\n",
      " Call ID: call_zoufUJEfa31IRsvKqVH7Tqs9\n",
      "  Args:\n",
      "    reference: {'title': 'Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach', 'url': 'https://aclanthology.org/2024.emnlp-industry.66/', 'publisher': 'ACL Anthology / EMNLP Industry Track 2024', 'author': 'Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky', 'year': 2024, 'snippet': 'Industry-track study comparing RAG with long-context LLMs; proposes a hybrid approach that combines RAG with long-context strategies for improved performance.'}\n",
      "  add_general_reference_tool (call_mC1jMlwtQTpXZxY8lJkjKZD8)\n",
      " Call ID: call_mC1jMlwtQTpXZxY8lJkjKZD8\n",
      "  Args:\n",
      "    reference: {'title': 'REVerSum: A Multi-staged Retrieval-Augmented Generation for Wikipedia Content Enhancement', 'url': 'https://aclanthology.org/2025.coling-industry.61.pdf', 'publisher': 'ACL Anthology / COLING 2025 Industry Track', 'author': 'Jiebin Zhang, Eugene J. Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xi-aoguang Li, Qun Liu, Sujian Li', 'year': 2025, 'snippet': 'Introduces REVERSUM, a multi-staged retrieval-augmented generation framework to enhance Wikipedia sections by integrating retrieved context and generation; includes experiments on Wikipedia content quality.'}\n",
      "  add_general_reference_tool (call_qEuGjAa4Iz7qmGBLaoCUijOW)\n",
      " Call ID: call_qEuGjAa4Iz7qmGBLaoCUijOW\n",
      "  Args:\n",
      "    reference: {'title': 'Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG', 'url': 'https://arxiv.org/abs/2501.09136', 'publisher': 'arXiv', 'author': 'Abul Ehtesham, et al.', 'year': 2025, 'snippet': 'Presents a survey on Agentic RAG, where retrieval is augmented with agentic control (planning, action execution) to improve retrieval quality and task success.'}\n",
      "  add_general_reference_tool (call_51fhA8GUhzyzXG5u8JMvQE7G)\n",
      " Call ID: call_51fhA8GUhzyzXG5u8JMvQE7G\n",
      "  Args:\n",
      "    reference: {'title': 'A Survey on Knowledge-Oriented Retrieval-Augmented Generation', 'url': 'https://arxiv.org/abs/2503.10677', 'publisher': 'arXiv', 'author': 'Unknown; Saad-Falcon et al.', 'year': 2025, 'snippet': 'Reviews knowledge-oriented RAG, focusing on knowledge integration, memory, and evaluation, and outlining future directions.'}\n",
      "  add_general_reference_tool (call_YekAsKtWrzCiwb0JtjMJg4mU)\n",
      " Call ID: call_YekAsKtWrzCiwb0JtjMJg4mU\n",
      "  Args:\n",
      "    reference: {'title': 'Retrieval-augmented generation - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Retrieval-augmented_generation', 'publisher': 'Wikipedia', 'author': None, 'year': 2023, 'snippet': 'Overview of Retrieval-Augmented Generation (RAG): concept, architecture variants (RAG-Token, RAG-Sequence), data sources, and typical evaluation.'}\n",
      "  add_general_reference_tool (call_NqKxxN7rrcvchDnypb4MqbE7)\n",
      " Call ID: call_NqKxxN7rrcvchDnypb4MqbE7\n",
      "  Args:\n",
      "    reference: {'title': 'Building RAG-Enhanced LLM using Wikimedia Enterprise APIs', 'url': 'https://enterprise.wikimedia.com/blog/build-rag-llm-wikimedia-enterprise-api/', 'publisher': 'Wikimedia Enterprise', 'author': 'Wikimedia Enterprise Team', 'year': 2024, 'snippet': 'Hands-on tutorial showing how to build a RAG-enabled LLM using Wikimedia Enterprise APIs, including indexing and retrieval steps.'}\n",
      "  add_general_reference_tool (call_UYstQR5TZ502crbwrntzcL0X)\n",
      " Call ID: call_UYstQR5TZ502crbwrntzcL0X\n",
      "  Args:\n",
      "    reference: {'title': 'What is RAG? - Retrieval-Augmented Generation AI Explained - AWS', 'url': 'https://aws.amazon.com/what-is/retrieval-augmented-generation/', 'publisher': 'Amazon Web Services (AWS)', 'author': 'AWS', 'year': 2024, 'snippet': 'AWS overview defining Retrieval-Augmented Generation and how retrieval of external data sources improves accuracy and relevance of generated responses.'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected to have a matching ToolMessage in Command.update for tool 'add_general_reference_tool', got: []. Every tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage. You can fix it by modifying the tool to return `Command(update=[ToolMessage(\"Success\", tool_call_id=tool_call_id), ...], ...)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      5\u001b[39m init_state = {\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: query}],\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfacts\u001b[39m\u001b[33m\"\u001b[39m: [],           \u001b[38;5;66;03m# seed your reducer fields\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mblackboard\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdone\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     10\u001b[39m }\n\u001b[32m     12\u001b[39m last_parent_state = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupervisor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecursion_limit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# full state\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# include parent + child snapshots\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ns is a tuple path like (\"supervisor:<id>\",) or (\"supervisor:<id>\", \"web_agent:<id>\")\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(\"STATE FROM:\", \" > \".join(ns))\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFIELDS:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBLACKBOARD SIZE:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mblackboard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/_runner.py:253\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/_runner.py:511\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    509\u001b[39m                 interrupts.append(exc)\n\u001b[32m    510\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/_executor.py:81\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:394\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    396\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph_supervisor/supervisor.py:103\u001b[39m, in \u001b[36m_make_call_agent.<locals>.call_agent\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_agent\u001b[39m(state: \u001b[38;5;28mdict\u001b[39m, config: RunnableConfig) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m    102\u001b[39m     thread_id = config.get(\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m, {}).get(\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     output = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_configurable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthread_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muuid5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUUID\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRemoteGraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _process_output(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/main.py:3026\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3024\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3032\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/_runner.py:253\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/_runner.py:511\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    509\u001b[39m                 interrupts.append(exc)\n\u001b[32m    510\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/_executor.py:81\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:367\u001b[39m, in \u001b[36mToolNode._func\u001b[39m\u001b[34m(self, input, config, store)\u001b[39m\n\u001b[32m    365\u001b[39m input_types = [input_type] * \u001b[38;5;28mlen\u001b[39m(tool_calls)\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     outputs = [\n\u001b[32m    368\u001b[39m         *executor.map(\u001b[38;5;28mself\u001b[39m._run_one, tool_calls, input_types, config_list)\n\u001b[32m    369\u001b[39m     ]\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._combine_tool_outputs(outputs, input_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langchain_core/runnables/config.py:556\u001b[39m, in \u001b[36mContextThreadPoolExecutor.map.<locals>._wrapped_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_fn\u001b[39m(*args: Any) -> T:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontexts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:480\u001b[39m, in \u001b[36mToolNode._run_one\u001b[39m\u001b[34m(self, call, input_type, config)\u001b[39m\n\u001b[32m    472\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ToolMessage(\n\u001b[32m    473\u001b[39m         content=content,\n\u001b[32m    474\u001b[39m         name=call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    475\u001b[39m         tool_call_id=call[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    476\u001b[39m         status=\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    477\u001b[39m     )\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, Command):\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_tool_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ToolMessage):\n\u001b[32m    482\u001b[39m     response.content = cast(\n\u001b[32m    483\u001b[39m         Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m], msg_content_output(response.content)\n\u001b[32m    484\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:769\u001b[39m, in \u001b[36mToolNode._validate_tool_command\u001b[39m\u001b[34m(self, command, call, input_type)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m updated_command.graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_matching_tool_message:\n\u001b[32m    764\u001b[39m     example_update = (\n\u001b[32m    765\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m`Command(update=\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m: [ToolMessage(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccess\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, tool_call_id=tool_call_id), ...]}, ...)`\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    766\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m input_type == \u001b[33m\"\u001b[39m\u001b[33mdict\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    767\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m`Command(update=[ToolMessage(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccess\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, tool_call_id=tool_call_id), ...], ...)`\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    768\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m769\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    770\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected to have a matching ToolMessage in Command.update for tool \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcall[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessages_update\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    771\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEvery tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    772\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou can fix it by modifying the tool to return \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_update\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    773\u001b[39m     )\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m updated_command\n",
      "\u001b[31mValueError\u001b[39m: Expected to have a matching ToolMessage in Command.update for tool 'add_general_reference_tool', got: []. Every tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage. You can fix it by modifying the tool to return `Command(update=[ToolMessage(\"Success\", tool_call_id=tool_call_id), ...], ...)`.",
      "During task with name 'tools' and id '454a5ddc-020f-185c-a307-44a60858fcc2'",
      "During task with name 'web_agent' and id '314a03eb-4dde-e7d4-434d-1df048846184'"
     ]
    }
   ],
   "source": [
    "query = \"Deep research plan: Find SOTA RAG papers (2024–2025), collect canonical citations and key claims; add solid Wikipedia/overview links.\"\n",
    "# query = plan_res.steps[0].task\n",
    "\n",
    "\n",
    "init_state = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "    \"facts\": [],           # seed your reducer fields\n",
    "    \"blackboard\": [],\n",
    "    \"done\": False\n",
    "}\n",
    "\n",
    "last_parent_state = None\n",
    "\n",
    "for ns, state in supervisor.stream(\n",
    "    init_state,\n",
    "    config={\"recursion_limit\": 15},\n",
    "    stream_mode=\"values\",          # full state\n",
    "    subgraphs=True,                # include parent + child snapshots\n",
    "):\n",
    "    # ns is a tuple path like (\"supervisor:<id>\",) or (\"supervisor:<id>\", \"web_agent:<id>\")\n",
    "    # print(\"STATE FROM:\", \" > \".join(ns))\n",
    "    print(\"FIELDS:\", list(state.keys()))\n",
    "    print(\"BLACKBOARD SIZE:\", len(state.get(\"facts\", [])))\n",
    "    # print messages for that graph (parent or child)\n",
    "    for m in state.get(\"messages\", []):\n",
    "        m.pretty_print()\n",
    "    # keep last parent snapshot to read facts later\n",
    "    if len(ns) == 1 and ns[0].startswith(\"supervisor:\"):\n",
    "        last_parent_state = state\n",
    "\n",
    "facts = (last_parent_state or state).get(\"facts\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f42ac5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['messages'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# system_prompt = (\n",
    "#     \"You are a specialized research assistant. \"\n",
    "#     \"For any given question, use the available tools to find the most relevant and up-to-date information. \"\n",
    "#     \"Think step-by-step about which tool is best for the task.\"\n",
    "#     \"Additional Rules:\"\n",
    "#     \"- Do NOT produce a narrative final answer.\"\n",
    "#     \"- When COMPLETELY DONE, you MUST call the tool `submit_research` EXACTLY ONCE with a payload matching the ResearchOutput schema (topic, facts[]).\"\n",
    "#     \"- After calling `submit_research`, STOP. Do not say anything else.\"\n",
    "# )\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a specialized research assistant. Your goal is to build a knowledge base of facts about the research topic.\"\n",
    "    \"Think step-by-step. First, search for information using the available tools.\"\n",
    "    \"When you find a relevant source, extract the information and add it to the blackboard using the appropriate tool:\"\n",
    "    \"- For a peer-reviewed paper or arXiv preprint, use `add_scholar_reference`.\"\n",
    "    \"- For a blog post, documentation, or tutorial, use `add_general_reference`.\"\n",
    "    \"- For synthesizing an insight from multiple sources you've already added, use `add_finding`.\" # A potential future tool\n",
    "    \n",
    "    \"Continue this process of searching and adding facts until you have a comprehensive overview of the topic.\"\n",
    "    \"When you believe you have gathered enough information and have no more steps, use the `finish_research` tool to end the process.\"\n",
    ")\n",
    "\n",
    "research_llm = ChatOpenAI(model=\"gpt-5-nano\",temperature=0) #for react agent must have \"free\" llm, no structured output\n",
    "\n",
    "researcher_agent = create_react_agent(\n",
    "    model= research_llm,\n",
    "    tools = tools, #research_tools,\n",
    "    prompt=system_prompt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036da60",
   "metadata": {},
   "source": [
    "##### TOY GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f06f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner_node(state: ResearchState):\n",
    "    \"\"\"\n",
    "    Takes the topic and generates a research plan.\n",
    "    \"\"\"\n",
    "    print(\"--- 🧠 Planning ---\")\n",
    "    # This is the planner chain you created earlier\n",
    "    plan = planner_chain.invoke({\"topic\": state[\"topic\"]}) \n",
    "    return {\"plan\": plan}\n",
    "\n",
    "\n",
    "def researcher_node(state: ResearchState):\n",
    "    \"\"\"\n",
    "    This node runs the researcher agent for a single step of the plan.\n",
    "    It inspects the agent's tool calls and collects any facts that were added.\n",
    "    \"\"\"\n",
    "    print(\"--- 📚 Researching ---\")\n",
    "    \n",
    "    # Determine which step of the plan to execute\n",
    "    current_step_index = len(state['facts'])\n",
    "    plan = state[\"plan\"]\n",
    "    \n",
    "    if current_step_index >= len(plan.steps):\n",
    "        return {}\n",
    "\n",
    "    current_step = plan.steps[current_step_index]\n",
    "    print(f\"--- Running Step {current_step_index + 1}: {current_step.task} ---\")\n",
    "    \n",
    "    # Run the agent executor\n",
    "    result = research_agent_executor.invoke({\"input\": current_step.task})\n",
    "    \n",
    "    # Collect all facts that were added during this run\n",
    "    new_facts_from_this_run = []\n",
    "    for step in result.get('intermediate_steps', []):\n",
    "        action, observation = step\n",
    "        # Check if the tool called was our 'add_fact' tool\n",
    "        if action.tool == \"add_fact\":\n",
    "            # The 'observation' is the return value of the tool, which is our Fact object\n",
    "            new_facts_from_this_run.append(observation)\n",
    "            \n",
    "    # Return the list of newly added facts to be appended to the state\n",
    "    return {\"facts\": new_facts_from_this_run}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f3714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv_search (call_yJNxMDSgzIheE7g7XxhfmOWl)\n",
      " Call ID: call_yJNxMDSgzIheE7g7XxhfmOWl\n",
      "  Args:\n",
      "    query: \"Retrieval-Augmented Generation\" survey 2024\n",
      "  arxiv_search (call_O7hSkUlfMF8vcFS2FkJWPJBo)\n",
      " Call ID: call_O7hSkUlfMF8vcFS2FkJWPJBo\n",
      "  Args:\n",
      "    query: Self-RAG: Learning to Retrieve, Generate, and Critique 2023\n",
      "  arxiv_search (call_UWe9HJJUGqosW2ewvg3b3GKD)\n",
      " Call ID: call_UWe9HJJUGqosW2ewvg3b3GKD\n",
      "  Args:\n",
      "    query: Corrective Retrieval Augmented Generation 2023\n",
      "  arxiv_search (call_lXOtwCN1zELMOIWYmXbngGDY)\n",
      " Call ID: call_lXOtwCN1zELMOIWYmXbngGDY\n",
      "  Args:\n",
      "    query: RAPTOR retrieval augmented generation 2024\n",
      "  arxiv_search (call_P1sFJigOgPD2qI5jEqliqrAD)\n",
      " Call ID: call_P1sFJigOgPD2qI5jEqliqrAD\n",
      "  Args:\n",
      "    query: GraphRAG 2024 arXiv\n",
      "  arxiv_search (call_mAF28J9hkQP0BnuLi6mbn0N7)\n",
      " Call ID: call_mAF28J9hkQP0BnuLi6mbn0N7\n",
      "  Args:\n",
      "    query: RA-DIT retrieval-augmented dual instruction tuning 2024\n",
      "  arxiv_search (call_91zm02jHMhhYw1M1MV8Bl0O5)\n",
      " Call ID: call_91zm02jHMhhYw1M1MV8Bl0O5\n",
      "  Args:\n",
      "    query: FreshLLMs 2023 arXiv\n",
      "  arxiv_search (call_sCdsx4gCA4pNHGZ5hi4yXmAR)\n",
      " Call ID: call_sCdsx4gCA4pNHGZ5hi4yXmAR\n",
      "  Args:\n",
      "    query: HyDE hypothetical document embeddings 2023 arXiv\n",
      "  arxiv_search (call_IveSUGO3uUziGzZTKbjF6cf5)\n",
      " Call ID: call_IveSUGO3uUziGzZTKbjF6cf5\n",
      "  Args:\n",
      "    query: ActiveRAG 2024 arXiv\n",
      "  arxiv_search (call_27LbEG9fJcidRQBbjF4w1F9C)\n",
      " Call ID: call_27LbEG9fJcidRQBbjF4w1F9C\n",
      "  Args:\n",
      "    query: LongRAG 2024 arXiv\n",
      "  tavily_search_results_json (call_uFY7YnpijKLsAYCH6V9ozfHF)\n",
      " Call ID: call_uFY7YnpijKLsAYCH6V9ozfHF\n",
      "  Args:\n",
      "    query: Microsoft GraphRAG 2024 blog GitHub\n",
      "  tavily_search_results_json (call_s2Rp1ER8yamTFfU5J8tYOIBf)\n",
      " Call ID: call_s2Rp1ER8yamTFfU5J8tYOIBf\n",
      "  Args:\n",
      "    query: RAPTOR RAG 2024 blog GitHub\n",
      "  tavily_search_results_json (call_XsWRqWnfqCAtmqpF1cgzl2m3)\n",
      " Call ID: call_XsWRqWnfqCAtmqpF1cgzl2m3\n",
      "  Args:\n",
      "    query: Haystack 2.0 release RAG 2024\n",
      "  tavily_search_results_json (call_K9YbGbsOO34JeYSjU0bwdlQ8)\n",
      " Call ID: call_K9YbGbsOO34JeYSjU0bwdlQ8\n",
      "  Args:\n",
      "    query: LangChain RAG 2023 2024 release blog\n",
      "  tavily_search_results_json (call_mwPyjkJ1PZwA7IhOMj8xufPb)\n",
      " Call ID: call_mwPyjkJ1PZwA7IhOMj8xufPb\n",
      "  Args:\n",
      "    query: LlamaIndex Graph RAG 2024 blog\n",
      "  tavily_search_results_json (call_Zcg2MqWbb23KXkc159PSnAHR)\n",
      " Call ID: call_Zcg2MqWbb23KXkc159PSnAHR\n",
      "  Args:\n",
      "    query: OpenAI text-embedding-3 announcement 2024 blog\n",
      "  tavily_search_results_json (call_y4R9cjo5qOzFQxSv2tPIfGwN)\n",
      " Call ID: call_y4R9cjo5qOzFQxSv2tPIfGwN\n",
      "  Args:\n",
      "    query: OpenAI Responses API retrieval 2024 blog\n",
      "  tavily_search_results_json (call_lGo7DfmG35WqYtOk7efGwwpV)\n",
      " Call ID: call_lGo7DfmG35WqYtOk7efGwwpV\n",
      "  Args:\n",
      "    query: DSPy RAG 2024\n",
      "  tavily_search_results_json (call_RCrDLf6tIm2veCA3tQHUKEze)\n",
      " Call ID: call_RCrDLf6tIm2veCA3tQHUKEze\n",
      "  Args:\n",
      "    query: Weaviate RAG 2024 blog\n",
      "  tavily_search_results_json (call_kdDzJY5uSiHgn0OmNl2dy5Q6)\n",
      " Call ID: call_kdDzJY5uSiHgn0OmNl2dy5Q6\n",
      "  Args:\n",
      "    query: Pinecone RAG 2023 2024 blog\n",
      "  tavily_search_results_json (call_SR9fWWC9AFwI8PXl7QfKLlOv)\n",
      " Call ID: call_SR9fWWC9AFwI8PXl7QfKLlOv\n",
      "  Args:\n",
      "    query: Milvus RAG 2023 2024 blog\n",
      "  tavily_search_results_json (call_GhMQoUA3o64VsiKpRmjufImT)\n",
      " Call ID: call_GhMQoUA3o64VsiKpRmjufImT\n",
      "  Args:\n",
      "    query: Anthropic RAG guide 2024 blog\n",
      "  tavily_search_results_json (call_g21ljIVg9jmzWe7KNxIUnnm9)\n",
      " Call ID: call_g21ljIVg9jmzWe7KNxIUnnm9\n",
      "  Args:\n",
      "    query: Cohere RAG 2024 blog\n",
      "  tavily_search_results_json (call_t0a5f9QhMXL7ItQhHHLyVUVC)\n",
      " Call ID: call_t0a5f9QhMXL7ItQhHHLyVUVC\n",
      "  Args:\n",
      "    query: NeurIPS 2024 tutorial retrieval-augmented generation\n",
      "  tavily_search_results_json (call_dFfbjhnPhPeNlDPNVx1MPr4F)\n",
      " Call ID: call_dFfbjhnPhPeNlDPNVx1MPr4F\n",
      "  Args:\n",
      "    query: ICLR 2024 tutorial retrieval augmented generation talk\n",
      "  tavily_search_results_json (call_x4Lt779WNleyA8GT42OmMBpP)\n",
      " Call ID: call_x4Lt779WNleyA8GT42OmMBpP\n",
      "  Args:\n",
      "    query: RAG evaluation survey 2024 arXiv\n",
      "  tavily_search_results_json (call_6ofckdgqDmdFn6dDxCBCHNU1)\n",
      " Call ID: call_6ofckdgqDmdFn6dDxCBCHNU1\n",
      "  Args:\n",
      "    query: RAG triad 2024 paper\n",
      "  tavily_search_results_json (call_eWNpY4JvoTU3wJcPY22sJPjF)\n",
      " Call ID: call_eWNpY4JvoTU3wJcPY22sJPjF\n",
      "  Args:\n",
      "    query: GraphRAG Microsoft paper 2024 arXiv\n",
      "  tavily_search_results_json (call_fn8bIyk5lyFnbF8Jp9u8z2k8)\n",
      " Call ID: call_fn8bIyk5lyFnbF8Jp9u8z2k8\n",
      "  Args:\n",
      "    query: ActiveRAG 2024 paper blog\n",
      "  tavily_search_results_json (call_KmntguE3jzO72KM3Y7oraEki)\n",
      " Call ID: call_KmntguE3jzO72KM3Y7oraEki\n",
      "  Args:\n",
      "    query: RAG 2025 survey arXiv\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2024-10-17\n",
      "Title: ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents\n",
      "Authors: Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Chaojun Xiao, Zhiyuan Liu, Ge Yu, Chenyan Xiong\n",
      "Summary: Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\n",
      "leverage external knowledge, enhancing their performance on knowledge-intensive\n",
      "tasks. However, existing RAG models often treat LLMs as passive recipients of\n",
      "information, which can lead to interference from noisy retrieved content. In\n",
      "this paper, we introduce ActiveRAG, a multi-agent framework that mimics human\n",
      "learning behavior to help LLMs actively engage with and learn from retrieved\n",
      "evidence. ActiveRAG designs a knowledge assimilation agent to form the\n",
      "knowledge understanding by associating external knowledge with the parametric\n",
      "memory of LLMs. Then our model employs the thought accommodation agent to\n",
      "calibrate the internal thought of LLMs for response refinement. Our experiments\n",
      "show that ActiveRAG achieves a 10\\% improvement over vanilla RAG on various\n",
      "question-answering benchmarks. Further analysis reveals that ActiveRAG\n",
      "mitigates the impact of noisy retrievals, alleviates conflicts between external\n",
      "knowledge and parametric memory and improves the self-consistency of LLMs in\n",
      "answering the question. All data and codes are available at\n",
      "https://github.com/OpenMatch/ActiveRAG.\n",
      "\n",
      "Published: 2024-02-01\n",
      "Title: The algebraic classification of nilpotent Novikov algebras\n",
      "Authors: Kobiljon Abdurasulov, Ivan Kaygorodov, Abror Khudoyberdiyev\n",
      "Summary: This paper is devoted to the complete algebraic classification of complex\n",
      "$5$-dimensional nilpotent Novikov algebras.\n",
      "\n",
      "Published: 2024-03-01\n",
      "Title: The algebraic classification of nilpotent bicommutative algebras\n",
      "Authors: Kobiljon Abdurasulov, Ivan Kaygorodov, Abror Khudoyberdiyev\n",
      "Summary: This paper is devoted to the complete algebraic classification of complex\n",
      "5-dimensional nilpotent bicommutative algebras.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2023-08-02\n",
      "Title: Large Language Models are Strong Zero-Shot Retriever\n",
      "Authors: Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Tianyi Zhou, Daxin Jiang\n",
      "Summary: In this work, we propose a simple method that applies a large language model\n",
      "(LLM) to large-scale retrieval in zero-shot scenarios. Our method, the Language\n",
      "language model as Retriever (LameR), is built upon no other neural models but\n",
      "an LLM, while breaking brute-force combinations of retrievers with LLMs and\n",
      "lifting the performance of zero-shot retrieval to be very competitive on\n",
      "benchmark datasets. Essentially, we propose to augment a query with its\n",
      "potential answers by prompting LLMs with a composition of the query and the\n",
      "query's in-domain candidates. The candidates, regardless of correct or wrong,\n",
      "are obtained by a vanilla retrieval procedure on the target collection. As a\n",
      "part of the prompts, they are likely to help LLM generate more precise answers\n",
      "by pattern imitation or candidate summarization. Even if all the candidates are\n",
      "wrong, the prompts at least make LLM aware of in-collection patterns and\n",
      "genres. Moreover, due to the low performance of a self-supervised retriever,\n",
      "the LLM-based query augmentation becomes less effective as the retriever\n",
      "bottlenecks the whole pipeline. Therefore, we propose to leverage a\n",
      "non-parametric lexicon-based method (e.g., BM25) as the retrieval module to\n",
      "capture query-document overlap in a literal fashion. As such, LameR makes the\n",
      "retrieval procedure transparent to the LLM, thus circumventing the performance\n",
      "bottleneck.\n",
      "\n",
      "Published: 2024-04-05\n",
      "Title: TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction\n",
      "Authors: Shuo Li, Sangdon Park, Insup Lee, Osbert Bastani\n",
      "Summary: When applied to open-domain question answering, large language models (LLMs)\n",
      "frequently generate incorrect responses based on made-up facts, which are\n",
      "called $\\textit{hallucinations}$. Retrieval augmented generation (RAG) is a\n",
      "promising strategy to avoid hallucinations, but it does not provide guarantees\n",
      "on its correctness. To address this challenge, we propose the Trustworthy\n",
      "Retrieval Augmented Question Answering, or $\\textit{TRAQ}$, which provides the\n",
      "first end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal\n",
      "prediction, a statistical technique for constructing prediction sets that are\n",
      "guaranteed to contain the semantically correct response with high probability.\n",
      "Additionally, TRAQ leverages Bayesian optimization to minimize the size of the\n",
      "constructed sets. In an extensive experimental evaluation, we demonstrate that\n",
      "TRAQ provides the desired correctness guarantee while reducing prediction set\n",
      "size by 16.2% on average compared to an ablation. The implementation is\n",
      "available at $\\href{https://github.com/shuoli90/TRAQ.git}{TRAQ}$.\n",
      "\n",
      "Published: 2024-04-08\n",
      "Title: Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs\n",
      "Authors: Ye Liu, Semih Yavuz, Rui Meng, Meghana Moorthy, Shafiq Joty, Caiming Xiong, Yingbo Zhou\n",
      "Summary: The integration of retrieved passages and large language models (LLMs), such\n",
      "as ChatGPTs, has significantly contributed to improving open-domain question\n",
      "answering. However, there is still a lack of exploration regarding the optimal\n",
      "approach for incorporating retrieved passages into the answer generation\n",
      "process. This paper aims to fill this gap by investigating different methods of\n",
      "combining retrieved passages with LLMs to enhance answer generation. We begin\n",
      "by examining the limitations of a commonly-used concatenation approach.\n",
      "Surprisingly, this approach often results in generating \"unknown\" outputs, even\n",
      "when the correct document is among the top-k retrieved passages. To address\n",
      "this issue, we explore four alternative strategies for integrating the\n",
      "retrieved passages with the LLMs. These strategies include two single-round\n",
      "methods that utilize chain-of-thought reasoning and two multi-round strategies\n",
      "that incorporate feedback loops. T\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2023-11-22\n",
      "Title: FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation\n",
      "Authors: Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, Thang Luong\n",
      "Summary: Most large language models (LLMs) are trained once and never updated; thus,\n",
      "they lack the ability to dynamically adapt to our ever-changing world. In this\n",
      "work, we perform a detailed study of the factuality of LLM-generated text in\n",
      "the context of answering questions that test current world knowledge.\n",
      "Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a\n",
      "diverse range of question and answer types, including questions that require\n",
      "fast-changing world knowledge as well as questions with false premises that\n",
      "need to be debunked. We benchmark a diverse array of both closed and\n",
      "open-source LLMs under a two-mode evaluation procedure that allows us to\n",
      "measure both correctness and hallucination. Through human evaluations involving\n",
      "more than 50K judgments, we shed light on limitations of these models and\n",
      "demonstrate significant room for improvement: for instance, all models\n",
      "(regardless of model size) struggle on questions that involve fast-changing\n",
      "knowledge and false premises. Motivated by these results, we present\n",
      "FreshPrompt, a simple few-shot prompting method that substantially boosts the\n",
      "performance of an LLM on FreshQA by incorporating relevant and up-to-date\n",
      "information retrieved from a search engine into the prompt. Our experiments\n",
      "show that FreshPrompt outperforms both competing search engine-augmented\n",
      "prompting methods such as Self-Ask (Press et al., 2022) as well as commercial\n",
      "systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that\n",
      "both the number of retrieved evidences and their order play a key role in\n",
      "influencing the correctness of LLM-generated answers. Additionally, instructing\n",
      "the LLM to generate concise and direct answers helps reduce hallucination\n",
      "compared to encouraging more verbose answers. To facilitate future work, we\n",
      "release FreshQA at github.com/freshllms/freshqa and commit to updating it at\n",
      "regular intervals.\n",
      "\n",
      "Published: 2023-08-24\n",
      "Title: The Nature of Concurrency\n",
      "Authors: Yong Wang\n",
      "Summary: We try to clarify the relationship between interleaving concurrency and true\n",
      "concurrency, based on the very recent work on truly concurrent process algebra,\n",
      "especially, giving models of true concurrency, such as event structure, Petri\n",
      "net and concurrent Kleene algebra, (truly concurrent) process algebra\n",
      "foundations.\n",
      "\n",
      "Published: 2023-07-01\n",
      "Title: The algebraic and geometric classification of nilpotent Leibniz algebras\n",
      "Authors: Kobiljon Abdurasulov, Ivan Kaygorodov, Abror Khudoyberdiyev\n",
      "Summary: This paper is devoted to the complete algebraic and geometric classification\n",
      "of complex $5$-dimensional nilpotent Leibniz algebras. In particular, the\n",
      "variety of complex $5$-dimensional nilpotent Leibniz algebras has dimension\n",
      "$24$ it has $10$ irreducible components (there is only one rigid algebra in\n",
      "this variety).\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2024-01-31\n",
      "Title: RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "Authors: Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning\n",
      "Summary: Retrieval-augmented language models can better adapt to changes in world\n",
      "state and incorporate long-tail knowledge. However, most existing methods\n",
      "retrieve only short contiguous chunks from a retrieval corpus, limiting\n",
      "holistic understanding of the overall document context. We introduce the novel\n",
      "approach of recursively embedding, clustering, and summarizing chunks of text,\n",
      "constructing a tree with differing levels of summarization from the bottom up.\n",
      "At inference time, our RAPTOR model retrieves from this tree, integrating\n",
      "information across lengthy documents at different levels of abstraction.\n",
      "Controlled experiments show that retrieval with recursive summaries offers\n",
      "significant improvements over traditional retrieval-augmented LMs on several\n",
      "tasks. On question-answering tasks that involve complex, multi-step reasoning,\n",
      "we show state-of-the-art results; for example, by coupling RAPTOR retrieval\n",
      "with the use of GPT-4, we can improve the best performance on the QuALITY\n",
      "benchmark by 20% in absolute accuracy.\n",
      "\n",
      "Published: 2025-06-04\n",
      "Title: Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models\n",
      "Authors: Alex Laitenberger, Christopher D. Manning, Nelson F. Liu\n",
      "Summary: With the rise of long-context language models (LMs) capable of processing\n",
      "tens of thousands of tokens in a single pass, do multi-stage\n",
      "retrieval-augmented generation (RAG) pipelines still offer measurable benefits\n",
      "over simpler, single-stage approaches? To assess this question, we conduct a\n",
      "controlled evaluation for QA tasks under systematically scaled token budgets,\n",
      "comparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three\n",
      "baselines, including DOS RAG (Document's Original Structure RAG), a simple\n",
      "retrieve-then-read method that preserves original passage order. Despite its\n",
      "straightforward design, DOS RAG consistently matches or outperforms more\n",
      "intricate methods on multiple long-context QA benchmarks. We recommend\n",
      "establishing DOS RAG as a simple yet strong baseline for future RAG\n",
      "evaluations, pairing it with emerging embedding and language models to assess\n",
      "trade-offs between complexity and effectiveness as model capabilities evolve.\n",
      "\n",
      "Published: 2024-12-13\n",
      "Title: Raptor: Distributed Scheduling for Serverless Functions\n",
      "Authors: Kevin Exton, Maria Read\n",
      "Summary: To support parallelizable serverless workflows in applications like media\n",
      "processing, we have prototyped a distributed scheduler called Raptor that\n",
      "reduces both the end-to-end delay time and failure rate of parallelizable\n",
      "serverless workflows. As modern serverless frameworks are typically deployed to\n",
      "extremely large scale distributed computing environments by major cloud\n",
      "providers, Raptor is specifically designed to exploit the property of\n",
      "statistically independent function execution that tends to emerge at very large\n",
      "scales. To demonstrate the effect of horizontal scale on function execution,\n",
      "our evaluation demonstrates that mean delay time improvements provided by\n",
      "Raptor for RSA public-private key pair generation can be accurately predicted\n",
      "by mutually independent exponential random variables, but only once the\n",
      "serverless framework is deployed in a highly available configuration and\n",
      "horizontally scaled across three availability zones.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2024-09-01\n",
      "Title: LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs\n",
      "Authors: Ziyan Jiang, Xueguang Ma, Wenhu Chen\n",
      "Summary: In traditional RAG framework, the basic retrieval units are normally short.\n",
      "The common retrievers like DPR normally work with 100-word Wikipedia\n",
      "paragraphs. Such a design forces the retriever to search over a large corpus to\n",
      "find the `needle' unit. In contrast, the readers only need to generate answers\n",
      "from the short retrieved units. The imbalanced `heavy' retriever and `light'\n",
      "reader design can lead to sub-optimal performance. The loss of contextual\n",
      "information in the short, chunked units may increase the likelihood of\n",
      "introducing hard negatives during the retrieval stage. Additionally, the reader\n",
      "might not fully leverage the capabilities of recent advancements in LLMs. In\n",
      "order to alleviate the imbalance, we propose a new framework LongRAG,\n",
      "consisting of a `long retriever' and a `long reader'. In the two\n",
      "Wikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire\n",
      "Wikipedia corpus into 4K-token units by grouping related documents. By\n",
      "increasing the unit size, we significantly reduce the total number of units.\n",
      "This greatly reduces the burden on the retriever, resulting in strong retrieval\n",
      "performance with only a few (less than 8) top units. Without requiring any\n",
      "training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which\n",
      "are on par with the (fully-trained) SoTA model. Furthermore, we test on two\n",
      "non-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes\n",
      "each individual document as a single (long) unit rather than chunking them into\n",
      "smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5%\n",
      "on MultiFieldQA-en. Our study offers insights into the future roadmap for\n",
      "combining RAG with long-context LLMs.\n",
      "\n",
      "Published: 2024-11-01\n",
      "Title: LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering\n",
      "Authors: Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, Jie Tang\n",
      "Summary: Long-Context Question Answering (LCQA), a challenging task, aims to reason\n",
      "over long-context documents to yield accurate answers to questions. Existing\n",
      "long-context Large Language Models (LLMs) for LCQA often struggle with the\n",
      "\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\n",
      "issue by providing external factual evidence. However, its chunking strategy\n",
      "disrupts the global long-context information, and its low-quality retrieval in\n",
      "long contexts hinders LLMs from identifying effective factual details due to\n",
      "substantial noise. To this end, we propose LongRAG, a general,\n",
      "dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\n",
      "RAG's understanding of complex long-context knowledge (i.e., global information\n",
      "and factual details). We design LongRAG as a plug-and-play paradigm,\n",
      "facilitating adaptation to various domains and LLMs. Extensive experiments on\n",
      "three multi-hop datasets demonstrate that LongRAG significantly outperforms\n",
      "long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n",
      "(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\n",
      "multi-dimensional analyses, highlighting the effectiveness of the system's\n",
      "components and fine-tuning strategies. Data and code are available at\n",
      "https://github.com/QingFei1/LongRAG.\n",
      "\n",
      "Published: 2024-02-01\n",
      "Title: The algebraic classification of nilpotent Novikov algebras\n",
      "Authors: Kobiljon Abdurasulov, Ivan Kaygorodov, Abror Khudoyberdiyev\n",
      "Summary: This paper is devoted to the complete algebraic classification of complex\n",
      "$5$-dimensional nilpotent Novikov algebras.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2022-12-20\n",
      "Title: Precise Zero-Shot Dense Retrieval without Relevance Labels\n",
      "Authors: Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan\n",
      "Summary: While dense retrieval has been shown effective and efficient across tasks and\n",
      "languages, it remains difficult to create effective fully zero-shot dense\n",
      "retrieval systems when no relevance label is available. In this paper, we\n",
      "recognize the difficulty of zero-shot learning and encoding relevance. Instead,\n",
      "we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a\n",
      "query, HyDE first zero-shot instructs an instruction-following language model\n",
      "(e.g. InstructGPT) to generate a hypothetical document. The document captures\n",
      "relevance patterns but is unreal and may contain false details. Then, an\n",
      "unsupervised contrastively learned encoder~(e.g. Contriever) encodes the\n",
      "document into an embedding vector. This vector identifies a neighborhood in the\n",
      "corpus embedding space, where similar real documents are retrieved based on\n",
      "vector similarity. This second step ground the generated document to the actual\n",
      "corpus, with the encoder's dense bottleneck filtering out the incorrect\n",
      "details. Our experiments show that HyDE significantly outperforms the\n",
      "state-of-the-art unsupervised dense retriever Contriever and shows strong\n",
      "performance comparable to fine-tuned retrievers, across various tasks (e.g. web\n",
      "search, QA, fact verification) and languages~(e.g. sw, ko, ja).\n",
      "\n",
      "Published: 2025-06-12\n",
      "Title: Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion\n",
      "Authors: Andrejs Sorstkins\n",
      "Summary: Resource efficiency is a critical barrier to deploying large language models\n",
      "(LLMs) in edge and privacy-sensitive applications. This study evaluates the\n",
      "efficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)\n",
      "and Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion\n",
      "and 4 billion parameters, within the context of a privacy-first personal\n",
      "assistant. We implement short-term memory via MongoDB and long-term semantic\n",
      "storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the\n",
      "system through a React.js frontend. Across both model scales, RAG consistently\n",
      "reduces latency by up to 17\\% and eliminates factual hallucinations when\n",
      "responding to user-specific and domain-specific queries. HyDE, by contrast,\n",
      "enhances semantic relevance--particularly for complex physics prompts--but\n",
      "incurs a 25--40\\% increase in response time and a non-negligible hallucination\n",
      "rate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that\n",
      "scaling yields marginal throughput gains for baseline and RAG pipelines, but\n",
      "magnifies HyDE's computational overhead and variability. Our findings position\n",
      "RAG as the pragmatic choice for on-device personal assistants powered by\n",
      "small-scale LLMs.\n",
      "\n",
      "Published: 2024-04-01\n",
      "Title: ARAGOG: Advanced RAG Output Grading\n",
      "Authors: Matouš Eibich, Shivay Nagpal, Alexander Fred-Ojala\n",
      "Summary: Retrieval-Augmented Generation (RAG) is essential for integrating external\n",
      "knowledge into Large Language Model (LLM) outputs. While the literature on RAG\n",
      "is growing, it primarily focuses on systematic reviews and comparisons of new\n",
      "state-of-the-art (SoTA) techniques against their predecessors, with a gap in\n",
      "extensive experimental comparisons. This study begins to address this gap by\n",
      "assessing various RAG methods' impacts on retrieval precision and answer\n",
      "similarity. We found that Hypothetical Document Embedding (HyDE) and LLM\n",
      "reranking significantly enhance retrieval precision. However, Maximal Marginal\n",
      "Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a\n",
      "baseline Naive RAG system, and Multi-query approaches underperformed. Sentence\n",
      "Window Retrieval emerged as the most effective for retrieval precision, despite\n",
      "its variable performance on answer similarity. The study confirms the potential\n",
      "of the Document Summary Index as a competent retrieval approach. All resources\n",
      "related to this rese\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2025-08-19\n",
      "Title: LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration\n",
      "Authors: Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, S. Kevin Zhou, Jianliang Xu\n",
      "Summary: GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\n",
      "improve reasoning accuracy and contextual relevance. Despite its promising\n",
      "applications and strong relevance to multiple research communities, such as\n",
      "databases and natural language processing, GraphRAG currently lacks modular\n",
      "workflow analysis, systematic solution frameworks, and insightful empirical\n",
      "studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\n",
      "that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\n",
      "systematic classification of existing techniques and implemented GraphRAG\n",
      "instances, and 3) creation of new GraphRAG instances. Our framework facilitates\n",
      "comprehensive empirical studies of GraphRAG on large-scale real-world graphs\n",
      "and diverse query sets, revealing insights into balancing reasoning quality,\n",
      "runtime efficiency, and token or GPU cost, that are essential for building\n",
      "advanced GraphRAG systems.\n",
      "\n",
      "Published: 2024-09-26\n",
      "Title: Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study\n",
      "Authors: Zahra Sepasdar, Sushant Gautam, Cise Midoglu, Michael A. Riegler, Pål Halvorsen\n",
      "Summary: Extracting meaningful insights from large and complex datasets poses\n",
      "significant challenges, particularly in ensuring the accuracy and relevance of\n",
      "retrieved information. Traditional data retrieval methods such as sequential\n",
      "search and index-based retrieval often fail when handling intricate and\n",
      "interconnected data structures, resulting in incomplete or misleading outputs.\n",
      "To overcome these limitations, we introduce Structured-GraphRAG, a versatile\n",
      "framework designed to enhance information retrieval across structured datasets\n",
      "in natural language queries. Structured-GraphRAG utilizes multiple knowledge\n",
      "graphs, which represent data in a structured format and capture complex\n",
      "relationships between entities, enabling a more nuanced and comprehensive\n",
      "retrieval of information. This graph-based approach reduces the risk of errors\n",
      "in language model outputs by grounding responses in a structured format,\n",
      "thereby enhancing the reliability of results. We demonstrate the effectiveness\n",
      "of Structured-GraphRAG by comparing its performance with that of a recently\n",
      "published method using traditional retrieval-augmented generation. Our findings\n",
      "show that Structured-GraphRAG significantly improves query processing\n",
      "efficiency and reduces response times. While our case study focuses on soccer\n",
      "data, the framework's design is broadly applicable, offering a powerful tool\n",
      "for data analysis and enhancing language model applications across various\n",
      "structured domains.\n",
      "\n",
      "Published: 2024-09-10\n",
      "Title: Graph Retrieval-Augmented Generation: A Survey\n",
      "Authors: Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang\n",
      "Summary: Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\n",
      "success in addressing the challenges of Large Language Models (LLMs) without\n",
      "necessitating retraining. By referencing an external knowledge base, RAG\n",
      "refines LLM outputs, effectively mitigating issues such as ``hallucination'',\n",
      "lack of domain-specific knowledge, and outdated information. However, the\n",
      "complex structure of relationships among different entities in databases\n",
      "presents challenges for RAG systems. In response, GraphRAG leverages structural\n",
      "information across entities to enable more precise and comprehensive retrieval,\n",
      "capturing relational knowledge and facilitating more accurate, context-aware\n",
      "responses. Given the novelty and potential of GraphRAG, a systematic review of\n",
      "current technologies is imperative. This paper provides the first comprehensive\n",
      "overview of GraphRAG methodologies. We formalize the GraphRAG workflow,\n",
      "encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enh\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2024-05-06\n",
      "Title: RA-DIT: Retrieval-Augmented Dual Instruction Tuning\n",
      "Authors: Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, Scott Yih\n",
      "Summary: Retrieval-augmented language models (RALMs) improve performance by accessing\n",
      "long-tail and up-to-date knowledge from external data stores, but are\n",
      "challenging to build. Existing approaches require either expensive\n",
      "retrieval-specific modifications to LM pre-training or use post-hoc integration\n",
      "of the data store that leads to suboptimal performance. We introduce\n",
      "Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning\n",
      "methodology that provides a third option by retrofitting any LLM with retrieval\n",
      "capabilities. Our approach operates in two distinct fine-tuning steps: (1) one\n",
      "updates a pre-trained LM to better use retrieved information, while (2) the\n",
      "other updates the retriever to return more relevant results, as preferred by\n",
      "the LM. By fine-tuning over tasks that require both knowledge utilization and\n",
      "contextual awareness, we demonstrate that each stage yields significant\n",
      "performance improvements, and using both leads to additional gains. Our best\n",
      "model, RA-DIT 65B, achieves state-of-the-art performance across a range of\n",
      "knowledge-intensive zero- and few-shot learning benchmarks, significantly\n",
      "outperforming existing in-context RALM approaches by up to +8.9% in 0-shot\n",
      "setting and +1.4% in 5-shot setting on average.\n",
      "\n",
      "Published: 2024-03-27\n",
      "Title: Dual Instruction Tuning with Large Language Models for Mathematical Reasoning\n",
      "Authors: Yongwei Zhou, Tiejun Zhao\n",
      "Summary: Recent advancements highlight the success of instruction tuning with large\n",
      "language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical\n",
      "reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as\n",
      "incorrect, missing, and redundant steps in CoT generation leading to\n",
      "inaccuracies in answer predictions. To alleviate this problem, we propose a\n",
      "dual instruction tuning strategy to meticulously model mathematical reasoning\n",
      "from both forward and reverse directions. This involves introducing the\n",
      "Intermediate Reasoning State Prediction task (forward reasoning) and the\n",
      "Instruction Reconstruction task (reverse reasoning) to enhance the LLMs'\n",
      "understanding and execution of instructions. Training instances for these tasks\n",
      "are constructed based on existing mathematical instruction tuning datasets.\n",
      "Subsequently, LLMs undergo multi-task fine-tuning using both existing\n",
      "mathematical instructions and the newly created data. Comprehensive experiments\n",
      "validate the effectiveness and domain generalization of the dual instruction\n",
      "tuning strategy across various mathematical reasoning tasks.\n",
      "\n",
      "Published: 2025-07-01\n",
      "Title: SMoLoRA: Exploring and Defying Dual Catastrophic Forgetting in Continual Visual Instruction Tuning\n",
      "Authors: Ziqi Wang, Chang Che, Qi Wang, Yangyang Li, Zenglin Shi, Meng Wang\n",
      "Summary: Visual instruction tuning (VIT) enables multimodal large language models\n",
      "(MLLMs) to effectively handle a wide range of vision tasks by framing them as\n",
      "language-based instructions. Building on this, continual visual instruction\n",
      "tuning (CVIT) extends the capability of MLLMs to incrementally learn new tasks,\n",
      "accommodating evolving functionalities. While prior work has advanced CVIT\n",
      "through the development of new benchmarks and approaches to mitigate\n",
      "catastrophic forgetting, these efforts largely follow traditional continual\n",
      "learning paradigms, neglecting the unique challenges specific to CVIT. We\n",
      "identify a dual form of catastrophic forgetting in CVIT, where MLLMs not only\n",
      "forget previously learned visual understanding but also experience a decline in\n",
      "instruction following abilities as they acquire new tasks. To address this, we\n",
      "introduce the Separable Mixture of Low-Rank Adaptation (SMoLoRA) framework,\n",
      "which employs separable routing through two distinct modules-one for visual\n",
      "understandin\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2024-11-08\n",
      "Title: Quadratic Hessian equation\n",
      "Authors: Yu Yuan\n",
      "Summary: We survey quadratic Hessian equations: definition, background, rigidity of\n",
      "entire solutions, regularity of viscosity solutions, a priori Hessian\n",
      "estimates, and open problems.\n",
      "\n",
      "Published: 2024-12-01\n",
      "Title: Statistics in Survey Sampling\n",
      "Authors: Jae Kwang Kim\n",
      "Summary: Survey sampling theory and methods are introduced. Sampling designs and\n",
      "estimation methods are carefully discussed as a textbook for survey sampling.\n",
      "Topics includes Horvitz-Thompson estimation, simple random sampling, stratified\n",
      "sampling, cluster sampling, ratio estimation, regression estimation, variance\n",
      "estimation, two-phase sampling, and nonresponse adjustment methods.\n",
      "\n",
      "Published: 2024-01-10\n",
      "Title: A survey on submanifolds of nearly Kähler spaces\n",
      "Authors: Mateo Anarella\n",
      "Summary: In this survey, we give an introduction to nearly K\\\"ahler geometry, and list\n",
      "some results on submanifolds of these spaces. This survey tries by no means to\n",
      "be complete.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Welcome to Weaviate 2024: AI Insights, Learning, and Community ...\", \"url\": \"https://newsletter.weaviate.io/p/welcome-weaviate-2024-ai-insights-learning-community-highlights\", \"content\": \"Image 8: FREE Advanced RAG eBook, New Explorer Tool, and Embedding Model Guide FREE Advanced RAG eBook, New Explorer Tool, and Embedding Model Guide ---------------------------------------------------------------------Image 9: The Personalization Agent, Weaviate 1.30 Release Highlights and Exploring GraphRAG The Personalization Agent, Weaviate 1.30 Release Highlights and Exploring GraphRAG ----------------------------------------------------------------------------------Image 10: Weaviate 1.30 [...] Is your team building an AI application this year?\\n\\nWeaviate’s Byron Voorbach and Innovative Solutions' Travis Rehl are here to share best practices for creating a fast, secure and scalable foundation for your next AI project. Sign up and you’ll get the chance to ask questions and see a demo of how these concepts work in the real world!\\n\\nStay tuned for more events on the Events & Webinars Page and in our Weaviate newsletter. [...] Do you know someone who has significantly impacted your Weaviate journey? Nominate them as a Weaviate Hero! Let's acknowledge and appreciate the individuals who enrich our community. 💚\\n\\nNominate your hero\\n\\nAs we continue to build, learn, and collaborate, we're excited to see the innovations that 2024 will bring. 💫\\n\\nWelcoming new faces\\n\\n   Mohamed Badawiis joining us as a Software Engineer in the Cloud Team\\n\\n   José Luis Franco Arzais joining us as a QA Engineer in the Cloud Strategy Team\", \"score\": 0.63098115}, {\"title\": \"Advanced RAG Techniques - Weaviate\", \"url\": \"https://weaviate.io/blog/advanced-rag\", \"content\": \"← Back to Blogs\\n\\nSkip to main content\\n\\nProduct Update: Meet Weaviate Agents — Read the blog\\n\\n# Advanced RAG Techniques\\n\\n· 12 min read\\n\\nZain Hasan\\n\\nDeveloper Advocate\\n\\nIn this post we’ll dig into Retrieval-Augmented Generation (RAG) and show you practical tips and tricks to improve every part of the RAG pipeline! We’ll start by considering a a practical example: building a chatbot to emulate or assist a human doctor. [...] Check out the Quickstart tutorial, or build amazing apps with a free trial of Weaviate Cloud (WCD).\\n\\nGitHub\\n\\nForum\\n\\nSlack\\n\\nX (Twitter)\\n\\n## Don't want to miss another blog post?\\n\\nSign up for our bi-weekly newsletter to stay updated!\\n\\n  \\nBy submitting, I agree to the Terms of Service and Privacy Policy. [...] Overview: This advanced method uses a language model to create self-contained propositions from text, ensuring that each chunk is semantically complete.\\n Implementation: A language model (e.g., a 7-billion parameter model) processes the text, breaking it into propositions that make sense independently. The model then combines these propositions into chunks, balancing semantic completeness and context. This method is computationally intensive but provides the highest accuracy.\", \"score\": 0.62633044}, {\"title\": \"Introduction to Retrieval Augmented Generation (RAG) - Weaviate\", \"url\": \"https://weaviate.io/blog/introduction-to-rag\", \"content\": \"In this article, we take a deep dive into Retrieval Augmented Generation (RAG), a framework that enhances the capabilities of generative models by allowing them to reference external data. We’ll explore the limitations of generative models that led to the creation of RAG, explain how RAG works, and break down the architecture behind RAG pipelines. We’ll also get practical and outline some real-world RAG use cases, suggest concrete ways to implement RAG, introduce you to a few advanced RAG [...] 👩‍🍳 Naive RAG with LlamaIndex (Recipe)\\n📝 Advanced RAG Techniques (Blog post)\\n📒 Agentic RAG with Multi-Document Agents (Notebook)\\n📝 An Overview of RAG Evaluation (Blog post)\\n📄 Evaluation of Retrieval-Augmented Generation: A Survey (Academic paper)\\nReady to start building?​ [...] Image 5: Advanced RAG\\nFor a more in-depth exploration of this topic, check out our blog post on advanced RAG techniques.\\n### Agentic RAG​\", \"score\": 0.52612424}, {\"title\": \"What is Agentic RAG\", \"url\": \"https://weaviate.io/blog/what-is-agentic-rag\", \"content\": \"While Retrieval-Augmented Generation (RAG) dominated 2023, agentic workflows are driving massive progress in 2024. The usage of AI agents opens up new possibilities for building more powerful, robust, and versatile Large Language Model(LLM)-powered applications. One possibility is enhancing RAG pipelines with AI agents in agentic RAG pipelines.\\n\\nThis article introduces you to the concept of agentic RAG, its implementation, and its benefits and limitations.\\n\\n## Fundamentals of Agentic RAG​ [...] ← Back to Blogs\\n\\nSkip to main content\\n\\nProduct Update: Meet Weaviate Agents — Read the blog\\n\\n# What is Agentic RAG\\n\\n· 11 min read\\n\\nErika Cardenas\\n\\nTechnology Partner Manager\\n\\nLeonie Monigatti\\n\\nMachine Learning Engineer [...] Check out the Quickstart tutorial, or build amazing apps with a free trial of Weaviate Cloud (WCD).\\n\\nX (Twitter)\\n\\n## Don't want to miss another blog post?\\n\\nSign up for our bi-weekly newsletter to stay updated!\\n\\n  \\nBy submitting, I agree to the Terms of Service and Privacy Policy.\", \"score\": 0.50915486}, {\"title\": \"Vanilla RAG versus Agentic RAG - weaviate/recipes\", \"url\": \"https://github.com/weaviate/recipes/blob/main/integrations/llm-agent-frameworks/agentic-rag-benchmark/vanilla-rag-vs-agentic-rag.ipynb\", \"content\": \"1112 lines (1112 loc) · 39.2 KB\\n\\nRaw\\n\\nCopy raw file\\n\\nDownload raw file\\n\\nYou must be signed in to make or propose changes\\n\\nMore edit options\\n\\nEdit and raw actions\\n\\nLoading Viewer requires iframe.\\n\\nFooter\\n\\n[]( © 2025 GitHub,Inc. \\n\\n### Footer navigation\\n\\n   Terms\\n   Privacy\\n   Security\\n   Status\\n   Docs\\n   Contact\\n    Manage cookies \\n    Do not share my personal information \\n\\n You can’t perform that action at this time. [...] recipes/integrations/llm-agent-frameworks/agentic-rag-benchmark/vanilla-rag-vs-agentic-rag.ipynb at main · weaviate/recipes · GitHub\\n\\nSkip to content\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\nOpen commit details\\n\\nAug 11, 2025\\n\\n84520dd·Aug 11, 2025\\n\\nHistory\\n\\nHistory\\n\\nOpen commit details\\n\\n · 39.2 KB\\n\\nBreadcrumbs\\n\\n1.   recipes\\n2.   /integrations\\n3.   /llm-agent-frameworks\\n4.   /agentic-rag-benchmark\\n\\n/\\nvanilla-rag-vs-agentic-rag.ipynb\\n\\nTop\\n\\nFile metadata and controls\\n\\n   Preview \\n   Code \\n   Blame\", \"score\": 0.356492}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Retrieval-Augmented Generation (RAG) - Pinecone\", \"url\": \"https://www.pinecone.io/learn/retrieval-augmented-generation/\", \"content\": \"Building a reliable, curated, and accurate RAG system with Cleanlab and Pinecone\\n\\nMatt Turk\\nJan 16, 2024\\n\\nRAG makes LLMs better and equal\\n\\nAmnon, \\nRoy, \\nIlai, \\nNathan, \\nAmir [...] Want to dig into a RAG code example? Create a free Pinecone account and check out our example notebooks to implement retrieval-augmented generation with Pinecone or get started with Pinecone Assistant, to build production-grade chat and agent-based applications quickly.\\n\\nShare:\\n\\nWas this article helpful?\\n\\nRecommended for you\\n\\nFurther Reading\\n\\nLearn\\n\\nJun 25, 2025\\n\\nBeyond the hype: Why RAG remains essential for modern AI\\n\\nJenna Pederson\\nLearn\\n\\nOct 25, 2024 [...] These foundation model limitations can impact your business bottom line and erode the trust of your users. Retrieval-augmented generation can address these limitations.\\n\\n## What is Retrieval-Augmented Generation?\\n\\nRetrieval-augmented generation, or RAG, is a technique that uses authoritative, external data to improve the accuracy, relevancy, and usefulness of a model’s output. It does this through the following four core components, which we’ll cover in more detail later in this article:\", \"score\": 0.7498395}, {\"title\": \"Beyond the hype: Why RAG remains essential for modern AI\", \"url\": \"https://www.pinecone.io/learn/rag-2025/\", \"content\": \"Domain-specific AI agents at scale: CustomGPT.ai serves 10,000+ customers with Pinecone\\nLearn\\n\\nOct 25, 2024\\n\\nBuilding a reliable, curated, and accurate RAG system with Cleanlab and Pinecone\\n\\nMatt Turk [...] But still, people are claiming \\\"RAG is dead.\\\" While 2023 was the year of RAG, it only focused on very simple implementations, often with a vector database and a one-shot prompt with context sent to a model to generate output. Through 2024 and now into 2025, we're seeing much more complex RAG systems being built. These systems involve query processing, multiple retrieval sources and steps, model processing, and evaluation of results. And with the growth of agentic systems, each have their own\", \"score\": 0.6783488}, {\"title\": \"Implementing a RAG chatbot using Databricks and Pinecone\", \"url\": \"https://www.databricks.com/blog/implementing-rag-chatbot-using-databricks-and-pinecone\", \"content\": \"Skip to main content\\n\\n# Implementing a RAG chatbot using Databricks and Pinecone\\n\\nPublished: September 9, 2024\\n\\nPartners11 min read\\n\\nby Prasad Kona and Maria Pere-Perez\\n\\nShare this post\\n\\nKeep up with us\\n\\nImagine giving your business an intelligent bot to talk to customers. Chatbots are commonly used to talk to customers and provide them with help or information. But, the usual chatbots sometimes struggle to answer complicated questions.\\n\\n## What is RAG? [...] Check out our free training to learn more about Generative AI with Databricks, and read additional Pinecone and Databricks documentation here. Access sample notebooks from this blog here.\\n\\nKeep up with us\\n\\nRecommended for you\\n\\nShare this post\\n\\n## Never miss a Databricks post\\n\\nSubscribe to the categories you care about and get the latest posts delivered to your inbox\\n\\n## Sign up\\n\\n## What's next?\\n\\nProduct\\n\\nNovember 20, 2024/4 min read\\n\\n#### Introducing Predictive Optimization for Statistics [...] Product\\n\\nNovember 21, 2024/3 min read\\n\\n#### How to present and share your Notebook insights in AI/BI Dashboards\\n\\n## We Care About Your Privacy\\n\\nDatabricks uses cookies and similar technologies to enhance site navigation, analyze site usage, personalize content and ads, and as further described in our Cookie Notice. To disable non-essential cookies, click “Reject All”. You can also manage your cookie settings by clicking “Manage Preferences.”\\n\\n## Privacy Preference Center\", \"score\": 0.6490677}, {\"title\": \"Simplify, enhance, and evaluate RAG development with Pinecone ...\", \"url\": \"https://www.pinecone.io/blog/pinecone-assistant-for-all/\", \"content\": \"# Simplify, enhance, and evaluate RAG development with Pinecone Assistant, now in public preview\\n\\nGibbs Cullen\\n\\nSep 18, 2024\\n\\nProduct\\n\\nShare: [...] ```\\n\\n                        # Upload a file with metadata\\nresp = assistant.upload_file(\\n   file_path=\\\"./annual-filings-10-k.pdf\\\",\\n   metadata={\\\"quarter\\\": \\\"Q4-2023\\\"},\\n)\\n\\n# List files with a filter\\nassistant.list_files(filter={\\\"month\\\": {\\\"$eq\\\": \\\"october\\\"}})\", \"score\": 0.61972505}, {\"title\": \"August 2024 Product Update - Pinecone\", \"url\": \"https://www.pinecone.io/blog/august-2024-product-update/\", \"content\": \"Reranking is now available with Pinecone Inference, an API that provides instant access to fully managed models hosted on Pinecone’s infrastructure. You can now embed, manage, query, and rerank with Pinecone via a single API.\\n\\nIntegrating reranking into your RAG applications can improve accuracy and reduce overall latency and cost. Check out our guide and Colab notebook and start building more accurate AI applications with Reranking today. [...] We are also announcing API Key Roles in early access to enable Project Owners to set granular access controls – NoAccess, ReadOnly, or ReadWrite – for both the Control Plane and Data Plane within Pinecone serverless.\\n\\n### Build faster with new SDKs\\n\\nWe have also added several new SDKs for Go, C# and Scala.\\n\\nScala (community developed)\\n\\nLink: \\n\\nGo\\n\\nLink:\\n\\nC#\\n\\nLink:\\n\\nShare:\\n\\nWas this article helpful?\\n\\nRecommended for you\\n\\nFurther Reading [...] Opens in a new window Opens an external website Opens an external website in a new window\\n\\nThis website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising. To learn more, view the following link:    Cookie Policy\\n\\nNew case study: Delphi is using Pinecone to create over 100 million conversational agents with <30% of their total response time spent on retrieval. - Learn more\\n\\n← Blog\", \"score\": 0.61972505}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Microsoft Research Blog - GraphRAG\", \"url\": \"https://microsoft.github.io/graphrag/blog_posts/\", \"content\": \"# Microsoft Research Blog\\n\\n GraphRAG: Unlocking LLM discovery on narrative private data\\n\\n  ###### Published February 13, 2024\\n\\n  By Jonathan Larson, Senior Principal Data Architect; Steven Truitt, Principal Program Manager\\n GraphRAG: New tool for complex data discovery now on GitHub\\n\\n  ###### Published July 2, 2024 [...] By Julian Whiting, Senior Machine Learning Engineer; Zachary Hills , Senior Software Engineer; Alonso Guevara Fernández, Sr. Software Engineer; Ha Trinh, Senior Data Scientist; Adam Bradley , Managing Partner, Strategic Research; Jonathan Larson, Senior Principal Data Architect\\n GraphRAG: Improving global search via dynamic community selection\\n\\n  ###### Published November 15, 2024 [...] By Bryan Li, Research Intern; Ha Trinh, Senior Data Scientist; Darren Edge, Senior Director; Jonathan Larson, Senior Principal Data Architect\\n LazyGraphRAG: Setting a new standard for quality and cost\\n\\n  ###### Published November 25, 2024\\n\\n  By Darren Edge, Senior Director; Ha Trinh, Senior Data Scientist; Jonathan Larson, Senior Principal Data Architect\\n Moving to GraphRAG 1.0 – Streamlining ergonomics for developers and users\\n\\n  ###### Published December 16, 2024\", \"score\": 0.8849276}, {\"title\": \"Moving to GraphRAG 1.0 - Streamlining ergonomics for ...\", \"url\": \"https://www.microsoft.com/en-us/research/blog/moving-to-graphrag-1-0-streamlining-ergonomics-for-developers-and-users/\", \"content\": \"Microsoft debuted the pre-release version of GraphRAG (opens in new tab) in July 2024 to advance AI use in complex domains. Since that time, we’ve seen incredible adoption and community engagement (over 20k stars and 2k forks on GitHub as of this writing), with numerous fixes and improvements by the core team and community contributors. We’re deeply grateful for the contributions and feedback we’ve received and are excited to share a number of major ergonomic and structural improvements that [...] In August 2024 we introduced a standalone API layer to simplify developer usage. The original CLI contained all the code required to instantiate and execute basic indexing and query commands, which users often needed to replicate. The API layer is still considered provisional as we gather feedback, but is intended to be the primary entry point for developers who wish to integrate GraphRAG functionality into their own applications without deep pipeline or query class customization. In fact, the [...] Senior Data Scientist\\n\\nLearn more\\n\\n## Continue reading\\n\\n### LazyGraphRAG: Setting a new standard for quality and cost\\n\\n### GraphRAG: Improving global search via dynamic community selection\\n\\n### GraphRAG: New tool for complex data discovery now on GitHub\\n\\n### GraphRAG: Unlocking LLM discovery on narrative private data\\n\\nSee all blog posts\\n\\n## Research Areas\\n\\n Artificial intelligence\\n Data platforms and analytics\\n\\n## Research Groups\\n\\n Societal Resilience\\n Microsoft Research Special Projects\", \"score\": 0.84277284}, {\"title\": \"microsoft/graphrag: A modular graph-based Retrieval ... - GitHub\", \"url\": \"https://github.com/microsoft/graphrag\", \"content\": \"| LICENSE | | LICENSE | Initial Release | Jul 1, 2024 |\\n| RAI\\\\_TRANSPARENCY.md | | RAI\\\\_TRANSPARENCY.md | Initial Release | Jul 1, 2024 |\\n| README.md | | README.md | Update docs for 2.0+ (   Update docs   Fix prompt links\\\")#1984)   Update docs   Fix prompt links\\\") | Jun 23, 2025 |\\n| SECURITY.md | | SECURITY.md | Initial Release | Jul 1, 2024 |\\n| SUPPORT.md | | SUPPORT.md | Initial Release | Jul 1, 2024 | [...] | .gitattributes | | .gitattributes | move mkdocs-typer to devdeps (   move mkdocs-typer to devdeps   add .gitattributes for toml parsing issues on Windows CI   bump timeout  ---------  Co-authored-by: Alonso Guevara <alonsog@microsoft.com>\\\")#1331)   move mkdocs-typer to devdeps   add .gitattributes for toml parsing issues on Windows CI   bump timeout  ---------  Co-authored-by: Alonso Guevara <alonsog@microsoft.com>\\\") | Oct 30, 2024 | [...] | .vsts-ci.yml | | .vsts-ci.yml | Initial Release | Jul 1, 2024 |\\n| CHANGELOG.md | | CHANGELOG.md | Release v2.5.0 (\\\")#2028)\\\") | Aug 14, 2025 |\\n| CODEOWNERS | | CODEOWNERS | Stabilize smoke tests for query community context building (   Stabilize smoke tests for query community context building   Fix CODEOWNERS\\\")#908)   Stabilize smoke tests for query community context building   Fix CODEOWNERS\\\") | Aug 12, 2024 |\\n| CODE\\\\_OF\\\\_CONDUCT.md | | CODE\\\\_OF\\\\_CONDUCT.md | Initial Release | Jul 1, 2024 |\", \"score\": 0.6607204}, {\"title\": \"GraphRAG: Unlocking LLM discovery on narrative private ...\", \"url\": \"https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/\", \"content\": \"# GraphRAG: Unlocking LLM discovery on narrative private data\\n\\nPublished\\n\\nBy \\nJonathan Larson\\n,\\n\\nPartner Data Architect \\n\\nSteven Truitt\\n,\\n\\nPrincipal Program Manager\\n\\nShare this page\\n\\n Share on Facebook\\n Share on X\\n Share on LinkedIn\\n Share on Reddit\\n Subscribe to our RSS feed\\n\\nEditor’s note, Apr. 2, 2024 – Figure 1 was updated to clarify the origin of each source. [...] ### GraphRAG: Improving global search via dynamic community selection\\n\\n### GraphRAG: New tool for complex data discovery now on GitHub\\n\\nSee all blog posts\\n\\n## Research Areas\\n\\n Artificial intelligence\\n\\n## Related tools\\n\\n Graspologic\\n\\n## Related projects\\n\\n Project GraphRAG\\n\\nFollow us:\\n\\n Follow on X\\n Like on Facebook\\n Follow on LinkedIn\\n Subscribe on Youtube\\n Follow on Instagram\\n Subscribe to our RSS feed\\n\\nShare this page:\\n\\n Share on X\\n Share on Facebook\\n Share on LinkedIn\\n Share on Reddit\", \"score\": 0.64986813}, {\"title\": \"GraphRAG: New tool for complex data discovery now on ...\", \"url\": \"https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/\", \"content\": \"Earlier this year, we introduced GraphRAG, a graph-based approach to retrieval-augmented generation (RAG) that enables question-answering over private or previously unseen datasets. Today, we’re pleased to announce that GraphRAG is now available on GitHub (opens in new tab), offering more structured information retrieval and comprehensive response generation than naive RAG approaches. The GraphRAG code repository is complemented by a solution accelerator (opens in new tab), providing an [...] # GraphRAG: New tool for complex data discovery now on GitHub\\n\\nPublished\\n\\nBy \\nDarren Edge\\n,\\n\\nSenior Director \\n\\nHa Trinh\\n,\\n\\nSenior Data Scientist \\n\\nSteven Truitt\\n,\\n\\nPrincipal Program Manager \\n\\nJonathan Larson\\n,\\n\\nPartner Data Architect\\n\\nShare this page\\n\\n Share on Facebook\\n Share on X\\n Share on LinkedIn\\n Share on Reddit\\n Subscribe to our RSS feed\\n\\nDownload\\nGraphRAG\\n\\nDownload\\nGraphRAG Accelerator\", \"score\": 0.6399392}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Optimize Vector Databases, Enhance RAG-Driven ...\", \"url\": \"https://milvus.io/blog/optimize-vector-databases-enhance-rag-driven-generative-ai.md\", \"content\": \"+ What is Milvus\\n  + Use Cases\\n Docs\\n + Bootcamp\\n  + Demos\\n  + Video\\n + Attu\\n  + Milvus CLI\\n  + Sizing Tool\\n  + Milvus Backup\\n  + VTS\\n  + Deep Searcher\\n  + Claude Context\\n Blog\\n + Discord\\n  + GitHub\\n  + Milvus Office Hours\\n  + More Channels\\n\\nStar\\n36.8K\\nContact Us\\nTry Managed Milvus\\n\\n Home\\n Blog\\n Optimize Vector Databases, Enhance RAG-Driven Generative AI\\n\\n# Optimize Vector Databases, Enhance RAG-Driven Generative AI\\n\\n Engineering\\n\\nMay 12, 2024\\nCathy Zhang, Dr. Malini Bhandaru [...] We changed the merge buffer logic to directly append the new data to be inserted into the old data, avoiding allocating a new buffer and moving the large old data. Perf profiles confirm that there is no overhead to this logic. The microcode metrics metric\\\\_CPU operating frequency and metric\\\\_CPU utilization indicate an improvement that is consistent with the system not having to wait for the long memory movement anymore. Load latency improved by more than 60 percent. The improvement is captured\", \"score\": 0.7463059}, {\"title\": \"Building RAG with Milvus Lite, Llama3, and LlamaIndex\", \"url\": \"https://zilliz.com/learn/build-rag-with-milvus-lite-llama3-and-llamaindex\", \"content\": \"English 日本語 한국어 Español Français Deutsch Italiano Português Русский\\n\\n    101\\n\\nBuilding RAG with Milvus Lite, Llama3, and LlamaIndex\\n\\nJul 29, 2024 13 min read\\n\\nRetrieval Augmented Generation (RAG) is a method for mitigating LLM hallucinations. Learn how to build a chatbot RAG with Milvus, Llama3, and LlamaIndex.\\n\\nByRuben Winastwan\\n\\nRead the entire series [...] Published Time: 2024-07-29T00:00:00.000Z\\n\\nHow to Build Retrieval Augmented Generation (RAG) with Milvus Lite, Llama3 and LlamaIndex - Zilliz Learn\\n\\n\\n\\n   Products\\n\\nZilliz Cloud Fully-managed vector database service designed for speed, scale and high performance.Zilliz Cloud vs. Milvus Milvus Open-source vector database built for billion-scale vector similarity search.  \\n       BYOC\\n       Migration\\n       Benchmark\\n       Integrations\\n       Open Source\\n       Support Portal [...] Generative AI Resource Hub | Zilliz\\n\\n   Exploring Retrieval Augmented Generation (RAG): Chunking, LLMs, and Evaluations\\n\\n   A Guide to Chunking Strategies for Retrieval Augmented Generation (RAG)\\n\\n   Mastering LLM Challenges: An Exploration of RAG\\n\\n   Will RAG Be Killed by Long-Context LLMs?\\n\\n   Retrieval Augmented Generation with Citations\\n\\nUpdated on Aug 06, 2024\\n\\n   Image 6: Ruben WinastwanRuben Winastwan \\n\\nprevious\", \"score\": 0.7318455}, {\"title\": \"Milvus in 2023: An Unprecedented Vector Database ...\", \"url\": \"https://milvus.io/blog/milvus-in-2023-unprecedented-vector-database-amidst-tech-buzz.md\", \"content\": \"+ What is Milvus\\n  + Use Cases\\n Docs\\n + Bootcamp\\n  + Demos\\n  + Video\\n + Attu\\n  + Milvus CLI\\n  + Sizing Tool\\n  + Milvus Backup\\n  + VTS\\n  + Deep Searcher\\n  + Claude Context\\n Blog\\n + Discord\\n  + GitHub\\n  + Milvus Office Hours\\n  + More Channels\\n\\nStar\\n36.8K\\nContact Us\\nTry Managed Milvus\\n\\n Home\\n Blog\\n Milvus in 2023: An Unprecedented Vector Database Amidst Tech Buzz\\n\\n# Milvus in 2023: An Unprecedented Vector Database Amidst Tech Buzz\\n\\nJanuary 04, 2024\\nJames Luan\\n\\nThis image is generated by AI. [...] As we enter 2024, it’s an sensible moment to reflect on the entire vector database industry, with a special focus on Milvus—a standout product in this landscape.\\n\\n## Milvus in 2023: numbers don’t lie [...] Retrieval Augmented Generation (RAG) was the most popular use case for vector databases in 2023. However, the increase in vector data volumes with RAG applications presents a storage challenge for these applications. This challenge is especially true when the volume of transformed vectors exceeds that of the original document chunks, potentially escalating memory usage costs. For example, after dividing documents into chunks, the size of a 1536-dimensional float32 vector (roughly 3kb)\", \"score\": 0.70702106}, {\"title\": \"Inkeep Built an Efficient RAG-driven AI Assistant using Milvus\", \"url\": \"https://zilliz.com/blog/how-inkeep-and-milvus-built-rag-driven-ai-assisstant-for-smarter-interaction\", \"content\": \"Published Time: 2024-11-08T00:00:00.000Z\\n\\nInkeep Built an Efficient RAG-driven AI Assistant using Milvus - Zilliz blog\\n\\n\\n\\n   Products\\n\\nZilliz Cloud Fully-managed vector database service designed for speed, scale and high performance.Zilliz Cloud vs. Milvus Milvus Open-source vector database built for billion-scale vector similarity search.  \\n       BYOC\\n       Migration\\n       Benchmark\\n       Integrations\\n       Open Source\\n       Support Portal [...] To learn more about how Milvus and Inkeep built this AI assistant, check out the replay of Robert’s talk on YouTube.\\n\\nFurther Reading\\n\\n   Generative AI Resource Hub | Zilliz\\n\\n   Top Performing AI Models for Your GenAI Apps | Zilliz\\n\\n   Build AI Apps with Milvus: Tutorials & Notebooks\\n\\n   Join the Milvus Community of AI Developers\\n\\n   What is RAG?\\n\\nUpdated on Nov 08, 2024\\n\\n   Image 11: Ruben WinastwanRuben Winastwan \\n\\n### Content\", \"score\": 0.69099766}, {\"title\": \"Our Journey to 35K+ GitHub Stars: Building ...\", \"url\": \"https://milvus.io/blog/journey-to-35k-github-stars-story-of-building-milvus-from-scratch.md\", \"content\": \"2023 was the year of RAG (retrieval-augmented generation). Suddenly, semantic search went from an interesting AI technique to essential infrastructure for chatbots, document Q&A systems, and AI agents.\\n\\nThe GitHub stars of Milvus spiked. Support requests multiplied. Developers who had never heard of vector databases were suddenly asking sophisticated questions about indexing strategies and query optimization. [...] Then came 2024. Forrester named us a leader in the vector database category. Milvus passed 30,000 GitHub stars. And we realized: the road we’d been paving for seven years had finally become the highway. As more enterprises adopted vector databases as critical infrastructure, our business growth accelerated rapidly—validating that the foundation we’d built could scale both technically and commercially.\\n\\n## The Team Behind Milvus: Zilliz\", \"score\": 0.6641271}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2022-04-05\n",
      "Title: Positive and Negative Critiquing for VAE-based Recommenders\n",
      "Authors: Diego Antognini, Boi Faltings\n",
      "Summary: Providing explanations for recommended items allows users to refine the\n",
      "recommendations by critiquing parts of the explanations. As a result of\n",
      "revisiting critiquing from the perspective of multimodal generative models,\n",
      "recent work has proposed M&Ms-VAE, which achieves state-of-the-art performance\n",
      "in terms of recommendation, explanation, and critiquing. M&Ms-VAE and similar\n",
      "models allow users to negatively critique (i.e., explicitly disagree). However,\n",
      "they share a significant drawback: users cannot positively critique (i.e.,\n",
      "highlight a desired feature). We address this deficiency with M&Ms-VAE+, an\n",
      "extension of M&Ms-VAE that enables positive and negative critiquing. In\n",
      "addition to modeling users' interactions and keyphrase-usage preferences, we\n",
      "model their keyphrase-usage dislikes. Moreover, we design a novel critiquing\n",
      "module that is trained in a self-supervised fashion. Our experiments on two\n",
      "datasets show that M&Ms-VAE+ matches or exceeds M&Ms-VAE in recommendation and\n",
      "explanation performance. Furthermore, our results demonstrate that representing\n",
      "positive and negative critiques differently enables M&Ms-VAE+ to significantly\n",
      "outperform M&Ms-VAE and other models in positive and negative multi-step\n",
      "critiquing.\n",
      "\n",
      "Published: 2023-07-11\n",
      "Title: RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs\n",
      "Authors: Afra Feyza Akyürek, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, Niket Tandon\n",
      "Summary: Despite their unprecedented success, even the largest language models make\n",
      "mistakes. Similar to how humans learn and improve using feedback, previous work\n",
      "proposed providing language models with natural language feedback to guide them\n",
      "in repairing their outputs. Because human-generated critiques are expensive to\n",
      "obtain, researchers have devised learned critique generators in lieu of human\n",
      "critics while assuming one can train downstream models to utilize generated\n",
      "feedback. However, this approach does not apply to black-box or limited access\n",
      "models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\n",
      "large general-purpose language agents, fine-tuning is neither computationally\n",
      "nor spatially efficient as it results in multiple copies of the network. In\n",
      "this work, we introduce RL4F (Reinforcement Learning for Feedback), a\n",
      "multi-agent collaborative framework where the critique generator is trained to\n",
      "maximize end-task performance of GPT-3, a fixed model more than 200 times its\n",
      "size. RL4F produces critiques that help GPT-3 revise its outputs. We study\n",
      "three datasets for action planning, summarization and alphabetization and show\n",
      "relative improvements up to 10% in multiple text similarity metrics over other\n",
      "learned, retrieval-augmented or prompting-based critique generators.\n",
      "\n",
      "Published: 2023-10-07\n",
      "Title: Critique Ability of Large Language Models\n",
      "Authors: Liangchen Luo, Zi Lin, Yinxiao Liu, Lei Shu, Yun Zhu, Jingbo Shang, Lei Meng\n",
      "Summary: Critical thinking is essential for rational decision-making and\n",
      "problem-solving. This skill hinges on the ability to provide precise and\n",
      "reasoned critiques and is a hallmark of human intelligence. In the era of large\n",
      "language models (LLMs), this study explores the ability of LLMs to deliver\n",
      "accurate critiques across various tasks. We are interested in this topic as a\n",
      "capable critic model could not only serve as a reliable evaluator, but also as\n",
      "a source of supervised signals for model tuning. Particularly, if a model can\n",
      "self-critique, it has the potential for autonomous self-improvement. To examine\n",
      "this, we introduce a unified evaluation framework for assessing the critique\n",
      "abilities of LLMs. We develop a benchmark called CriticBench, which comprises\n",
      "3K high-quality natural language queries and corresponding model responses; and\n",
      "annotate the correctness of these responses. The benchmark cover\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"What is RAG? Understand the Next Evolution of GenAI - Cohere\", \"url\": \"https://cohere.com/blog/what-is-rag\", \"content\": \"On-Demand Events\\n\\nCompany\\n\\nCompany\\n\\nAbout\\n\\nCareers\\n\\nNewsroom\\n\\nSign in\\n\\nWhat is RAG? Understanding the latest evolution of GenAI\\n\\nDec 12, 2024\\n\\nRAG (retrieval-augmented generation) is a way to optimize output of a large language model by allowing it to access information outside its training data.\\n\\nShare:\\n\\nReady to put AI to work?\\n\\nProducts\\n\\nNorth\\n\\nCompass\\n\\nCommand\\n\\nEmbed\\n\\nRerank\\n\\nCustomization\\n\\nFine-tuning\\n\\nPricing\\n\\nDeployment Options\\n\\nCohere in Slack\\n\\nSolutions\\n\\nEnergy and Utilities [...] Financial Services\\n\\nHealthcare and Life Sciences\\n\\nManufacturing\\n\\nPublic Sector\\n\\nResources\\n\\nBlog\\n\\nDevelopers\\n\\nEvents\\n\\nOn-Demand Events\\n\\nLLM University\\n\\nDocumentation\\n\\nRelease Notes\\n\\nCompany\\n\\nAbout\\n\\nCareers\\n\\nResearch\\n\\nNewsroom\\n\\nPartners\\n\\nSecurity\\n\\nTrust Center\\n\\nNorth\\n\\nCompass\\n\\nCommand\\n\\nEmbed\\n\\nRerank\\n\\nCustomization\\n\\nFine-tuning\\n\\nPricing\\n\\nDeployment Options\\n\\nCohere in Slack\\n\\nEnergy and Utilities\\n\\nFinancial Services\\n\\nHealthcare and Life Sciences\\n\\nManufacturing\\n\\nPublic Sector\\n\\nBlog\\n\\nDevelopers [...] Best-in-class AI security and data protection\\n\\nDeployment Options\\n\\nPartners\\n\\nResearch\\n\\nCohere Labs\\n\\nCohere's research lab that seeks to solve complex ML problems\\n\\nModel\\n\\nAya\\n\\nPapers\\n\\nVideos\\n\\nBlog\\n\\nPress\\n\\nInitiatives\\n\\nOpen Science Community\\n\\nScholars Program\\n\\nAI Research Grant Program\\n\\nEvents\\n\\nResources\\n\\nResources\\n\\nBlog\\n\\nDevelopers\\n\\nDocs\\n\\nLLM University\\n\\nCookbooks\\n\\nCustomer Stories\\n\\nBambooHR\\n\\nNotion\\n\\nOracle\\n\\nJohnson Lambert\\n\\nAtomicwork\\n\\nBorderless AI\\n\\nBlueDot\\n\\nCommunity\\n\\nDiscord\\n\\nEvents\", \"score\": 0.6504012}, {\"title\": \"Chunking for RAG: Enhance Enterprise Knowledge Retrieval with ...\", \"url\": \"https://cohere.com/blog/chunking-for-rag-maximize-enterprise-knowledge-retrieval\", \"content\": \"< Back to blog\\n\\n# Chunking for RAG: Maximize enterprise knowledge retrieval\\n\\nKasim Patel\\n\\nOct 30, 2024\\n\\nLearn how to approach some of the most common challenges around chunking – a vital skill to tap enterprise knowledge and succeed with RAG solutions.\\n\\nDevelopers\\n\\nDevelopers\\n\\nShare:\\n\\nAs enterprises tap into their vast knowledge bases to leverage generative AI, mastering the critical skill of chunking has become essential for optimizing retrieval-augmented generation (RAG) systems.\", \"score\": 0.5524679}, {\"title\": \"Cohere's Command R7B Model\", \"url\": \"https://docs.cohere.com/docs/command-r7b\", \"content\": \"| Model Name | Description | Modality | Context Length | Maximum Output Tokens | Endpoints |\\n ---  ---  --- |\\n| `command-r7b-12-2024` | `command-r7b-12-2024` is a small, fast update delivered in December 2024. It excels at RAG, tool use, agents, and similar tasks requiring complex reasoning and multiple steps. | Text | 128k | 4k | Chat |\\n\\n## What Can Command R7B Be Used For?\\n\\nCommand R7B is excellent for: [...] Command R7B is available today on the Cohere Platform as well as accessible on HuggingFace, or you can access it in the SDK with `command-r7b-12-2024`. For more information, check out our dedicated blog post.\\n\\n## Model Details\", \"score\": 0.5370662}, {\"title\": \"RAG is here to stay: Four reasons why large context windows can't ...\", \"url\": \"https://cohere.com/blog/rag-is-here-to-stay\", \"content\": \"Company\\n\\n       About\\n       Careers\\n       Newsroom\\n\\nRequest a demo\\n\\nSign in\\n\\nImage 15: Blog Post Featured Image\\n\\n< Back to blog\\nRAG is here to stay: Four reasons why large context windows can't replace it\\n\\nImage 16: Image of Jay AlammarImage 17: Image of Maxime VoisinImage 18: Image of Sam BarnettMultiple Authors\\nApr 19, 2024\\n\\nImage 19: Blog Post Featured Image\\n\\nDevelopersFor Business\\n\\nDevelopersFor Business\\n\\nShare:\\n\\nImage 20: X (Formerly Twitter) IconImage 21: LinkedIn Icon [...] Products North Compass Command Embed Rerank Customization Fine-tuning Pricing Deployment Options Cohere in Slack   \\n   Solutions Energy and Utilities Financial Services Healthcare and Life Sciences Manufacturing Public Sector   \\n   Resources Blog Developers Events On-Demand Events LLM University Documentation Release Notes   \\n   Company About Careers Research Newsroom Partners Security Trust Center [...] Products\\n   North\\n   Compass\\n   Command\\n   Embed\\n   Rerank\\n   Customization\\n   Fine-tuning\\n   Pricing\\n   Deployment Options\\n   Cohere in Slack\\n\\n   Solutions\\n   Energy and Utilities\\n   Financial Services\\n   Healthcare and Life Sciences\\n   Manufacturing\\n   Public Sector\\n\\n   Resources\\n   Blog\\n   Developers\\n   Events\\n   On-Demand Events\\n   LLM University\\n   Documentation\\n   Release Notes\\n\\n   Company\\n   About\\n   Careers\\n   Research\\n   Newsroom\\n   Partners\\n   Security\\n   Trust Center\", \"score\": 0.48191768}, {\"title\": \"Cohere Command R+ (08-2024)\", \"url\": \"https://docs.oracle.com/en-us/iaas/Content/generative-ai/cohere-command-r-plus-08-2024.htm\", \"content\": \"For the dedicated mode, the response length isn't capped off and the context length is 128,000 tokens.\\n Improved math, coding, and reasoning skills.\\n Enhanced multilingual retrieval-augmented generation (RAG) feature with customizable citation options.\\n For dedicated inferencing, create a dedicated AI cluster and endpoint and host the model on the cluster. [...] - Getting Started\\n    - Getting Access\\n    - Using the Large Language Models (LLMs)\\n    - Cluster Performance Benchmarks\\n    - Managing Dedicated AI Clusters\\n    - Fine-Tuning the Base Models\\n    - Managing the Custom Models\\n    - Managing an Endpoint\\n    - Integrating the Models\\n    - Model Limitations\\n    - Calculating Cost\\n    - Service Limits\\n    - Metrics\\n    - Retiring the Models\\n    - Data Handling\\n    - Indemnification\\n  + Generative AI Agents [...] - Getting Started\\n    - Getting Access\\n    - Using the Large Language Models (LLMs)\\n    - Cluster Performance Benchmarks\\n    - Managing Dedicated AI Clusters\\n    - Fine-Tuning the Base Models\\n    - Managing the Custom Models\\n    - Managing an Endpoint\\n    - Integrating the Models\\n    - Model Limitations\\n    - Calculating Cost\\n    - Service Limits\\n    - Metrics\\n    - Retiring the Models\\n    - Data Handling\\n    - Indemnification\\n  + Generative AI Agents\", \"score\": 0.4718322}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Real time RAG with Bytewax and Haystack 2.0\", \"url\": \"https://bytewax.substack.com/p/real-time-rag-with-bytewax-and-haystack\", \"content\": \"Published Time: 2024-05-30T15:17:18+00:00\\n\\nReal time RAG with Bytewax and Haystack 2.0\\n\\nImage 1: Bytewax\\n\\nBytewax\\n\\nSubscribe Sign in\\n\\n#### Share this post\\n\\nImage 2 Image 3: Bytewax Bytewax Real time RAG with Bytewax and Haystack 2.0\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNotes\\n\\nMore\\n\\nImage 4: User's avatar\\n\\nDiscover more from Bytewax\\n\\nA newsletter to discuss Python, data streaming, Bytewax, and more.\\n\\nOver 1,000 subscribers\\n\\nSubscribe [...] By subscribing, I agree to Substack's Terms of Use, and acknowledge its Information Collection Notice and Privacy Policy.\\n\\n \\n\\nAlready have an account? Sign in\\n\\nReal time RAG with Bytewax and Haystack 2.0\\n\\n### Future of AI 🚀 - Building Dynamic Real-Time Responses with RAG.\\n\\nImage 5: Laura Funderburk's avatar\\n\\nLaura Funderburk\\n\\nMay 30, 2024\\n\\n#### Share this post\\n\\nImage 6 Image 7: Bytewax Bytewax Real time RAG with Bytewax and Haystack 2.0\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNotes\\n\\nMore\\n\\n\\n\\nImage 8 [...] Image 15A guest post by Laura Funderburk Senior Developer Advocate at Bytewax\\n\\n#### Discussion about this post\\n\\nComments Restacks\\n\\nImage 16: User's avatar\\n\\nTop Latest Discussions\\n\\nBuilding Real-time RAG Systems with Python\\n\\nCrack the Code of Real-Time NLP with RAG and Bytewax 💥\\n\\nJul 11, 2024•\\n\\nLaura Funderburk\\n\\n6\\n\\n#### Share this post\\n\\nImage 17 Image 18: Bytewax Bytewax Building Real-time RAG Systems with Python\\n\\nCopy link\\n\\nFacebook\\n\\nEmail\\n\\nNotes\\n\\nMore\\n\\n[](\\n\\nImage 19\", \"score\": 0.72977066}, {\"title\": \"Building a RAG Pipeline with Milvus and Haystack 2.0 - Zilliz Learn\", \"url\": \"https://zilliz.com/learn/using-milvus-and-haystack-for-building-efficient-rag-pinepipes\", \"content\": \"Published Time: 2024-05-05T00:00:00.000Z\\n\\nBuilding a RAG Pipeline with Milvus and Haystack 2.0 - Zilliz Learn\\n\\n\\n\\n   Products\\n\\nZilliz Cloud Fully-managed vector database service designed for speed, scale and high performance.Zilliz Cloud vs. Milvus Milvus Open-source vector database built for billion-scale vector similarity search.  \\n       BYOC\\n       Migration\\n       Benchmark\\n       Integrations\\n       Open Source\\n       Support Portal\\n\\nHigh-Performance Vector Database Made Serverless. [...] Milvus documentation: \\n\\n   Milvus community (GitHub, Discord, Reddit, Twitter): \\n\\n   Haystack documentation: \\n\\n   Haystack tutorials: \\n\\n   Haystack API reference: \\n\\n   Haystack Discord: \\n\\nUpdated on Sep 02, 2024 [...] Haystack (by deepset.ai) is an open-source Python framework for building production-ready LLM applications, retrieval-augmented generative pipelines, and search systems that work intelligently over large document collections. Haystack 2.0 is a major rework of the previous version that allows you to implement composable AI systems that are easy to use, customize, extend, optimize, evaluate, and ultimately deploy to production.\", \"score\": 0.7144891}, {\"title\": \"Creating a Simple RAG Pipeline using Haystack 2.0 - GoPenAI\", \"url\": \"https://blog.gopenai.com/creating-a-simple-rag-pipeline-using-haystack-2-0-c84c7c660569\", \"content\": \"Sitemap\\n\\nOpen in app\\n\\nSign in\\n\\nSign in\\n\\n·\\n\\nFollow publication\\n\\nWhere the ChatGPT community comes together to share insights and stories.\\n\\nFollow publication\\n\\n# Creating a Simple RAG Pipeline using Haystack 2.0\\n\\nAiden Lang\\n\\n7 min readAug 14, 2024\\n\\n## What is RAG? [...] After running the file, your console should look something like this:\\n\\nOur model was able to query the NFL box score data from Hugging Face and return an accurate Bears score from the 2023 season!\\n\\n## Why Haystack 2.0?\\n\\nRAG is a powerful technique that continues to grow in sophistication as research develops. It has proven to be a fertile ground for innovation, sprouting new variants and applications that push the boundaries of AI capabilities.\\n\\nSo why choose Haystack as your framework? [...] Retrieval-Augmented Generation (RAG) is a budding area of exploration in the world of machine learning. Large Language Models such as ChatGPT and Claude are undoubtedly powerful, but they have an inherent limitation: their knowledge base is generalized and not tailored to specific domains or datasets. This means they may struggle with highly specialized or proprietary information that wasn’t part of their training data. They’re locked in a time capsule, their knowledge frozen at the moment of\", \"score\": 0.6549173}, {\"title\": \"Haystack 2.0-Beta · deepset-ai haystack · Discussion #6684 - GitHub\", \"url\": \"https://github.com/deepset-ai/haystack/discussions/6684\", \"content\": \"{{ message }}\\n\\ndeepset-ai \\n/\\nhaystack\\nPublic\\n\\n Notifications You must be signed in to change notification settings\\n Fork\\n  2.3k\\n Star\\n   22k\\n\\n# Haystack 2.0-Beta #6684\\n\\nClosed\\n\\nbilgeyucel\\nannounced in\\nAnnouncements\\n\\nHaystack 2.0-Beta\\n#6684\\n\\nbilgeyucel\\n\\nJan 4, 2024\\n·\\n8 comments\\n·\\n11 replies\\n\\nReturn to top\\n\\nDiscussion options\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page. [...] ## bilgeyucel Jan 4, 2024 Maintainer\\n\\nOriginal comment in English - [...] Comment options\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\n#### masci Jan 9, 2024\\n\\n-\\n\\n|  |\\n\\n| @basharullah you're trying to run Haystack 1.x code after installing Haystack 2.0. Please refer to the 2.x docs to solve your problem. Note that Haystack 2.0 is still in beta, and a full port of your 1.x code might not be possible straight ahead. |\\n\\nBeta\\nWas this translation helpful?\\nGive feedback.\\n\\nAll reactions\\n\\nComment options\\n\\n### Uh oh!\", \"score\": 0.5815176}, {\"title\": \"A Practical Guide to RAG with Haystack and LangChain - DigitalOcean\", \"url\": \"https://www.digitalocean.com/community/tutorials/production-ready-rag-pipelines-haystack-langchain\", \"content\": \"OpenAI’s latest generations (particularly text-embedding-3-small and text-embedding-3-large, released in late 2024) have further improved quality or lowered prices, but ada-002 remains widely available and a “safe default” for most production scenarios.\\n\\n### Open-Source and Specialized Alternatives\\n\\nFor many organizations, open-source or self-hosted options are preferable for reasons of privacy, fine-tuning, or cost avoidance. [...] Haystack (by Deepset) is an open-source framework focused on RAG and other use cases for LLMs. The central feature of Haystack, further emphasized in the most recent release, is its pipeline-centric, modular architecture. Haystack adopts a graph-like approach, where each component (reader, retriever, generator, etc.) represents a node in a directed acyclic graph (DAG) pipeline. Haystack pipelines are composable and customizable – nodes can be added, removed, and replaced with minimal side [...] | RAG-Optimized | Tailored for retrieval-based question answering, Haystack comes with out-of-the-box support and utilities for popular tricks and new techniques such as HyDE and query expansion. Multi-hop pipelines, agents, and other complex retrieval patterns are supported in Haystack 2.0. |\", \"score\": 0.5236402}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"OpenAI Embeddings: The Game-Changer for Natural Language ...\", \"url\": \"https://mlconference.ai/blog/openai-embeddings-technology-2024/\", \"content\": \"OpenAI’s latest innovations in the field of embeddings, introduced at the end of January 2024, mark a significant advance in this technology. With the introduction of the new text-embedding-3-small and text-embedding-3-large models, OpenAI now offers more powerful and cost-efficient options for developers. These models not only show improved performance in standardized benchmarks, but also offer the ability to find the right balance between performance and memory requirements on a [...] At the end of January 2024, OpenAI presented new, improved models for embeddings:\\n\\ntext-embedding-3-small: A smaller, more efficient model with improved performance compared to its predecessor. It performs better in benchmarks and is significantly cheaper.\\n\\n text-embedding-3-large: A larger model that is more powerful and creates embeddings with up to 3072 dimensions. It shows the best performance in the benchmarks but is slightly more expensive than ada v2. [...] New embeddings models\\n\\n At the end of January 2024, OpenAI introduced new models for creating embeddings. All code examples and result values contained in this article already refer to the new text-embedding-3-large model.\\n\\nMEET OUR MLcon NY SPEAKERS\\n\\nImage 18\\n\\nDr. Pieter Buteneers\\nEmma Legal\\n\\nImage 19\\n\\nNathan Beach\\nGoogle\\n\\nImage 20\\n\\nDebjyoti Paul\\nAmazon\\n\\nCreate embeddings with Python\", \"score\": 0.8687491}, {\"title\": \"Exploring Text-Embedding-3-Large: A Comprehensive Guide to the ...\", \"url\": \"https://www.datacamp.com/tutorial/exploring-text-embedding-3-large-new-openai-embeddings\", \"content\": \"Announced on January 25, 2024, these models are the latest and most powerful embedding models designed to represent text in high-dimensional space, making it easier to have a better understanding of text. [...] Image 3: Free Access Week ##### Learn Python and AI for free! One week only. No credit card needed. Ends in: 4 d 02 h 06 m 38 s Start Now\\nSkip to main content\\n is continuously evolving, with OpenAI consistently leading the way toward innovation.\\nThe introduction of its \\\\\\\\text-embedding-3-large,\\\\\\\\and\\\\\\\\text-embedding-3-small\\\\\\\\models marks a new era in the field, set to change how developers and AI practitioners approach text analysis and embedding tasks. [...] In conclusion, OpenAI's new text-embedding-3-large and text-embedding-3-small models represent a significant leap in the realm of natural language processing, offering a more nuanced and high-dimensional representation of text data.\", \"score\": 0.829285}, {\"title\": \"Embeddings FAQ - OpenAI Help Center\", \"url\": \"https://help.openai.com/en/articles/6824809-embeddings-faq\", \"content\": \"# Embeddings FAQ\\n\\nFAQ for the new and improved embedding models\\n\\nUpdated: 16 days ago\\n\\nOn January 25, 2024 we released two new embeddings models: `text-embedding-3-small` and `text-embedding-3-large`. These are our newest and most performant embedding models with lower costs, higher multilingual performance, and a new parameter for shortening embeddings. Read more.\\n\\n## What's different about the latest embeddings models? [...] Our latest v3 models provide stronger performance on common benchmarks at a reduced price. You can read more about the performance improvements in the announcement blog post and developer documentation.\\n\\n## How can I tell how many tokens a string will have before I try to embed it?\\n\\nYou can use OpenAI's Tiktoken package to check how many tokens a string will have. Learn more in our embeddings developer guide.\\n\\n## How can I retrieve K nearest embedding vectors quickly?\", \"score\": 0.81489426}, {\"title\": \"Open AI 3'rd gen embedding models — what's driving the ... - Medium\", \"url\": \"https://medium.com/@arundona/open-ai-3rd-gen-embedding-models-whats-driving-the-improvements-4c23b88751f1\", \"content\": \"In this blog I will share my experience in comparing Open AI’s previous generation text-embedding-ada-002(released Dec’2022) embedding model vs their 3'rd generation embedding model, text-embedding-3-small (released Jan’2024)\\n\\nDetailed walkthrough of the benchmark tests run by OpenAI is available \\n\\nThe main highlights are [...] Sitemap\\n\\nOpen in app\\n\\nSign in\\n\\nSign in\\n\\n# Open AI 3'rd gen embedding models — what’s driving the improvements?\\n\\nArun Prasad\\n\\n7 min readMar 21, 2024\\n\\nThe bedrock of semantic search is the vector embedding. The effectiveness of the vector embedding to capture (i) the concepts in the content , (ii) similarity and relationships between words, has an outsized impact on the final result of a RAG pipeline.\", \"score\": 0.783542}, {\"title\": \"New OpenAI Embedding Models - Support - Weaviate Forum\", \"url\": \"https://forum.weaviate.io/t/new-openai-embedding-models/1305\", \"content\": \"antas-marcin\\n\\nWeaviate team member\\n\\nJan 2024\\n\\nWeaviate v1.23.6 has been released with support for OpenAI’s new V3 embeddings models.\\n\\nIf you want configure new V3 models you just to need to set `model` property to  \\n`text-embedding-3-small` or `text-embedding-3-large`.\\n\\nYou can also additionally set `dimensions` value, bc with V3 models you can choose vector dimensionality. Here are possible values for `dimensions` setting: [...] text-embedding-3-small and text-embedding-3-large are supposed to be superior to the current text-embedding-ada-002, which is the default for the tex2vec-openai transformer.\\n\\nWhat are the plans to incorporate, and what will be the process if one wishes to change their existing cluster objects to one of the newer models?\\n\\n8\\n\\n4\\n\\n4\\n\\n3\\n\\nread \\n\\n5\\nmin\\n\\nantas-marcin\\n\\nWeaviate team member\\n\\nJan 2024\\n\\nWe are working on adding support for them right now so they should be available later today [...] 📅 Intro to building AI-native applications with Weaviate\\n| September 4th |\\nJoin us online!\\n\\nSkip to main content\\n\\n# New OpenAI Embedding Models\\n\\nSupport\\n\\nYou have selected 0 posts.\\n\\nselect all\\n\\ncancel selecting\\n\\nJan 2024\\n\\n1 / 21\\n\\nJan 2024\\n\\nFeb 2024\\n\\nSomebodySysop\\n\\nJan 2024\\n\\nToday OpenAI announced two new embedding models: New embedding models and API updates\", \"score\": 0.7825466}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Azure OpenAI Responses API - Microsoft Learn\", \"url\": \"https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/responses\", \"content\": \"```json\\n{\\n  \\\"id\\\": \\\"resp_67cbc9705fc08190bbe455c5ba3d6daf\\\",\\n  \\\"created_at\\\": 1741408624.0,\\n  \\\"error\\\": null,\\n  \\\"incomplete_details\\\": null,\\n  \\\"instructions\\\": null,\\n  \\\"metadata\\\": {},\\n  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n  \\\"object\\\": \\\"response\\\",\\n  \\\"output\\\": [\\n    {\\n      \\\"id\\\": \\\"msg_67cbc970fd0881908353a4298996b3f6\\\",\\n      \\\"content\\\": [\\n        {\\n          \\\"annotations\\\": [], [...] client = OpenAI(  \\n  base_url = \\\"\\n  api_key=os.getenv(\\\"AZURE_OPENAI_API_KEY\\\")  \\n)\\n\\nresponse = client.responses.create(\\n    model = \\\"o3\\\",\\n    input = \\\"Write me a very long story\\\",\\n    background = True\\n)\\n\\nwhile response.status in {\\\"queued\\\", \\\"in_progress\\\"}:\\n    print(f\\\"Current status: {response.status}\\\")\\n    sleep(2)\\n    response = client.responses.retrieve(response.id)\\n\\nprint(f\\\"Final status: {response.status}\\\\nOutput:\\\\n{response.output_text}\\\")\\n``` [...] client = OpenAI(\\n    api_key=os.getenv(\\\"AZURE_OPENAI_API_KEY\\\"),\\n    base_url=\\\"\\n)\\n\\nresponse = client.responses.retrieve(\\\"resp_67cb61fa3a448190bcf2c42d96f0d1a8\\\")\\n```\\n\\nImportant\\n\\nUse API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see API keys with Azure Key Vault.\", \"score\": 0.73459625}, {\"title\": \"Web Search and States with Responses API - OpenAI Cookbook\", \"url\": \"https://cookbook.openai.com/examples/responses_api/responses_example\", \"content\": \"```\\n{\\n    \\\"id\\\": \\\"resp_67bd65392a088191a3b802a61f4fba14\\\",\\n    \\\"created_at\\\": 1740465465.0,\\n    \\\"error\\\": null,\\n    \\\"metadata\\\": {},\\n    \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",\\n    \\\"object\\\": \\\"response\\\",\\n    \\\"output\\\": [\\n        {\\n            \\\"id\\\": \\\"msg_67bd653ab9cc81918db973f0c1af9fbb\\\",\\n            \\\"content\\\": [\\n                {\\n                    \\\"annotations\\\": [], [...] ### Mar 11, 2025\\n\\n# Web Search and States with Responses API\\n\\nOpen in GitHub\\nView as Markdown\\n\\nThe Responses API is a new way to interact with OpenAI models, designed to be simpler and more flexible than previous APIs. It makes it easy to build advanced AI applications that use multiple tools, handle multi-turn conversations, and work with different types of data (not just text). [...] One key feature of the Response API is that it is stateful. This means that you do not have to manage the state of the conversation by yourself, the API will handle it for you. For example, you can retrieve the response at any time and it will include the full conversation history.\\n\\n```\\n\\n                            fetched_response = client.responses.retrieve(\\nresponse_id=response.id)\\n\\nprint(fetched_response.output.content.text)\\n\\n                          \\n```\", \"score\": 0.7009158}, {\"title\": \"Introducing Structured Outputs in the API - OpenAI\", \"url\": \"https://openai.com/index/introducing-structured-outputs-in-the-api/\", \"content\": \"`1POST /v1/chat/completions2{3  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",4  \\\"messages\\\": [5    {6      \\\"role\\\": \\\"system\\\",7      \\\"content\\\": \\\"You are a helpful math tutor.\\\"8    },9    {10      \\\"role\\\": \\\"user\\\",11      \\\"content\\\": \\\"solve 8x + 31 = 2\\\"12    }13  ],14  \\\"response_format\\\": {15    \\\"type\\\": \\\"json_schema\\\",16    \\\"json_schema\\\": {17      \\\"name\\\": \\\"math_response\\\",18      \\\"strict\\\": true,19      \\\"schema\\\": {20        \\\"type\\\": \\\"object\\\",21        \\\"properties\\\": {22          \\\"steps\\\": {23            \\\"type\\\": \\\"array\\\",24 [...] `1{2  \\\"model\\\": \\\"gpt-4o-2024-08-06\\\",3  \\\"messages\\\": [4    {5      \\\"role\\\": \\\"system\\\",6      \\\"content\\\": \\\"You are a helpful assistant\\\"7    },8    {9      \\\"role\\\": \\\"user\\\",10      \\\"content\\\": \\\"9.11 and 9.9 -- which is bigger?\\\"11    }12  ],13  \\\"response_format\\\": {14    \\\"type\\\": \\\"json_schema\\\",15    \\\"json_schema\\\": {16      \\\"name\\\": \\\"reasoning_schema\\\",17      \\\"strict\\\": true,18      \\\"schema\\\": {19        \\\"type\\\": \\\"object\\\",20        \\\"properties\\\": {21          \\\"reasoning_steps\\\": {22            \\\"type\\\": \\\"array\\\",23 [...] `1from pydantic import BaseModel23from openai import OpenAI456class Step(BaseModel):7    explanation: str8    output: str91011class MathResponse(BaseModel):12    steps: list[Step]13    final_answer: str141516client = OpenAI()1718completion = client.beta.chat.completions.parse(19    model=\\\"gpt-4o-2024-08-06\\\",20    messages=[21        {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful math tutor.\\\"},22        {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"solve 8x + 31 = 2\\\"},23    ],24\", \"score\": 0.6747586}, {\"title\": \"Using OpenAI API to derive information about your data - Medium\", \"url\": \"https://medium.com/@nick.rios/deep-text-analysis-and-categorization-using-openai-api-function-calls-693ef82c63c6\", \"content\": \"Sitemap\\n\\nOpen in app\\n\\nSign in\\n\\nSign in\\n\\n# Using OpenAI API to derive information about your data\\n\\nNicholas Rios\\n\\n4 min readMay 26, 2024\\n\\nIn this post I am going to use the script we built in:\\n\\nSource code available at:\\n\\nWhat we have so far is a simple API request to OpenAI that saves the results to a text file.\\n\\nindex.js [...] Nicholas Rios\\n\\n## How to quickly setup a tiny OpenAI API bot using nodejs.\\n\\nMay 26, 2024\\n\\nNicholas Rios\\n\\n## Poker statistics to remember\\n\\n### In the old days people thought you needed to be a real math-whiz or a gun-slinger to understand Poker odds. No in-between. But then…\\n\\nDec 22, 2018\\n\\n74\\n\\n1\\n\\nNicholas Rios\\n\\n## Some thoughts about Poker starting hands. [...] > My name is John Smith, and I am writing to report a significant issue with my recently purchased XYZ Model Laptop, serial number ABC123456789, which I bought on April 10, 2024, under order number 987654321. I live at 123 Elm Street, Apt 4B, Springfield, IL, 62701, USA. Approximately two weeks ago, on May 12, 2024, I started experiencing intermittent flickering and blackouts on the display, rendering the laptop unusable for extended periods. Despite updating all drivers and ensuring the system\", \"score\": 0.63125396}, {\"title\": \"Assistant file search text retrieval - API - OpenAI Developer Community\", \"url\": \"https://community.openai.com/t/assistant-file-search-text-retrieval/861886\", \"content\": \"# Assistant file search text retrieval\\n\\nAPI\\n\\nassistants-api\\n\\nYou have selected 0 posts.\\n\\nselect all\\n\\ncancel selecting\\n\\nJul 2024\\n\\n1 / 26\\n\\nJul 2024\\n\\nMar 18\\n\\ngroverkartik25\\n\\nJul 2024\\n\\nHi, [...] I want to see the relevance of the data being retrieved from the vector store.\\n\\nThanks\\n\\n[8\\n\\n7\\n\\n3\\n\\n3\\n\\nread \\n\\n5\\nmin\\n\\naaron.lutz\\n\\nJul 2024\\n\\nLooking for a similar solution, but have not found anything promising.\\n\\nI managed to get some more info on how file search actually works, but unfortunately this is not documented…\\n\\nHere’s a quick rundown.\\n\\nThe AI model you specified (gpt 3.5, 4, 4o etc) outputs a search query to the search tool. This looks like this:\", \"score\": 0.5199851}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"How To Implement Contextual RAG From Anthropic\", \"url\": \"https://docs.together.ai/docs/how-to-implement-contextual-rag-from-anthropic\", \"content\": \"```\\nChunk 1: September 2024At a YC event last week Brian Chesky gave a talk that everyone who was there will remember. Most founders I talked to afterward said it was the best they'd ever heard. Ron Conway, for the first time in his life, forgot to take notes. I' [...] - Fine-tuning\\n  - Fine-tuning Guide\\n  - Data preparation\\n  - Supported Models\\n  - Serverless LoRA Inference\\n  - Preference Fine-Tuning\\n  - Deploying a fine-tuned model\\n  - Pricing\\n- GPU Clusters\\n  - Cluster user management\\n  - Cluster storage\\n  - Slurm management system\\n\\n## Guides [...] ## frequently asked questions\\n\\n- Deployment Options\\n- Rate limits\\n- Error codes\\n- Deploy dedicated endpoints in the web\\n- Priority Support\\n  - Create tickets in Slack\\n  - Customer Ticket Portal\\n- Deprecations\\n- Playground\\n- Inference FAQs\\n- Fine-tuning FAQs\\n- Multiple API Keys\\n- Dedicated endpoints\\n\\n# How to Implement Contextual RAG from Anthropic\\n\\nAn open source line-by-line implementation and explanation of Contextual RAG from Anthropic!\\n\\nSuggest Edits\", \"score\": 0.6721816}, {\"title\": \"Implementing Anthropic's Contextual Retrieval for Powerful ...\", \"url\": \"https://medium.com/data-science/implementing-anthropics-contextual-retrieval-for-powerful-rag-performance-b85173a65b83\", \"content\": \"Published Time: 2024-10-18T17:31:38.604Z\\n\\nImplementing Anthropic’s Contextual Retrieval for Powerful RAG Performance | by Eivind Kjosbakken | TDS Archive | Medium\\n\\nSitemap\\n\\nOpen in app\\n\\nSign up\\n\\nSign in [...] How to Create a Powerful AI Email Search for Gmail with RAG ----------------------------------------------------------- ### Learn how you can develop an application to search emails using RAG\\n\\nSep 10, 2024\\n\\n3\\n\\n[](\\n\\nSee all from Eivind Kjosbakken\\n\\nSee all from TDS Archive\\n\\nRecommended from Medium\\n\\nImage 21: Finally, Reranking Helped My RAG — With a Tweak\\n\\nImage 22: AI Advances\\n\\nIn\\n\\nAI Advances\\n\\nby\\n\\nThuwarakesh Murallie [...] Finally, Reranking Helped My RAG — With a Tweak ----------------------------------------------- ### Rerank context with limited resources and create RAGs that perform effectively.\\n\\nMar 10\\n\\n[](\\n\\n[](\\n\\nImage 23: Building Google Veo 3 from Scratch Using Python\\n\\nImage 24: Level Up Coding\\n\\nIn\\n\\nLevel Up Coding\\n\\nby\\n\\nFareed Khan\\n\\nBuilding Google Veo 3 from Scratch Using Python ----------------------------------------------- ### Preprocessing Data, Encoders, Latent Diffusion, and more\\n\\n5d ago\\n\\n6\\n\\n[](\", \"score\": 0.64020914}, {\"title\": \"Anthropic Introduces Contextual Retrieval Using Prompt Caching ...\", \"url\": \"https://www.linkedin.com/pulse/anthropic-introduces-contextual-retrieval-using-prompt-aditi-khare-uniqc\", \"content\": \"In this blog release Anthropic Introduces a method called “Contextual Retrieval” and uses two sub-techniques -\\n\\nContextual Embeddings & Contextual BM25. This method can reduce the number of failed retrievals by 49% and, when combined with reranking, by 67%. These represent significant improvements in retrieval accuracy, which directly translates to better performance in downstream tasks.\\n\\n### A primer on RAG - Scaling to Larger Knowledge Bases - [...] Anthropic Introduces Contextual Retrieval Using Prompt Caching & Contextual Embeddings & Reranking Techniques\\n\\n   Report this article\\n\\nAditi KhareImage 3: Aditi Khare\\n\\n### Aditi Khare\\n\\n#### AWS & AI Research Specialist-Principal Machine Learning Scientist & AI Architect | IIM-A | Author | Agentic AI | Generative AI | Inference Optimization | Open-Source Dev | Build Production-Grade AI Products from Scratch\\n\\n Published Sep 23, 2024 \\n\\n+ Follow\\n\\n#ai #airesearch #anthropic #embeddings #llm #genai [...] Image 21 9     \\n   Architecture Search Framework for Inference-Time Techniques & Designing Priors for Better Few-Shot Image SynthesisImage 22 Oct 7, 2024 \\n### Architecture Search Framework for Inference-Time Techniques & Designing Priors for Better Few-Shot Image Synthesis\\n\\n#ai #genai #architecture #search #researchpapers #researchskills #computervision #pattern recognition Inference-time…\", \"score\": 0.59824234}, {\"title\": \"Implementing Anthropic's Contextual Retrieval with Async ...\", \"url\": \"https://python.useinstructor.com/blog/2024/09/26/implementing-anthropics-contextual-retrieval-with-async-processing/\", \"content\": \"# Implementing Anthropic's Contextual Retrieval with Async Processing¶\\n\\nAnthropic's Contextual Retrieval technique enhances RAG systems by preserving crucial context.\\n\\nThis post examines the method and demonstrates an efficient implementation using async processing. We'll explore how to optimize your RAG applications with this approach, building on concepts from our async processing guide.\\n\\n## Background: The Context Problem in RAG¶\", \"score\": 0.46380913}, {\"title\": \"Implementing Anthropic's Contextual Retrieval for Powerful RAG ...\", \"url\": \"https://towardsdatascience.com/implementing-anthropics-contextual-retrieval-for-powerful-rag-performance-b85173a65b83/\", \"content\": \"In this article, I have discussed how you can implement contextual retrieval for your RAG system and the idea proposed by Anthropic. First, I discussed the problem I am working with, which is making court rulings in Norway more accessible. I then used the text from different court rulings, divided it into chunks, and added context to each chunk using GPT-4o-mini. I then created embeddings for each chunk and stored them in Pinecone. Furthermore, I created a BM25 index, which is used together [...] Skip to content\\n\\nPublish AI, ML & data-science insights to a global community of data professionals.\\n\\nSubmit an Article\\n\\n LinkedIn\\n X\\n\\nMachine Learning\\n\\n# Implementing Anthropic’s Contextual Retrieval for Powerful RAG Performance\\n\\nThis article will show you how to implement the contextual retrieval idea proposed by Anthropic\\n\\nEivind Kjosbakken\\n\\n16 min read [...] Retrieval augmented generation (RAG) is a powerful technique that utilizes large language models (LLMs) and vector databases to create more accurate responses to user queries. RAG allows LLMs to utilize large knowledge bases when responding to user queries, improving the quality of the responses. However, RAG also has some downsides. One downside is that RAG utilizes vector similarity when retrieving context to respond to a user query. Vector similarity is not always consistent and can, for\", \"score\": 0.4635177}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"[PDF] arXiv:2408.04187v2 [cs.CV] 15 Oct 2024\", \"url\": \"https://arxiv.org/pdf/2408.04187?\", \"content\": \"The results show a gradual perfor-mance improvement as more of our modules are added, with significant gains observed when replac-ing GraphRAG graph construction with our Triple Graph Construction. Additionally, by replacing the summary-based retrieval(Edge et al., 2024b) in GraphRAG with our U-Retrieval method, we achieved further improvements, setting new state-of-the-art results across all three benchmarks. [...] individually improves performance by less than 2%, and Med-Dictionary data by less than 1%. Accumulating three tier data also leads to mediocre improvements. GraphRAG shows improvement in retrieving individual data but has minimal gains when incrementally adding more data, likely due to superficiality from linking trivial entities, as dis-cussed in the previous case study. In contrast, Med-GraphRAG efficiently handles the additional data, using its hierarchical structure to clarify [...] relation-ships and show strong improvements as more data is added. With MedGraphRAG, we see significant improvements of over 6% and 8% for Med-Paper and Med-Dictionary data, respectively, highlight-ing the importance of the retrieval method in maxi-mizing the impact of the data.\", \"score\": 0.8638634}, {\"title\": \"[PDF] Graph Retrieval-Augmented Generation: A Survey - arXiv\", \"url\": \"https://arxiv.org/pdf/2408.08921\", \"content\": \"> J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. Graph Retrieval-Augmented Generation: A Survey 111:7\\n\\n4 Overview of GraphRAG \\n\\nGraphRAG is a framework that leverages external structured knowledge graphs to improve contex-tual understanding of LMs and generate more informed responses, as depicted in Figure 2. The goal of GraphRAG is to retrieve the most relevant knowledge from databases, thereby enhancing the answers of downstream tasks. The process can be defined as [...] • We delineate the downstream tasks, benchmarks, application domains, evaluation metrics, current challenges, and future research directions pertinent to GraphRAG, discussing both \\n\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. 111:4 Peng et al. G-Retrieval \\n\\nGraph Database & G-Indexing \\n\\nG-Generation Graph Format Retrieval \\n\\n> Results\\n> Output\\n> Response\\n\\nInput Query [...] 3 Preliminaries \\n\\nIn this section, we introduce background knowledge of GraphRAG for easier comprehension of our survey. First, we introduce Text-Attributed Graphs which is a universal and general format of  \\n\\n> J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. 111:6 Peng et al.\\n\\ngraph data used in GraphRAG. Then, we provide formal definitions for two types of models that can be used in the retrieval and generation stages: Graph Neural Networks and Language Models.\", \"score\": 0.8003337}, {\"title\": \"A Graph RAG Approach to Query-Focused Summarization - arXiv\", \"url\": \"https://arxiv.org/abs/2404.16130\", \"content\": \"Subjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)\\nACM classes:H.3.3; I.2.7\\nCite as:arXiv:2404.16130 [cs.CL]\\n(or arXiv:2404.16130v2 [cs.CL] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\n\\nSubmission history\\n\\n From: Darren Edge [view email] \\n\\n( Wed, 24 Apr 2024 18:38:11 UTC (6,306 KB)\\n\\n[v2] Wed, 19 Feb 2025 10:49:41 UTC (6,322 KB)\\n\\n\\n   TeX Source\\n   Other Formats\\n\\nImage 5: license iconview license [...] Published Time: Sat, 26 Jul 2025 13:55:54 GMT\\n\\n[2404.16130] From Local to Global: A Graph RAG Approach to Query-Focused Summarization\\n\\nSkip to main content\\n\\nImage 1: Cornell University Logo\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n Submitted on 24 Apr 2024 ([v1), last revised 19 Feb 2025 (this version, v2)]\\n\\nTitle:From Local to Global: A Graph RAG Approach to Query-Focused Summarization [...] Authors:Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson\\n\\nView a PDF of the paper titled From Local to Global: A Graph RAG Approach to Query-Focused Summarization, by Darren Edge and 9 other authors\", \"score\": 0.7621591}, {\"title\": \"Project GraphRAG - Microsoft Research: Publications\", \"url\": \"https://www.microsoft.com/en-us/research/project/graphrag/publications/\", \"content\": \"Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson\\n\\n    April 2024\\n\\n    Preprint\\n + #### From Local to Global: A Graph RAG Approach to Query-Focused Summarization\\n\\n    Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson\\n\\n    April 2024\\n\\n    Preprint\\n\\n| [...] + #### From Local to Global: A Graph RAG Approach to Query-Focused Summarization\\n\\n    Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson\\n\\n    April 2024\\n\\n    Preprint\\n\\nOpens in a new tab\\n\\nFollow us:\\n\\n Follow on X\\n Like on Facebook\\n Follow on LinkedIn\\n Subscribe on Youtube\\n Follow on Instagram\\n Subscribe to our RSS feed\\n\\nShare this page: [...] # Project GraphRAG\\n\\nLLM-Derived Knowledge Graphs\\n\\n## Publications\\n\\n|\\n\\n + #### From Local to Global: A Graph RAG Approach to Query-Focused Summarization\\n\\n    Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson\\n\\n    April 2024\\n\\n    Preprint\\n\\n|\\n\\n + #### From Local to Global: A Graph RAG Approach to Query-Focused Summarization\", \"score\": 0.7185356}, {\"title\": \"A GraphRAG Approach to Query-Focused Summarization - arXiv\", \"url\": \"https://arxiv.org/html/2404.16130v2\", \"content\": \"GraphRAG leverages summaries over large sections of the data source as a form of ”self-memory” (described in Cheng et al., 2024), which are later used to answer queries as in Mao et al., 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Shao et al.,, 2023; Wang et al.,, 2024; Su et al.,, 2020; Feng et al.,, 2023; Trivedi et al.,, 2022; Khattab et al.,, 2022; Gao et al.,, 2023). [...] The GraphRAG method and its ability to perform global sensemaking over an entire corpus form the main contribution of this work. To demonstrate this ability, we developed a novel application of the LLM-as-a-judge technique  (Zheng et al.,, 2024) suitable for questions targeting broad issues and themes where there is no ground-truth answer. [...] Podcast transcripts. Public transcripts of Behind the Tech with Kevin Scott, a podcast featuring conversations between Microsoft CTO Kevin Scott and various thought leaders in science and technology  (Scott,, 2024).\\nThis corpus was divided into 1669 600-token text chunks, with 100-token overlaps between chunks (1 million tokens).\", \"score\": 0.71687365}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"GitHub - profintegra/raptor-rag: The official implementation of RAPTOR\", \"url\": \"https://github.com/profintegra/raptor-rag\", \"content\": \"```\\n@inproceedings{sarthi2024raptor,\\n    title={RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval},\\n    author={Sarthi, Parth and Abdullah, Salman and Tuli, Aditi and Khanna, Shubh and Goldie, Anna and Manning, Christopher D.},\\n    booktitle={International Conference on Learning Representations (ICLR)},\\n    year={2024}\\n}\\n```\\n\\nStay tuned for more examples, configuration guides, and updates.\\n\\nYou can’t perform that action at this time. [...] RAPTOR is an open-source project, and contributions are welcome. Whether you're fixing bugs, adding new features, or improving documentation, your help is appreciated.\\n\\nRAPTOR is released under the MIT License. See the LICENSE file in the repository for full details.\\n\\nIf RAPTOR assists in your research, please cite it as follows: [...] ## Repository files navigation\\n\\n## RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\\n\\nRAPTOR introduces a novel approach to retrieval-augmented language models by constructing a recursive tree structure from documents. This allows for more efficient and context-aware information retrieval across large texts, addressing common limitations in traditional language models.\\n\\nFor detailed methodologies and implementations, refer to the original paper:\", \"score\": 0.7092009}, {\"title\": \"The Rise and Evolution of RAG in 2024 A Year in Review - RAGFlow\", \"url\": \"https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review\", \"content\": \"On April 1, 2024, we open-sourced the complete RAG engine, RAGFlow, which has since garnered over 26,000 stars on GitHub by the end of the year. The initial two design highlights of RAGFlow have become universal design principles for RAG architecture: [...] text, they can yield appropriate answers for vague inquiries and multi-hop questions that require crossing chunks. RAPTOR was integrated into RAGFlow mid-year to assist in answering complex questions. [...] By the end of 2024, SiReRAG [Reference 17] emerged based on RAPTOR, offering a finer-grained definition of text recall: it measures different dimensions of needs using similarity and relevance. Similarity calculates the semantic distance between text chunks using vectors or full-text search methods, which is what RAPTOR itself does (shown on the left side of the diagram). Relevance indicates a relationship between text chunks; it first extracts named entities from each chunk using an LLM and\", \"score\": 0.6607204}, {\"title\": \"RAG Implementation using RAPTOR with huggingface models #11966\", \"url\": \"https://github.com/run-llama/llama_index/discussions/11966\", \"content\": \"# RAG Implementation using RAPTOR with huggingface models #11966\\n\\n@Venpion\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n# {{title}}\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n## Venpion Mar 15, 2024\\n\\n-\\n\\n|  | [...] There was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n# {{editor}}'s edit\\n\\n# {{editor}}'s edit\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n### [dosubot[bot]](/apps/dosubot) bot Mar 15, 2024\\n\\n-\\n\\n|  |\", \"score\": 0.5996498}, {\"title\": \"Mastering RAG with RAPTOR: A comprehensive guide ...\", \"url\": \"https://www.educative.io/blog/mastering-rag-with-raptor\", \"content\": \"It's Black Friday in August! Get 50% off this week for our Vibe Friday sale.\\nLock in 50% off today!\\n\\n1d\\n17h\\n1m\\n\\n45s\\n\\nHome\\n/\\nBlog\\n/\\nGenerative Ai\\n/\\nMastering RAG with RAPTOR: A comprehensive guide using LlamaIndex\\n\\nHome\\n/\\nBlog\\n/\\nGenerative Ai\\n/\\nMastering RAG with RAPTOR: A comprehensive guide using LlamaIndex\\n\\n# Mastering RAG with RAPTOR: A comprehensive guide using LlamaIndex\\n\\n17 min readSep 06, 2024\\n\\nWhat is RAG?\\n\\nHow RAG works\\n\\nBenefits of RAG\\n\\nChallenges with traditional RAG\\n\\nWhat is RAPTOR? [...] Check out the official RAPTOR GitHub repository for more information and resources: RAPTOR GitHub Repository.\\n\\nHere’s a comparison table highlighting why RAPTOR is considered superior to traditional RAG methods:\\n\\n## RAG with RAPTOR vs. Traditional RAG [...] Response: RAPTOR was compared against BM25 and DPR as baselines. This comparison was conducted to showcase RAPTOR's superior performance in information retrieval tasks, particularly on datasets like QASPER. The reason for comparing against these baselines was to highlight RAPTOR's ability to outperform methods that can only extract top similar raw text chunks, as RAPTOR's hierarchical summarization approach allows it to capture a broader range of information, from general themes to specific\", \"score\": 0.5942929}, {\"title\": \"DEEP-PolyU/Awesome-GraphRAG - GitHub\", \"url\": \"https://github.com/DEEP-PolyU/Awesome-GraphRAG\", \"content\": \"(arXiv 2024) OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models (\\n (arXiv 2024) GRAG: Graph Retrieval-Augmented Generation (\\n (arXiv 2024) Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning (\\n (ICLR 2024) RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval (\\n (AAAI 2024) Knowledge graph prompting for multi-document question answering ( [...] (arXiv 2024) GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model (\\n (arXiv 2024) Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation (\\n (arXiv 2024) How to Make LLMs Strong Node Classifiers? ( [...] (arXiv 2024) LightRAG: Simple and Fast Retrieval-Augmented Generation (\\n (arXiv 2024) MEG: Medical Knowledge-Augmented Large Language Models for Question Answering (\\n (arXiv 2024) From local to global: A graph rag approach to query-focused summarization (\", \"score\": 0.5758033}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Using DSPy For A RAG Implementation\", \"url\": \"https://cobusgreyling.substack.com/p/using-dspy-for-a-rag-implementation\", \"content\": \"### RAG enables LLMs to adaptively access real-time knowledge, providing insightful responses beyond their original training. Yet, implementing refined RAG pipelines introduces complexities.\\n\\nCobus Greyling\\n\\nMay 29, 2024\\n\\n#### Share this post\\n\\nCobus Greyling on LLMs, NLU, NLP, chatbots & voicebots\\n\\nUsing DSPy For A RAG Implementation\\n\\nShare)\\n\\n#### DSPy simplifies this process, offering a seamless setup for prompting pipelines.\\n\\nSubscriber for free to all my content.\\n\\n# Introduction [...] DSPy has a seamless approach to setting up a RAG prompting pipeline.\\n\\n#### An Introduction To DSPy\\n\\n[Cobus Greyling\\n\\n·\\n\\nMay 28, 2024\\n\\nRead full story](\\n\\nBelow the DSPy RAG application sequence is shown…\\n\\n# DSPy RAG Program\\n\\nIn this notebook, GPT-3.5 (specifically gpt-3.5-turbo) and the ColBERTv2retriever are made use of. [...] Retrieval-augmented generation (RAG) is a methodology enabling Language Models (LLMs) to access extensive knowledge repositories, search them for pertinent passages, and leverage this information to generate refined responses.\\n\\nRAG enable LLMs to dynamically utilise real-time knowledge even if not originally trained on the data by leveraging In-Context Learning (ICL).\\n\\nHowever, with this approach comes greater complexities in setting up and refining RAG pipelines.\", \"score\": 0.83093774}, {\"title\": \"Tutorial: Retrieval-Augmented Generation (RAG) - DSPy\", \"url\": \"https://dspy.ai/tutorials/rag/\", \"content\": \"```\\n\\nEarlier with a CoT module, we got around 40% in terms of semantic F1 on our `devset`. Would this `RAG` module score better?\\n\\nIn :\\n\\nCopied!\\n\\n```\\nevaluate(RAG())\\n\\n```\\n\\nevaluate(RAG())\\n\\n```\\nAverage Metric: 166.54 / 300 (55.5%): 100%|██████████| 300/300 [00:04<00:00, 61.40it/s] \\n```\\n\\n```\\n2024/11/23 23:16:54 INFO dspy.evaluate.evaluate: Average Metric: 166.53601368289284 / 300 (55.5%)\\n\\n``` [...] ```\\ndspy.inspect_history(n=1)\\n\\n```\\n\\ndspy.inspect\\\\_history(n=1)\\n\\n```\\n\\n[2024-11-23T23:16:35.966534]\\n\\nSystem message:\\n\\nYour input fields are:\\n1. `question` (str)\\n\\nYour output fields are:\\n1. `response` (str)\\n\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n{question}\\n\\n{response}\\n\\nIn adhering to this structure, your objective is: \\n        Given the fields `question`, produce the fields `response`.\\n\\nUser message: [...] In :\\n\\nCopied!\\n\\n```\\ndspy.inspect_history()\\n\\n```\\n\\ndspy.inspect\\\\_history()\\n\\n```\\n\\n[2024-11-23T23:16:49.175612]\\n\\nSystem message:\\n\\nYour input fields are:\\n1. `context` (str)\\n2. `question` (str)\\n\\nYour output fields are:\\n1. `reasoning` (str)\\n2. `response` (str)\\n\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n{context}\\n\\n{question}\\n\\n{reasoning}\\n\\n{response}\", \"score\": 0.7950386}, {\"title\": \"LLMOps with DSPy: Build RAG Systems Using Declarative ...\", \"url\": \"https://pyimagesearch.com/2024/09/09/llmops-with-dspy-build-rag-systems-using-declarative-programming/\", \"content\": \"```\\n@incollection{Mangla_2024_LLMOps-DSPy-Build-RAG,\\n  author = {Puneet Mangla},\\n  title = {LLMOps with DSPy: Build RAG Systems Using Declarative Programming},\\n  booktitle = {PyImageSearch},\\n  editor = {Puneet Chugh and Aritra Roy Gosthipaty and Susan Huot and Kseniia Kidriavsteva and Ritwik Raha},\\n  year = {2024},\\n  url = {\\n}\\n```\\n\\n#### Join the PyImageSearch Newsletter and Grab My FREE 17-page Resource Guide PDF [...] ### Citation Information\\n\\nMangla, P. “LLMOps with DSPy: Build RAG Systems Using Declarative Programming,” PyImageSearch, P. Chugh, A. R. Gosthipaty, S. Huot, K. Kidriavsteva, and R. Raha, eds., 2024, [...] Retrieval Augmented Generation (RAG) is an advanced approach in natural language processing (NLP) that combines the strengths of retrieval-based and generation-based models to enhance the performance and accuracy of AI systems, particularly in tasks like question answering and conversational AI.\", \"score\": 0.79215926}, {\"title\": \"Using DSPy For A RAG Implementation | by Cobus Greyling | Medium\", \"url\": \"https://cobusgreyling.medium.com/using-dspy-for-a-rag-implementation-aa140caef50e\", \"content\": \"Published Time: 2024-05-29T12:39:35.331Z\\n\\nUsing DSPy For A RAG Implementation | by Cobus Greyling | Medium\\n\\nSitemap\\n\\nOpen in app\\n\\nSign up\\n\\nSign in\\n\\n is a methodology enabling Language Models (LLMs) to access extensive knowledge repositories, search them for pertinent passages, and leverage this information to generate refined responses.\\n\\nRAG enable LLMs to dynamically utilise real-time knowledge even if not originally trained on the data by leveraging In-Context Learning (ICL).\", \"score\": 0.752468}, {\"title\": \"Building RAG as Agent - DSPy\", \"url\": \"https://dspy.ai/tutorials/agents/\", \"content\": \"```\\noptimized_react(claim=\\\"The author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Koltès are both playwrights.\\\").titles\\n\\n```\\n\\noptimized\\\\_react(claim=\\\"The author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Koltès are both playwrights.\\\").titles\\n\\nOut:\\n\\n```\\n['Bernard-Marie Koltès', 'Joe Orton']\\n```\\n\\nIn :\\n\\nCopied!\\n\\n```\\ndspy.inspect_history(n=2)\\n\\n```\\n\\n```\\n\\n[2024-12-17T15:13:25.420335]\\n\\nSystem message: [...] Let's now evaluate again, after optimization.\\n\\nIn :\\n\\nCopied!\\n\\n```\\nevaluate(optimized_react)\\n\\n```\\n\\nevaluate(optimized\\\\_react)\\n\\n```\\nAverage Metric: 41.67 / 100 (41.7%): 100%|██████████| 100/100 [03:00<00:00,  1.81s/it]\\n```\\n\\n```\\n2024/12/17 15:12:06 INFO dspy.evaluate.evaluate: Average Metric: 41.66666666666667 / 100 (41.7%)\\n\\n``` [...] def safe\\\\_react(claim: str):\\ntry:\\nreturn react(claim=claim)\\nexcept Exception as e:\\nreturn dspy.Prediction(titles=[])\\nevaluate(safe\\\\_react)\\n\\n```\\n  0%|          | 0/100 [00:00<?, ?it/s]\\n```\\n\\n```\\nAverage Metric: 8.00 / 100 (8.0%): 100%|██████████| 100/100 [05:22<00:00,  3.22s/it]\\n```\\n\\n```\\n2024/12/17 14:09:47 INFO dspy.evaluate.evaluate: Average Metric: 7.999999999999997 / 100 (8.0%)\\n\\n```\", \"score\": 0.74363416}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"LangChain State of AI 2024 Report\", \"url\": \"https://blog.langchain.com/langchain-state-of-ai-2024/\", \"content\": \"The average number of steps per trace has more than doubled over the past year, rising from onaverage 2.8 steps (2023) to 7.7 steps (2024). We define a step as a distinct operation within a trace, such as a call to an LLM, retriever, or tool. This growth in steps signals that organizations are leveraging more complex and multi-faceted workflows. Rather than a simple question-answer interaction, users are building systems that chain together multiple tasks, such as retrieving information, [...] Published Time: 2024-12-19T17:30:11.000Z\\n\\nLangChain State of AI 2024 Report\\n\\nSkip to content\\n\\nImage 1: LangChain Blog\\n\\n   Case Studies\\n   In the Loop\\n   LangChain\\n   Docs\\n   Changelog\\n\\nSign inSubscribe\\n\\nImage 2: LangChain State of AI 2024 Report\\n\\nLangChain State of AI 2024 Report\\n\\nDive into LangSmith product usage patterns that show how the AI ecosystem and the way people are building LLM apps is evolving.\\n\\nBy LangChain6 min read Dec 19, 2024 [...] As companies are getting more serious about incorporating AI agents across various industries, adoption of our controllable agent framework, LangGraph, is also on the rise. Since its release in March 2024, LangGraph has steadily gained traction —with 43% of LangSmith organizations are now sending LangGraph traces. These traces represent complex, orchestrated tasks that go beyond basic LLM interactions.\", \"score\": 0.65966886}, {\"title\": \"Let Your Data Talk: An Illustrated Guide to RAG with LangChain\", \"url\": \"https://www.mercedes-benz.io/blog/2024-09-10-let-your-data-talk\", \"content\": \"Who we areWhat we doHow we workOur Blog\\n\\nJoin Us\\n\\n/All Articles/Let Your Data Talk: An Illustrated Guide to RAG with LangChain – Benefits and Relevance in 2024\\n\\n All Articles \\n\\n# Let Your Data Talk: An Illustrated Guide to RAG with LangChain – Benefits and Relevance in 2024\\n\\nVladimir Zaikin · September 9, 2024\\n\\n## TL;DR [...] Retrieval augmented generation (RAG) is an architectural pattern for GenAI application, a technique that aims to solve\\nthe above-mentioned problem of LLMs by augmenting their knowledge with custom data. It combines data retrieval and text\\ngeneration to improve accuracy by bringing the context to the LLM.\", \"score\": 0.61279696}, {\"title\": \"Building a Multi-Modal RAG Pipeline with Langchain\", \"url\": \"https://www.analyticsvidhya.com/blog/2023/12/multi-modal-rag-pipeline-with-langchain/\", \"content\": \"The Langchain is one of the hottest tools of 2023. It is an open-source framework for building chains of tasks and LLM agents. It has almost all the tools you need to create a functional AI application. From data loaders and vector stores to LLMs from different labs, it got it all covered. Langchain has two core value propositions. Chains and Agents.\", \"score\": 0.5611433}, {\"title\": \"Hands on LangChain: RAG - Medium\", \"url\": \"https://medium.com/@ai-data-drive/hands-on-langchain-rag-dd639d0576f6\", \"content\": \"In LangChain, `OpenAIEmbeddings` uses the `text-embedding-ada-002` embedding model by default. According to OpenAI's new embedding model announcement on 20240125, it is recommended to use the better-performing, lower-priced upgraded model `text-embedding-3-small`; there is also a higher-performance but more expensive `text-embedding-3-large` version available. Let's use `text-embedding-3-small` to implement chunk embedding. [...] 'included''00ad8dcd-31d0-4dcd-8ded-23f51f3a135f''0fce9646-bc90-43b3-a2c9-ed1280047d9f''1145787f-643b-403f-9a73-7643bbcede23''122c8ab3-0f79-47d6-9eaf-a4a74e47c084''16fd6bdd-fa37-4489-9e7c-7fcaa58e52b3''1a682023-b01e-4358-a3da-c7c9fc843e9c''20421c65-40d3-4677-9c50-2d83a32fca46''27929236-d98c-4cf7-876c-460c5b1a7620''3136e6f3-b1f2-4081-a456-961bf20c67af''3a0d5ae0-1d49-404e-908c-7cc6124008c8''3f67d47c-4c44-41b2-9d1d-34544b019795''441c0ad8-0d43-4fd0-9d4c-0621be202eb2''4a2dea60-0fec-4fc8-9a5f-6cc7e0c8 [...] Retrieval & generation: It includes:\\n\\n Retrieve: Retrieving relevant text. Converting the user’s input question into an embedding, and then retrieving similar data chunks from the vector storage as input context for LLMs.\\n Generate: LLM generates an answer. Combining the context obtained above with the question into a Prompt and inputting it into the LLM to generate the final answer.\\n\\nThe above is the entire process of RAG. Below we will implement a basic RAG through an actual example.\", \"score\": 0.498609}, {\"title\": \"Deconstructing RAG - LangChain Blog\", \"url\": \"https://blog.langchain.com/deconstructing-rag/\", \"content\": \"Many of these tasks to improve RAG are narrow and well-defined. For example, query expansion (sub-question generation) or structured query construction for metadata filtering are narrow, well-defined tasks that also may be done repeatedly. In turn, they may not require large (and most costly) generalist models to achieve acceptable performance. Instead, smaller open source models (potentially with fine-tuning) may be sufficient. We will be releasing a series of templates that showcases how to [...] This retrieved information is loaded into the context window and used in LLM output generation, a process typically called retrieval augmented generation (RAG). RAG is one of the most important concepts in LLM app development because it is an easy way to pass external information to an LLM with advantages over more complex / complex fine-tuning on problems that require factual recall. [...] The landscape of RAG methods has expanded greatly in recent months, resulting in some degree of overload or confusion among users about where to start and how to think about the various approaches. Over the past few months, we have worked to group RAG concepts into a few categories and have released guides for each. Below we'll provide a round-up of these concepts and present some future work.\\n\\n## Query Transformations\", \"score\": 0.49582586}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\", \"url\": \"https://arxiv.org/abs/2501.09136\", \"content\": \"Published Time: Wed, 05 Feb 2025 01:25:50 GMT\\n\\n[2501.09136] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\\n\\nImage 1: close this message\\n\\nImage 2: arXiv smileybones\\n\\narXiv Is Hiring a DevOps Engineer\\n\\nWork on one of the world's most important websites and make an impact on open science.\\n\\nView Jobs\\n\\nSkip to main content\\n\\nImage 3: Cornell University Logo\\n\\narXiv Is Hiring a DevOps Engineer\\n\\nView Jobs [...] Subjects:Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Retrieval (cs.IR)\\nCite as:arXiv:2501.09136 [cs.AI]\\n(or arXiv:2501.09136v3 [cs.AI] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\n\\nSubmission history\\n\\n From: Abul Ehtesham [view email] \\n\\n( Wed, 15 Jan 2025 20:40:25 UTC (20,962 KB)\\n\\n( Mon, 3 Feb 2025 04:01:36 UTC (22,453 KB)\\n\\n[v3] Tue, 4 Feb 2025 04:48:00 UTC (22,430 KB)\\n\\nFull-text links:\\nAccess Paper: [...] We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n Submitted on 15 Jan 2025 ([v1), last revised 4 Feb 2025 (this version, v3)]\\n\\nTitle:Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG\\n\\nAuthors:Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei\\n\\nView a PDF of the paper titled Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG, by Aditi Singh and 3 other authors\", \"score\": 0.8194462}, {\"title\": \"A Survey of RAG-Reasoning Systems in LLMs\", \"url\": \"https://arxiv.org/abs/2507.09477\", \"content\": \"Comments:submitted to ARR May\\nSubjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI)\\nCite as:arXiv:2507.09477 [cs.CL]\\n(or arXiv:2507.09477v2 [cs.CL] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\n\\nSubmission history\\n\\n From: Yangning Li [view email] \\n\\n( Sun, 13 Jul 2025 03:29:41 UTC (586 KB)\\n\\n[v2] Wed, 16 Jul 2025 15:44:18 UTC (587 KB)\\n\\n | next>\\\")\\n\\nnew | recent | 2025-07\\n\\n Change to browse by: \\n\\ncs\\n\\ncs.AI\\n\\n### References & Citations [...] [2507.09477] Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs\\n\\nSkip to main content\\n\\nImage 1: Cornell University Logo\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n Submitted on 13 Jul 2025 ([v1), last revised 16 Jul 2025 (this version, v2)]\\n\\nTitle:Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs\", \"score\": 0.75442725}, {\"title\": \"[2506.00054] Retrieval-Augmented Generation\", \"url\": \"https://arxiv.org/abs/2506.00054\", \"content\": \"[2506.00054] Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers\\n\\nSkip to main content\\n\\nImage 1: Cornell University Logo\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n [Submitted on 28 May 2025]\\n\\nTitle:Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers\\n\\nAuthors:Chaitanya Sharma [...] > Abstract:Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance large language models (LLMs) by conditioning generation on external evidence retrieved at inference time. While RAG addresses critical limitations of parametric knowledge storage-such as factual inconsistency and domain inflexibility-it introduces new challenges in retrieval quality, grounding fidelity, pipeline efficiency, and robustness against noisy or adversarial inputs. This survey provides a [...] retrieval mechanisms. This survey aims to consolidate current knowledge in RAG research and serve as a foundation for the next generation of retrieval-augmented language modeling systems.\", \"score\": 0.6757865}, {\"title\": \"[2503.10677] A Survey on Knowledge-Oriented Retrieval- ...\", \"url\": \"https://arxiv.org/abs/2503.10677\", \"content\": \"> Abstract:Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its [...] [2503.10677] A Survey on Knowledge-Oriented Retrieval-Augmented Generation\\n\\nSkip to main content\\n\\nImage 1: Cornell University Logo\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n Submitted on 11 Mar 2025 ([v1), last revised 17 Mar 2025 (this version, v2)]\\n\\nTitle:A Survey on Knowledge-Oriented Retrieval-Augmented Generation [...] Subjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI)\\nCite as:arXiv:2503.10677 [cs.CL]\\n(or arXiv:2503.10677v2 [cs.CL] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\n\\nSubmission history\\n\\n From: Mingyue Cheng [view email] \\n\\n( Tue, 11 Mar 2025 01:59:35 UTC (1,510 KB)\\n\\n[v2] Mon, 17 Mar 2025 11:24:11 UTC (1,510 KB)\\n\\nFull-text links:\\nAccess Paper:\", \"score\": 0.6271529}, {\"title\": \"A Survey on Reasoning Agentic Retrieval-Augmented ...\", \"url\": \"https://arxiv.org/html/2506.10408v1\", \"content\": \"MCTS-RAG Hu et al. (2025) integrates a Monte Carlo Tree Search loop into the RAG process for complex reasoning tasks. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured [...] The year 2025 is marked as the year of agentic AI, with applications emerging such as agentic LLMs and so on Ruan et al. (2023); Kong et al. (2024); Zhang et al.. Recent advances in RAG have seen a shift from static, rule-driven retrieval pipelines toward dynamic, reasoning-driven architectures, collectively referred to as Reasoning Agentic RAG. These systems embed decision-making into the retrieval process, enabling models to actively determine when, what, and how to retrieve based on their [...] what to search within a reasoning trajectory. ReZero Dao and Le (2025)incentivizes persistence, rewarding effective retry strategies. DeepResearcher Zheng et al. (2025) pushes further by training agents in open web environments, enabling robust search and synthesis across diverse, unstructured sources.\", \"score\": 0.61695945}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Ad Auctions for LLMs via Retrieval Augmented Generation\", \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2024/hash/20dcab0f14046a5c6b02b61da9f13229-Abstract-Conference.html\", \"content\": \"#### Ad Auctions for LLMs via Retrieval Augmented Generation\\n\\nPart of\\nAdvances in Neural Information Processing Systems 37 (NeurIPS 2024)\\nMain Conference Track\\n\\nBibtex Paper Supplemental\\n\\n  \\n\\n#### Authors\\n\\nMohammadTaghi Hajiaghayi, Sébastien Lahaie, Keivan Rezaei, Suho Shin\\n\\n#### Abstract [...] In the field of computational advertising, the integration of ads into the outputs of large language models (LLMs) presents an opportunity to support these services without compromising content integrity. This paper introduces novel auction mechanisms for ad allocation and pricing within the textual outputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a \\\\emph{segment auction} where an ad is probabilistically retrieved for each discourse segment (paragraph, section, or\", \"score\": 0.7610953}, {\"title\": \"Evaluating LLMs - Notes on a NeurIPS'24 Tutorial\", \"url\": \"https://blog.quipu-strands.com/eval-llms\", \"content\": \"1. RewardBench: This was discussed earlier in the section on AutoRaters.\\n2. LLM-AggreFact (Tang et al., 2024): For testing groundedness of LLM responses against reference documents. Useful for Retrieval Augmented Generation (RAG) systems, QA systems, summarizers, etc. Aside from the dataset contribution, the paper also shows how to build small fact-checking models that are much cheaper than GPT-4 for similar accuracies. [...] To put it bluntly: earlier NLG systems weren’t good enough for widespread use, and therefore didn’t warrant this level of attention.\\n\\nLet’s get started. I will summarize the session here, while interleaving my opinions which I’ll call out. Direct quotes from the presenters or the slides are in this color.\\n\\n## Tutorial Intro - Irina Sigler [...] | 1. Use Case: Ensure coverage of prompts that various user actions can produce.  [AG] This is like the \\\"data scope\\\", where data now is the set of prompts user actions might trigger. | A. Manual.   Think up some prompts, involve your team, your domain expert - more the better! | B. Synthetic.   Use the LLM to augment the scenario data you already have. But it is advised to perform the previous step first. | C. Traffic.   Use actual production data to validate your system. In other words, keep\", \"score\": 0.7373288}, {\"title\": \"NeurIPS 2024 Tutorials\", \"url\": \"https://neurips.cc/virtual/2024/events/tutorial\", \"content\": \"This tutorial delves into the critical and complex domain of evaluating large language models (LLMs), focusing on the unique challenges presented when assessing generative outputs. Despite the difficulty in assigning precise quality scores to such outputs, our tutorial emphasizes the necessity of rigorous evaluation throughout the development process of LLMs. This tutorial will provide an extensive presentation of evaluation scopes, from task-specific metrics to broader performance indicators [...] In this tutorial, we will present recent advances in program synthesis that enable the generation of programmatic policies for reinforcement learning and production software programs that satisfy user intent. The tutorial consists of two parts. In the first part of this tutorial, we consider the reinforcement learning (RL) setting, where the goal is to learn a policy that observes environments and acts optimally. Instead of representing policies using deep neural networks, programmatic RL [...] part of the tutorial, we consider code generation problems, where users provide their intent as input to a program synthesizer, which generates a program attempting to satisfy that intent. With the advancement of deep learning, neural networks and large language models (LLMs), with their impressive capabilities of understanding and reasoning over natural language and code, have …\", \"score\": 0.7120925}, {\"title\": \"RAGChecker: A Fine-grained Framework for Diagnosing ...\", \"url\": \"https://papers.nips.cc/paper_files/paper/2024/hash/27245589131d17368cccdfa990cbf16e-Abstract-Datasets_and_Benchmarks_Track.html\", \"content\": \"#### RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation\\n\\nPart of\\nAdvances in Neural Information Processing Systems 37 (NeurIPS 2024)\\nDatasets and Benchmarks Track\\n\\nBibtex Paper Supplemental\\n\\n  \\n\\n#### Authors\\n\\nDongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He, Zhiguo Wang, Pengfei Liu, Yue Zhang, Zheng Zhang\\n\\n#### Abstract [...] Despite Retrieval-Augmented Generation (RAG) has shown promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGChecker, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGChecker has\", \"score\": 0.6991933}, {\"title\": \"NeurIPS Poster RankRAG: Unifying Context Ranking with Retrieval ...\", \"url\": \"https://nips.cc/virtual/2024/poster/95135\", \"content\": \"Skip to yearly menu bar\\nSkip to main content\\n\\n  \\n  \\n\\n### Poster\\n\\n## RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs\\n\\n### Yue Yu · Wei Ping · Zihan Liu · Boxin Wang · Jiaxuan You · Chao Zhang · Mohammad Shoeybi · Bryan Catanzaro\\n\\n [Abstract ]\\n\\n[Paper] \\n\\n[OpenReview]\\n\\n2024 Poster\\n\\n  \\n\\nAbstract: [...] Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel method called RankRAG, which instruction-tunes a single LLM for both context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned\", \"score\": 0.6914979}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"ICLR 2024 Orals\", \"url\": \"https://iclr.cc/virtual/2024/events/oral\", \"content\": \"diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its generations using special tokens, called {\\\\it reflection} tokens. Generating reflection tokens makes [...] Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, [...] Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\\n\\nOral\\n\\nAkari Asai · Zeqiu Wu · Yizhong Wang · Avi Sil · Hannaneh Hajishirzi\\n\\n[ Halle A 8 - 9 ]\\n\\nAbstract\", \"score\": 0.77589405}, {\"title\": \"InstructRAG: Instructing Retrieval-Augmented Generation ...\", \"url\": \"https://openreview.net/forum?id=P1qhkp8gQT\", \"content\": \"method on the most recently released data collection (2024-11-04): [...] > \\n> Document  … \\n> \\n> Document  … \\n> \\n> Document  … [...] > \\n> Document  … \\n> \\n> Document  … \\n> \\n> Document  …\", \"score\": 0.70944256}, {\"title\": \"Retrieval-augmented Language Models - Akari Asai\", \"url\": \"https://akariasai.github.io/assets/pdf/akari_ralm_lecture_final.pdf\", \"content\": \"References (5) 133 Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela. Generative Representational Instruction Tuning. Arxiv 2024. Qingqing Cao, Sewon Min, Yizhong Wang, Hannaneh Hajishirzi. BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models. ICLR 2024.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. ICLR 2024. [...] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, Scott Yih. RA-DIT: Retrieval-Augmented Dual Instruction Tuning. ICLR 2024. [...] Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Rich James, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, Mike Lewis. In-Context Pretraining: Language Modeling Beyond Document Boundaries. ICLR 2024.\", \"score\": 0.6917478}, {\"title\": \"Retrieval-Augmented Generation for Natural Language Processing\", \"url\": \"https://arxiv.org/html/2407.13193v3\", \"content\": \"Wu et al. (Wu et al., 2024) propose ReFusion, which explores various learnable reranking schemes to first re-weight the retrieved knowledge’s embeddings, then use weighted addition to incorporate them into the hidden states of the model.\\nThose approaches signify a growing trend towards models that dynamically select and integrate relevant knowledge, paving the way for more sophisticated and nuanced language generation and understanding. [...] After building the retriever, this section demonstrates the detailed steps of querying the retriever to obtain the top- nearest neighbor knowledge in algorithm 2, including encoding the query (line 1), performing the approximate nearest neighbor search (line 2), and fetching the knowledge for fusion (line 3).\\nThese three steps depend on the specific APIs of encoders, indexing, and datastore.\\nAfter obtaining the top- retrievals, optimizations for post-processing are applied (line 4). [...] ## 2. Overview of Retrieval-Augmented Generation\", \"score\": 0.6793707}, {\"title\": \"Making Retrieval-Augmented Language Models Robust to ...\", \"url\": \"https://openreview.net/forum?id=ZS4m74kZpH\", \"content\": \"Making Retrieval-Augmented Language Models Robust to Irrelevant Context | OpenReview\\n\\nToggle navigationOpenReview.net\\n\\n   Login\\n\\n×\\n\\nImage 1: back arrowGo to ICLR 2024 Conferencehomepage\\n\\nMaking Retrieval-Augmented Language Models Robust to Irrelevant Context\\n\\nImage 2: Download PDF\\n\\n### Ori Yoran, Tomer Wolfson, Ori Ram, Jonathan Berant\\n\\nPublished: 16 Jan 2024, Last Modified: 07 Mar 2024 ICLR 2024 poster EveryoneRevisionsBibTeX\", \"score\": 0.6334335}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"LlamaIndex Newsletter 2024-07-16\", \"url\": \"https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-07-16\", \"content\": \"1.   We have launched the beta release of LlamaCloud, a data processing layer that enhances RAG workflows with advanced parsing, indexing, and retrieval capabilities. Blogpost, Tweet.\\n2.   We have launched an implementation[beta] of GraphRAG concepts with LlamaIndex focussing on graph generation, building communities and community summaries, and community-based retrieval to aggregate answers from summaries. Notebook, Tweet. [...] Welcome to this week’s edition of the LlamaIndex newsletter! We’re thrilled to share some exciting updates about our products, the implementation of GraphRAG, demos that have achieved over $1M in ARR, extensive guides, in-depth tutorials, and hackathons. [...] GraphRAG Implementation: Implementation of GraphRAG with LlamaIndex, focusing on graph generation, community building, summaries, and community-based retrieval to improve answer aggregation. Notebook, Tweet.\\n   Redis Queue Integration with Llama-Agents: We have integrated Redis Queue with llama-agents to boost coordination and communication in multi-agent workflows, ensuring robust performance. Notebook, Tweet.\", \"score\": 0.8812988}, {\"title\": \"Blog — LlamaIndex - Build Knowledge Assistants over your ...\", \"url\": \"https://www.llamaindex.ai/blog\", \"content\": \"Jan 30, 2024\\n Agentic RAG With LlamaIndex\\n\\n  Jan 30, 2024\\n LlamaIndex Newsletter 2024–01–30\\n\\n  Jan 30, 2024\\n Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex\\n\\n  Jan 26, 2024\\n Introducing the LlamaIndex retrieval-augmented generation command-line tool\\n\\n  Jan 26, 2024\\n Building Scalable RAG Applications with LlamaIndex and Zilliz Cloud Pipelines\\n\\n  Jan 25, 2024\\n Building a Slack bot that learns with LlamaIndex, Qdrant and Render [...] Dec 3, 2024\\n LlamaIndex Newsletter 2024-11-26\\n\\n  Nov 26, 2024\\n Case study: Streamlined Research Paper Analysis with LlamaParse at Arcee AI\\n\\n  Nov 25, 2024\\n Constructing a Knowledge Graph with LlamaIndex and Memgraph\\n\\n  Nov 21, 2024\\n Announcing the LlamaIndex integration with Azure AI\\n\\n  Nov 19, 2024\\n LlamaIndex Newsletter 2024-11-19\\n\\n  Nov 19, 2024\\n Automatically generating cloud configurations: Introducing RAGformation [...] Mar 18, 2024\\n LlamaIndex Accelerates Enterprise Generative AI with NVIDIA NIM\\n\\n  Mar 18, 2024\\n PII Detector: hacking privacy in RAG\\n\\n  Mar 13, 2024\\n Launching the first GenAI-native document parsing platform\\n\\n  Mar 13, 2024\\n LlamaIndex Newsletter 2024-03-12\\n\\n  Mar 12, 2024\\n LlamaIndex Newsletter 2024-03-05\\n\\n  Mar 5, 2024\\n Towards Long Context RAG\\n\\n  Mar 1, 2024\\n Unlocking the 3rd Dimension for Generative AI (Part 1)\\n\\n  Feb 29, 2024\\n Querying a network of knowledge with llama-index-networks\", \"score\": 0.8332299}, {\"title\": \"LlamaIndex Newsletter 2024-08-20\", \"url\": \"https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-08-20\", \"content\": \"Welcome to this week's edition of the LlamaIndex newsletter! We're excited to bring you updates including the Multimodal Report Generation Guide, a revamped Multi-Agent Concierge Workflow, robust Box integration for efficient data management, and innovative Event-Driven RAG Templates. Check out these developments along with our comprehensive guides and tutorials to maximize your use of these new features. [...] Event-Driven RAG Templates: Use our event-driven workflows to implement techniques from key RAG papers—LongRAG, CorrectiveRAG, Self-Discover RAG—with added visualization and debugging, available as templates or for custom development. Tweet.\\n   Box Integration in LlamaIndex: New Box Readers integrated into LlamaIndex workflows facilitate efficient data extraction and authentication for enhanced AI applications. Blogpost, Tweet. [...] 1.   Event-Driven Templates for Advanced RAG + Agent Techniques of three key RAG and agent papers - LongRAG, CorrectiveRAG, Self-Discover RAG, using our event-driven workflows, complete with visualization and debug features, available as templates or for custom development. Tweet.\", \"score\": 0.81770587}, {\"title\": \"LlamaIndex RAG: Build Efficient GraphRAG Systems - FalkorDB\", \"url\": \"https://www.falkordb.com/blog/llamaindex-rag-implementation-graphrag/\", \"content\": \"Skip to content\\n\\nFalkorDB Header Menu\\n\\nBack to all posts\\n\\n# LlamaIndex RAG: Build Efficient GraphRAG Systems\\n\\n Roi Lipman\\n Date Published:\\n Date Updated: November 19, 2024 [...] In this article, you’ll learn how to leverage LlamaIndex with FalkorDB to build an efficient RAG system. LlamaIndex is a versatile framework for developing LLM-powered applications, making it easy for you to connect LLMs with private or domain-specific data sources, including knowledge graph databases like FalkorDB. With LlamaIndex, you can seamlessly ingest, structure, and index data from diverse formats, such as PDFs, SQL databases, and APIs. [...] There are two parts to a RAG system: the retrieval module and the generation module. We will use LlamaIndex to orchestrate the two steps. To power our retrieval module, we will use FalkorDB. For the generation, you can use any LLM that has been trained on Cypher queries, which are needed for fetching data from modern graph databases like FalkorDB.\\n\\nGet GraphRAG, CodeGraph and Graph DBMS news, code examples and opinions delivered weekly.\\n\\nNo spam, cancel anytime.\\n\\n#### Setting Up\", \"score\": 0.76427686}, {\"title\": \"Introducing the Property Graph Index: A Powerful New Way to Build ...\", \"url\": \"https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms\", \"content\": \"Register to join the 9/30 webinar on Agentic Document Processing with LlamaCloud!\\n\\nTalk to us\\n\\n© 2025 LlamaIndex\\n\\nLlamaIndex • 2024-05-29\\n\\n# Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs\\n\\n Knowledge Graphs\\n\\nWe're thrilled to announce a new feature in LlamaIndex that expands our knowledge graph capabilities to be more flexible, extendible, and robust. Introducing the Property Graph Index!\\n\\n## Why Property Graphs? [...] 2025-01-15\\n Constructing a Knowledge Graph with LlamaIndex and Memgraph\\n\\n  2024-11-21\\n Customizing property graph index in LlamaIndex\\n\\n  2024-06-11\", \"score\": 0.64585793}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"2024: The Year of RAG (Part 1) - Medium\", \"url\": \"https://medium.com/@yu-joshua/2024-the-year-of-rag-part-1-bdf8a05f818d\", \"content\": \"Conclusion\\n\\nIn 2024, Retrieval-Augmented Generation (RAG) has undergone transformative advancements, solidifying its role as a cornerstone of modern AI. Researchers have pushed the boundaries of RAG, enabling it to handle increasingly complex tasks, scale to massive datasets, and deliver more natural, human-like interactions. [...] 3.   _Harish Tayyar Madabushi. “FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models.” arXiv preprint arXiv:2406.16167 (2024)._\\n4.   _Jiarui Li, Ye Yuan, Zehua Zhang. “Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases.” arXiv preprint arXiv:2403.10446 (2024)._ [...] . A quick search on arXiv reveals a staggering 1,202 papers published in 2024 alone, dwarfing the 93 that appeared in 2023 and the mere 10 in 2022. This explosive growth signals a major shift in the AI community’s priorities and marks 2024 as a breakthrough year for RAG. However, what does this mean to enterprises, and has RAG actually delivered its promises?_\", \"score\": 0.75464433}, {\"title\": \"Evaluating RAG pipelines with the RAG triad - JAVAPRO International\", \"url\": \"https://javapro.io/2025/05/14/evaluating-rag-pipelines-with-the-rag-triad/\", \"content\": \"Exploring Cost-Effective Solutions for Stateful Rest Services\\n\\n A cost-effective approach to implementing stateful REST services using IBM Cloud Code Engine, EclipseStore, and IBM Cloud Object Storage, highlighting their integration and scalability benefits for enterprise applications. \\n\\nImage 41: Johannes Rabauer\\n\\nJohannes Rabauer\\n\\n30. July 2024\\n\\nRead More\\n\\nImage 42: Natalie Johnson\\n\\nNatalie Johnson\\n\\n9. August 2024\\n\\n   API & Frameworks\\n   Project Management\\n\\nNavigating Change at Work [...] When we reference change at work we typically hear statements like, “Change is a constant” or “Change is… \\n\\n4 min\\n\\nImage 43\\n\\n3 min\\n\\n[](\\n\\n   API & Frameworks\\n   Online Courses\\n\\nQuarkus\\n\\n Quarkus is a full-stack, open-source, Kubernetes-native Java framework that enables Java developers to create applications for a modern,… \\n\\nImage 44\\n\\nJAVAPRO\\n\\n4. April 2024\\n\\nRead More\\n\\nSign Up\\n\\nTo Our Weekly Newsletter\\n\\n0\\n\\n0\\n\\n6K\\n\\n0\\n\\n0\\n\\nYour registration could not be saved. Please try again. [...] 1.   How should you parse and chunk text documents for vector embedding? What chunk size and overlay size should you use?\\n2.   What vector embedding model should you use?\\n3.   What retrieval method should I use to fetch the relevant context? How many documents should you retrieve by default? Does the retriever actually manage to retrieve the applicable documents?\", \"score\": 0.7382357}, {\"title\": \"The Rise and Evolution of RAG in 2024 A Year in Review - RAGFlow\", \"url\": \"https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review\", \"content\": \"The following is a summary of the technological developments in RAG throughout 2024 from both industrial and academic perspectives. RAG has been a hot topic in this year's research. Since the beginning of the year, the frequency of preprints on the topic of RAG has reached over ten papers per week, with some weeks seeing as many as several dozen. These papers primarily focus on experiments related to the applications, tuning, and evaluation of RAG, leading to various conclusions. This article [...] PageRank is an effective strategy worth mentioning alongside another influential 2024 paper on knowledge graphs—HippoRAG [Reference 15]. This paper discusses hippocampal indexing theory and a personalised PageRank-based random walk strategy that closely resembles how the human brain thinks based on memory. Thus, after constructing a knowledge graph, querying it using personalised PageRank can simulate human recall and thought processes related to long text information. Both Fast GraphRAG and [...] ## Semantic Gap​\\n\\nIn the first half of 2024, Microsoft published a paper on GraphRAG [Reference 8] and officially open-sourced it mid-year, quickly garnering over ten thousand stars on GitHub. What accounts for GraphRAG's popularity? This relates closely to the third pain point of RAG that we previously mentioned: the semantic gap.\", \"score\": 0.7099254}, {\"title\": \"Retrieval-Augmented Generation to Generate Knowledge ...\", \"url\": \"https://www.mdpi.com/2076-3417/15/11/6247\", \"content\": \"This article explores the current state of the art in RAG technology and examines various approaches to its implementation, with a particular focus on optimising the creation of knowledge assets and developing actionable insights . The main contribution of this research focusses on the implementation of an RAG-based system to address a real-world scenario, based on the Baseline Review 2024 for the Liverpool City Region , as discussed in Section 6, which provides an economic assessment of the [...] The Baseline Review 2024 underscores the importance of structured processes when applying AI to analyse layered, regional, and longitudinal datasets. Key recommendations include: [...] including naïve and advanced models, emphasising techniques such as optimised indexing, query refinement, metadata utilisation, and the incorporation of autonomous AI agents in agentic RAG systems. Methodologies for effective data preprocessing, semantic-aware chunking, and retrieval strategies—such as multihop retrieval and reranking—are also discussed to address challenges such as irrelevant retrieval and semantic fragmentation. This work further examines embedding models, notably the use of\", \"score\": 0.6296157}, {\"title\": \"ASTRID - An Automated and Scalable TRIaD for the ...\", \"url\": \"https://aclanthology.org/2025.findings-acl.857.pdf\", \"content\": \"2.2 Current RAG metrics Evaluating RAG systems presents challenges due to their hybrid structure and the overall quality of the output often depends on multiple components within these systems. While attempts have been made to assess the overall quality of responses us-ing deterministic methods (Liu et al., 2023; Lyu et al., 2024), most current evaluation metrics for RAG systems use an ensemble of component-level assessments, the majority of which leverage LLMs as judges (Yu et al., 2024). [...] To address some of these risks specific to clinical QA systems using RAG, various efforts have been made to develop performance benchmarks. Cur-rently, published benchmarks often utilize multiple-choice or categorical ground-truth answers for re-sponses (Xiong et al., 2024; Li et al., 2024; Wu et al., 2024; Nori et al., 2023), which fail to capture the complexities and risks associated with open-ended response generations. Where open-ended answers are evaluated, n-gram-based metrics such as [...] 3.2.3 Context Relevance (CR) For clinical QA systems built on RAG, using the right context when generating responses is essen-tial. This is typically achieved by creating embed-dings of the query and the knowledge source, which are then passed through a retriever (Lewis et al., 2020; Ding et al., 2024). The retrieval component of a RAG system takes in the encoded query and re-trieves the top matches from the knowledge source index, which is then passed to the LLM agent as context (Salemi and\", \"score\": 0.62687886}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Evaluation of Retrieval-Augmented Generation: A Survey - arXiv\", \"url\": \"https://arxiv.org/html/2405.07437v2\", \"content\": \"The targets of Retrieval and Generation components are introduced. Table 1 ‣ 3 A Unified Evaluation Process of RAG (Auepora) ‣ Evaluation of Retrieval-Augmented Generation: A Survey\\\") lists the relative work on improving and evaluating RAG and its benchmarks cut off in June 2024. Table 1 ‣ 3 A Unified Evaluation Process of RAG (Auepora) ‣ Evaluation of Retrieval-Augmented Generation: A Survey\\\") portrays this information, where each evaluation criterion is represented by a different colour. For [...] ## References\\n\\n \\n\\n  Balaguer, A., Benara, V., Cunha, R.L.d.F., Filho, R.d.M.E., Hendry, T., Holstein, D., Marsman, J., Mecklenburg, N., Malvar, S., Nunes, L.O., Padilha, R., Sharp, M., Silva, B., Sharma, S., Aski, V., Chandra, R.: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. Tech. rep. (Jan 2024), , arXiv:2401.08406 [cs] type: article [...] Blagojevic, V.: Enhancing RAG Pipelines in Haystack: Introducing DiversityRanker and LostInTheMiddleRanker (Aug 2023), \\n \\n\\n  Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., et al.: A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology 15(3), 1–45 (2024)\\n \\n\\n  Chen, J., Lin, H., Han, X., Sun, L.: Benchmarking large language models in retrieval-augmented generation (Sep 2023).\", \"score\": 0.8708723}, {\"title\": \"A Survey on Knowledge-Oriented Retrieval-Augmented Generation\", \"url\": \"https://arxiv.org/html/2503.10677v2\", \"content\": \"context-aware responses. In addition to concentrating on research pertaining to RAG technology, recent scholarship has increasingly directed its focus towards the assessment of RAG systems. A survey by Yu et al.(Yu et al., 2024a) addresses the evaluation challenges of RAG systems, providing an integrated evaluation framework and examining existing benchmarks along with their limitations. Despite covering multiple aspects of RAG, existing surveys still lack a comprehensive review that includes [...] and memory components. Section 6: RAG Evaluation(Saad-Falcon et al., 2024; Es et al., 2023; Petroni et al., 2021) focuses on the benchmarks and datasets commonly used to assess RAG systems, offering a critical review of existing evaluation practices and metrics. Section 7: RAG Applications highlights key use cases and real-world applications where RAG has demonstrated improvements, followed by Section 8: Prospects and Future Directions, which discusses emerging trends and potential areas for [...] offers a fundamental overview of RAG and discusses specific applications within the realm of text generation tasks. Recent surveys by Gao et al. and Fan et al. (Gao et al., 2024; Fan et al., 2024; Huang and Huang, 2024) explore RAG in the context of large language models, with a specific emphasis on query-oriented RAG enhancement methods. Wu et al. (Wu et al., 2024) delves into key technologies of RAG in the areas of retrieval, while simultaneously introducing its broad applications in natural\", \"score\": 0.86080295}, {\"title\": \"Evaluation of Retrieval-Augmented Generation: A Survey - arXiv\", \"url\": \"https://arxiv.org/abs/2405.07437\", \"content\": \"We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n Submitted on 13 May 2024 ([v1), last revised 3 Jul 2024 (this version, v2)]\\n\\nTitle:Evaluation of Retrieval-Augmented Generation: A Survey\\n\\nAuthors:Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu\\n\\nView a PDF of the paper titled Evaluation of Retrieval-Augmented Generation: A Survey, by Hao Yu and 5 other authors [...] Subjects:Computation and Language (cs.CL); Artificial Intelligence (cs.AI)\\nCite as:arXiv:2405.07437 [cs.CL]\\n(or arXiv:2405.07437v2 [cs.CL] for this version)\\n\\nFocus to learn more\\n\\n arXiv-issued DOI via DataCite\\nRelated DOI:\\n\\nFocus to learn more\\n\\n DOI(s) linking to related resources\\n\\nSubmission history\\n\\n From: Hao Yu [view email] \\n\\n( Mon, 13 May 2024 02:33:25 UTC (795 KB)\\n\\n[v2] Wed, 3 Jul 2024 04:59:32 UTC (437 KB)\\n\\n\\n   TeX Source\\n   Other Formats\\n\\nImage 7: license iconview license [...] > Abstract:Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to\", \"score\": 0.86052185}, {\"title\": \"Evaluation of Retrieval-Augmented Generation: A Survey - arXiv\", \"url\": \"https://arxiv.org/html/2405.07437v1\", \"content\": \"The targets of Retrieval and Generation components are introduced. Table 1 ‣ 3 RGAR: Analysis Framework for Evaluation ‣ Evaluation of Retrieval-Augmented Generation: A Survey\\\") lists the relative work about improving and evaluating RAG and RAG benchmarks cut off in March 2024. Table 1 ‣ 3 RGAR: Analysis Framework for Evaluation ‣ Evaluation of Retrieval-Augmented Generation: A Survey\\\") portrays this information, where each evaluation criterion is represented by a different colour. For example, [...] ## References\\n\\n \\n\\n  Balaguer, A., Benara, V., Cunha, R.L.d.F., Filho, R.d.M.E., Hendry, T., Holstein, D., Marsman, J., Mecklenburg, N., Malvar, S., Nunes, L.O., Padilha, R., Sharp, M., Silva, B., Sharma, S., Aski, V., Chandra, R.: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. Tech. rep. (Jan 2024), , arXiv:2401.08406 [cs] type: article [...] Blagojevic, V.: Enhancing RAG Pipelines in Haystack: Introducing DiversityRanker and LostInTheMiddleRanker (Aug 2023), \\n \\n\\n  Chen, J., Lin, H., Han, X., Sun, L.: Benchmarking large language models in retrieval-augmented generation (Sep 2023). \\n \\n\\n  Cuconasu, F., Trappolini, G., Siciliano, F., Filice, S., Campagnano, C., Maarek, Y., Tonellotto, N., Silvestri, F.: The power of noise: Redefining retrieval for rag systems (Jan 2024).\", \"score\": 0.8415267}, {\"title\": \"Retrieval-Augmented Generation: A Comprehensive Survey ... - arXiv\", \"url\": \"https://arxiv.org/html/2506.00054v1\", \"content\": \"Adversarial Robustness and Security: Emerging work also highlights new vulnerabilities. BadRAG (Xue et al., 2024) and TrojanRAG (Cheng et al., 2024a) demonstrate that adversarially poisoned passages can serve as semantic backdoors, triggering specific behaviors in LLM outputs even when base models remain unmodified. These attacks rely on stealthy corpus manipulations that are hard to detect and pose significant threats in open-domain or API-exposed RAG systems. [...] generation, minimizing reliance on generative interpolation and reducing domain-specific hallucination. RAG-Ex (Sudhi et al., 2024) simulates retrieval variability by injecting perturbed documents during training, improving robustness to inconsistent or adversarial context. In high-stakes domains such as healthcare, Confidence-Calibrated RAG (Ozaki et al., 2025) explores how document ordering and prompt design affect both answer accuracy and model certainty. [...] Hallucination-Aware Decoding Constraints: To mitigate factual inaccuracies in generation, several systems introduce decoding-time constraints or architectures designed to enforce grounding. RAGTruth (Niu et al., 2024) provides benchmarks and evaluation protocols for hallucination detection, guiding system-level design. Structured retrieval-based approaches have also been explored: one method (Hofstätter et al., 2023) retrieves executable templates (e.g., JSON workflows) to constrain output\", \"score\": 0.77179235}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"OpenMatch/ActiveRAG: This is the code repo for our paper ...\", \"url\": \"https://github.com/OpenMatch/ActiveRAG\", \"content\": \"4️⃣ Run the following script:\\n\\n5️⃣ Analyzing logs:\\n\\n6️⃣ Evaluate:\\n\\n## Citation\\n\\n`@article{xu2024activerag,\\ntitle={ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents},\\nauthor={Xu, Zhipeng and Liu, Zhenghao and Yan, Yukun and Wang, Shuo and Yu, Shi and Zeng, Zheni and Xiao, Chaojun and Liu, Zhiyuan and Yu, Ge and Xiong, Chenyan},\\njournal={arXiv preprint arXiv:2402.13547},\\nyear={2024}\\n}`\\n\\n## Contact Us [...] ## Latest commit\\n\\n## History\\n\\n## Repository files navigation\\n\\n# ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents\\n\\nSource code for our paper :  \\nActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents\\n\\nWe are currently optimizing the code and have uploaded the original version.\\n\\n## Quick Start\\n\\n1️⃣ Install from git\\n\\n2️⃣ Install the necessary packages\\n\\n3️⃣ Set your own api here. [...] If you have questions, suggestions, and bug reports, please send a email to us, we will try our best to help you.\\n\\n## About\\n\\nThis is the code repo for our paper \\\"Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents\\\".\\n\\n### Resources\\n\\n### License\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n### Stars\\n\\n### Watchers\\n\\n### Forks\\n\\n## Releases\\n\\n## Packages 0\\n\\n### Uh oh!\", \"score\": 0.63777614}, {\"title\": \"Revealing the Treasures of Knowledge via Active Learning - arXiv\", \"url\": \"https://arxiv.org/html/2402.13547v1\", \"content\": \"## 6 Conclusion\\n\\nThis paper proposes ActiveRAG, which constructs the retrieval-augmentation architecture incorporating active knowledge learning. Inspired by the Constructivism theory, ActiveRAG builds the knowledge construction and cognitive nexus mechanisms to facilitate the integration of retrieved knowledge with the intrinsic cognition of LLMs.\\n\\n## Limitation [...] | ActiveRAG | w. Anchoring | 60.2 | 92.4 | 63.6 | 59.8 | 61.6 | 92.2 | 64.4 | 59.0 |\\n| w. Logician | 60.0 | 91.0 | 67.0 | 56.0 | 63.0 | 91.2 | 68.6 | 58.2 |\\n| w. Cognition | 60.6 | 91.8 | 67.2 | 59.2 | 60.6 | 92.2 | 68.4 | 59.6 |\\n| w. Associate | 62.2 | 93.0 | 68.4 | 62.4 | 63.8 | 93.0 | 70.8 | 61.4 |\\n| Oracle Rerank | 69.4 | 95.2 | 75.0 | 66.6 | 70.0 | 95.2 | 79.0 | 67.6 | [...] Compared to all baseline models, ActiveRAG surpasses them all with over 5% improvements, particularly excelling on the WebQ dataset, demonstrating its effectiveness. Notably, ActiveRAG consistently achieves significant improvements over the ChatGPT-3.5 model across all datasets, indicating the capacity of our method to guide LLMs in uncovering valuable knowledge from retrieved passages through active learning.\", \"score\": 0.5513086}, {\"title\": \"Better RAG with Active Retrieval Augmented Generation ...\", \"url\": \"https://medium.com/etoai/better-rag-with-active-retrieval-augmented-generation-flare-3b66646e2a9f\", \"content\": \"## Advanced RAG: Extracting Complex PDFs containing tables & Text Using LlamaParse\\n\\n### In this blog, we’ll compare LangChain and LlamaIndex for better extraction of PDF data, especially those containing tables and text. Here’s…\\n\\nMay 25, 2024\\n\\n155\\n\\n2\\n\\nSee all from Akash A Desai\\n\\nSee all from LanceDB\\n\\n## Recommended from Medium\\n\\nIn\\n\\nDev Genius\\n\\nby\\n\\nNishad Ahamed\\n\\n## RAG vs Agentic RAG: A Comprehensive Comparison\\n\\n### Introduction\\n\\nMar 6\\n\\n4\\n\\nIn\\n\\nArtificial Intelligence in Plain English\\n\\nby [...] Kaushal Choudhary\\n\\n## 🎫 Enhance RAG: Integrate Contextual Compression and Filtering for Precision\\n\\nJan 9, 2024\\n\\n87\\n\\n2\\n\\nIn\\n\\nLanceDB\\n\\nby\\n\\nPrashant Dixit\\n\\n## Inverted File Product Quantization (IVF\\\\_PQ): Accelerate vector search by creating indices\\n\\n### Vector similarity search is finding similar vectors from a list of given vectors in a particular embedding space. It plays a vital role in…\\n\\nDec 17, 2023\\n\\n44\\n\\nAkash A Desai [...] Explore the full potential of this cutting-edge technology by visiting the vector-recipes repository. It’s filled with real-world examples, use cases, and recipes to inspire your next project. We hope you found this journey both informative and inspiring. Cheers!\\n\\n## Published in LanceDB\\n\\n178 followers\\n\\n·Last published Apr 3, 2024\\n\\nServerless, low-latency vector database for AI applications\\n\\n## Written by Akash A Desai\\n\\n185 followers\\n\\n·14 following\", \"score\": 0.4993414}, {\"title\": \"Retrieval Augmented Generation Research: 2017-2024\", \"url\": \"https://scalingknowledge.substack.com/p/rag\", \"content\": \"RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al. 2020):\\n\\nThis paper, mentioned in the introduction of this blog post, is the origin of the idea of a dynamic, end-to-end trained RAG system backpropagating into both the retriever and the generator. However, the document encoder step in this and the next paper is still static.\\n\\nRETRO: Improving language models by retrieving from trillions of tokens (Borgead et al. 2022): [...] Retrieving Texts based on Abstract Descriptions (Ravfogel et al. 2023)\\n Retrieve to Explain: Evidence-driven Predictions with Language Models (Patel et al. 2024) ⭐\\n RAG-Fusion: a New Take on Retrieval-Augmented Generation (Rackauchas 2024)\\n Similarity is Not All You Need: Endowing Retrieval-Augmented Generation with Multi–layered Thoughts (Gan et al. 2024) ⭐\\n\\nTransformer memory [...] GRAG: Graph Retrieval-Augmented Generation (Hu et al. 2024):\\n\\nUnlike RAG approaches that focus solely on text-based entity retrieval, GRAG maintains an acute awareness of the graph topology, which helps generate contextually and factually coherent responses. The researchers claim that GRAG significantly outperforms current SOTA RAG methods while effectively mitigating hallucinations.\", \"score\": 0.44709876}, {\"title\": \"ActiveRAG — Active Learning\", \"url\": \"https://medium.com/@bijit211987/activerag-active-learning-dada35f62527\", \"content\": \"--\\n\\n--\\n\\n1\\n\\nBijit Ghosh\\nBijit Ghosh\\n\\n## Written by Bijit Ghosh\\n\\nCTO | Senior Engineering Leader focused on Cloud Native | AI/ML | DevSecOps\\n\\n## Responses (1)\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nRules\\n\\nTerms\\n\\nText to speech [...] With its active approach, ActiveRAG demonstrates that simply retrieving knowledge is not enough. Constructing, reconciling, and actively integrating knowledge within an LLM’s intrinsic cognition leads to more accurate and coherent outcomes. While work remains to reach human-level comprehension, ActiveRAG provides a valuable step toward unlocking the treasures hidden within external knowledge resources.\\n\\n### Conclusion [...] There are still frontiers to explore in emulating human constructivist learning within neural networks. But ActiveRAG provides a learning blueprint moving forward — one that treats knowledge not as an external asset held statically, but as a living force to be constructed, shaped, and woven into a system of understanding. With this active spirit, the treasures locked within humankind’s knowledge become not just more accessible, but a source of enrichment that touches the essence of cognition\", \"score\": 0.44376975}]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-5 in organization org-CvhbXKiCglUO2AailKiueQpr on tokens per min (TPM): Limit 30000, Requested 41616. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# researcher_out = researcher_agent.invoke({\"messages\": [(\"user\", plan_res.steps[0].task)]})\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresearcher_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan_res\u001b[49m\u001b[43m.\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langgraph/pregel/main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langgraph/pregel/_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:394\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    396\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langgraph/prebuilt/chat_agent_executor.py:627\u001b[39m, in \u001b[36mcreate_react_agent.<locals>.call_model\u001b[39m\u001b[34m(state, runtime, config)\u001b[39m\n\u001b[32m    625\u001b[39m     response = cast(AIMessage, dynamic_model.invoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     response = cast(AIMessage, \u001b[43mstatic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[32m    630\u001b[39m response.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langchain_core/runnables/base.py:3082\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3080\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3081\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3082\u001b[39m                 input_ = context.run(step.invoke, input_, config)\n\u001b[32m   3083\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langchain_core/runnables/base.py:5495\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5488\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5489\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5490\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5493\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5494\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5495\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5496\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5497\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5498\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:393\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m     **kwargs: Any,\n\u001b[32m    389\u001b[39m ) -> BaseMessage:\n\u001b[32m    390\u001b[39m     config = ensure_config(config)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    403\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1019\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1016\u001b[39m     **kwargs: Any,\n\u001b[32m   1017\u001b[39m ) -> LLMResult:\n\u001b[32m   1018\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:837\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    836\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    843\u001b[39m         )\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    845\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1085\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1083\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1089\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1183\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1182\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1185\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1186\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1187\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1188\u001b[39m ):\n\u001b[32m   1189\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1178\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1172\u001b[39m             response,\n\u001b[32m   1173\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1174\u001b[39m             metadata=generation_info,\n\u001b[32m   1175\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1176\u001b[39m         )\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1179\u001b[39m         response = raw_response.parse()\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Request too large for gpt-5 in organization org-CvhbXKiCglUO2AailKiueQpr on tokens per min (TPM): Limit 30000, Requested 41616. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "During task with name 'agent' and id '66da63ee-370a-24f9-1a49-82b18905a117'"
     ]
    }
   ],
   "source": [
    "\n",
    "# This dictionary IS your blackboard for now.\n",
    "research_state = {\n",
    "    \"topic\": \"Latest techniques in LLM quantization\",\n",
    "    \"facts\": [], # Starts empty\n",
    "}\n",
    "\n",
    "# researcher_out = researcher_agent.invoke({\"messages\": [(\"user\", plan_res.steps[0].task)]})\n",
    "for event in researcher_agent.stream({\"messages\": [(\"user\", plan_res.steps[0].task)]}):\n",
    "    update = event.get(\"agent\", event.get(\"tools\", {}))\n",
    "    for msg in update.get(\"messages\", []):\n",
    "        msg.pretty_print()        # nice formatting of AI/Human/Tool messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b650d351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pregel.aget_state of <langgraph.graph.state.CompiledStateGraph object at 0x7f03fc0d2e90>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "researcher_agent.aget_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c9dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 submit_research payload (pretty):\n",
      "\n",
      "{\n",
      "  \"topic\": \"RAG (Retrieval-Augmented Generation): 2023–2025 sources (papers, blogs, library releases, talks)\",\n",
      "  \"facts\": [\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557529Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557550Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557554Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557558Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557562Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557566Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557569Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557573Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557576Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557580Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557584Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557587Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557590Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557593Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557599Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557604Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557608Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557613Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557617Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"scholar_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557620Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"general_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557624Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"general_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557627Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"general_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557630Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"general_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557632Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"general_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557635Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": null,\n",
      "      \"kind\": \"general_reference\",\n",
      "      \"created_at\": \"2025-09-03T13:15:10.557638Z\",\n",
      "      \"scholar_references\": null,\n",
      "      \"general_references\": null,\n",
      "      \"confidence\": null\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data = json.loads(msg.content)          # you returned model_dump_json()\n",
    "    print(\"\\n📦 submit_research payload (pretty):\\n\")\n",
    "    print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "except json.JSONDecodeError:\n",
    "    # Fallback if something odd happens\n",
    "    print(\"\\n⚠️ Could not parse submit_research JSON; raw content:\\n\")\n",
    "    print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f03de3",
   "metadata": {},
   "source": [
    "#### How to inspect outputs of react"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dde956",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = researcher_agent.invoke({\"messages\": [(\"user\", \"Latest on multimodal RAG?\")]})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in researcher_agent.stream({\"messages\": [(\"user\", \"Latest on multimodal RAG?\")]}):\n",
    "    # Each event is a *delta* keyed by node name, e.g. \"agent\" or \"tools\"\n",
    "    update = event.get(\"agent\", event.get(\"tools\", {}))\n",
    "    for msg in update.get(\"messages\", []):\n",
    "        msg.pretty_print()        # nice formatting of AI/Human/Tool messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e6a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing agent's tool choice ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'tool_calls'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m response = researcher_agent.invoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat is the latest on multimodal RAG?\u001b[39m\u001b[33m\"\u001b[39m)]})\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# The response AIMessage should contain tool_calls\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_calls\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'tool_calls'"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "print(\"--- Testing agent's tool choice ---\")\n",
    "# Invoke the agent with a sample message\n",
    "response = researcher_agent.invoke({\"messages\": [HumanMessage(content=\"What is the latest on multimodal RAG?\")]})\n",
    "\n",
    "# The response AIMessage should contain tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24c992c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the latest on multimodal RAG?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv_search (call_i8nSHa1CNgOUqypeBL3XWbHw)\n",
      " Call ID: call_i8nSHa1CNgOUqypeBL3XWbHw\n",
      "  Args:\n",
      "    query: multimodal RAG\n",
      "  arxiv_search (call_LvxxsYffKBGiym2o0voJFkvi)\n",
      " Call ID: call_LvxxsYffKBGiym2o0voJFkvi\n",
      "  Args:\n",
      "    query: multimodal retrieval-augmented generation\n",
      "  arxiv_search (call_Rx7GNzde8MeCRYzSJPfl6TD5)\n",
      " Call ID: call_Rx7GNzde8MeCRYzSJPfl6TD5\n",
      "  Args:\n",
      "    query: RAG images multimodal\n",
      "  arxiv_search (call_xRz8DTj9CKS55VZuzQCYJTTC)\n",
      " Call ID: call_xRz8DTj9CKS55VZuzQCYJTTC\n",
      "  Args:\n",
      "    query: MMRAG multimodal\n",
      "  tavily_search_results_json (call_7JUKbygl4N5JzATPmDVEkxdZ)\n",
      " Call ID: call_7JUKbygl4N5JzATPmDVEkxdZ\n",
      "  Args:\n",
      "    query: multimodal retrieval augmented generation latest\n",
      "  tavily_search_results_json (call_5HoiDs0C4lHfwXOMDnESaaIR)\n",
      " Call ID: call_5HoiDs0C4lHfwXOMDnESaaIR\n",
      "  Args:\n",
      "    query: RAG multimodal survey 2024 2025\n",
      "  tavily_search_results_json (call_zsPA0hSUQT9lOVvosQW14AHz)\n",
      " Call ID: call_zsPA0hSUQT9lOVvosQW14AHz\n",
      "  Args:\n",
      "    query: multimodal knowledge retrieval generation\n",
      "  wikipedia_search (call_F3razIYZfXbNGAkKlI9xNjvX)\n",
      " Call ID: call_F3razIYZfXbNGAkKlI9xNjvX\n",
      "  Args:\n",
      "    query: Multimodal retrieval augmented generation\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2025-06-10\n",
      "Title: Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment\n",
      "Authors: Tianyu Chen, Jian Lou, Wenjie Wang\n",
      "Summary: As Retrieval-Augmented Generation (RAG) evolves into service-oriented\n",
      "platforms (Rag-as-a-Service) with shared knowledge bases, protecting the\n",
      "copyright of contributed data becomes essential. Existing watermarking methods\n",
      "in RAG focus solely on textual knowledge, leaving image knowledge unprotected.\n",
      "In this work, we propose AQUA, the first watermark framework for image\n",
      "knowledge protection in Multimodal RAG systems. AQUA embeds semantic signals\n",
      "into synthetic images using two complementary methods: acronym-based triggers\n",
      "and spatial relationship cues. These techniques ensure watermark signals\n",
      "survive indirect watermark propagation from image retriever to textual\n",
      "generator, being efficient, effective and imperceptible. Experiments across\n",
      "diverse models and datasets show that AQUA enables robust, stealthy, and\n",
      "reliable copyright tracing, filling a key gap in multimodal RAG protection.\n",
      "\n",
      "Published: 2024-10-29\n",
      "Title: Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications\n",
      "Authors: Monica Riedler, Stefan Langer\n",
      "Summary: Large Language Models (LLMs) have demonstrated impressive capabilities in\n",
      "answering questions, but they lack domain-specific knowledge and are prone to\n",
      "hallucinations. Retrieval Augmented Generation (RAG) is one approach to address\n",
      "these challenges, while multimodal models are emerging as promising AI\n",
      "assistants for processing both text and images. In this paper we describe a\n",
      "series of experiments aimed at determining how to best integrate multimodal\n",
      "models into RAG systems for the industrial domain. The purpose of the\n",
      "experiments is to determine whether including images alongside text from\n",
      "documents within the industrial domain increases RAG performance and to find\n",
      "the optimal configuration for such a multimodal RAG system. Our experiments\n",
      "include two approaches for image processing and retrieval, as well as two LLMs\n",
      "(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies\n",
      "involve the use of multimodal embeddings and the generation of textual\n",
      "summaries from images. We evaluate our experiments with an LLM-as-a-Judge\n",
      "approach. Our results reveal that multimodal RAG can outperform single-modality\n",
      "RAG settings, although image retrieval poses a greater challenge than text\n",
      "retrieval. Additionally, leveraging textual summaries from images presents a\n",
      "more promising approach compared to the use of multimodal embeddings, providing\n",
      "more opportunities for future advancements.\n",
      "\n",
      "Published: 2025-08-07\n",
      "Title: mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering\n",
      "Authors: Xu Yuan, Liangbo Ning, Wenqi Fan, Qing Li\n",
      "Summary: Recently, Retrieval-Augmented Generation (RAG) has been proposed to expand\n",
      "internal knowledge of Multimodal Large Language Models (MLLMs) by incorporating\n",
      "external knowledge databases into the generation process, which is widely used\n",
      "for knowledge-based Visual Question Answering (VQA) tasks. Despite impressive\n",
      "advancements, vanilla RAG-based VQA methods that rely on unstructured documents\n",
      "and overlook the structural relationships among knowledge elements frequently\n",
      "introduce irrelevant or misleading content, reducing answer accuracy and\n",
      "reliability. To overcome these challenges, a promising solution is to integrate\n",
      "multimodal knowledge graphs (KGs) into RAG-based VQA frameworks to enhance the\n",
      "generation by introducing structured multimodal knowledge. Therefore, in this\n",
      "paper, we propose a novel multimodal knowledge-augmented generation framework\n",
      "(mKG-RAG) based on multimodal KGs for knowledge-intensive VQA tasks.\n",
      "Specifically, our approach leverages MLLM-powered keyword extraction and\n",
      "vision-text matching to distill semantically consistent and modality-aligned\n",
      "entities/relationships from multimodal documents, constructing high-quality\n",
      "multimodal KGs as structured knowledge representatio\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2025-08-07\n",
      "Title: Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation\n",
      "Authors: Haonan Shangguan, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Ge Yu\n",
      "Summary: The surge in rich multimodal content on social media platforms has greatly\n",
      "advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)\n",
      "further accelerating progress in this field. Current approaches primarily\n",
      "leverage the knowledge and reasoning capabilities of parameter-heavy\n",
      "(Multimodal) LLMs for sentiment classification, overlooking autonomous\n",
      "multimodal sentiment reasoning generation in resource-constrained environments.\n",
      "Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment\n",
      "Reasoning and Classification task, JMSRC, which simultaneously performs\n",
      "multimodal sentiment reasoning chain generation and sentiment classification\n",
      "only with a lightweight model. We propose a Multimodal Chain-of-Thought\n",
      "Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a\n",
      "\"Teacher-Assistant-Student\" distillation paradigm to address deployment\n",
      "constraints in resource-limited environments. We first leverage a\n",
      "high-performance Multimodal Large Language Model (MLLM) to generate the initial\n",
      "reasoning dataset and train a medium-sized assistant model with a multi-task\n",
      "learning mechanism. A lightweight student model is jointly trained to perform\n",
      "efficient multimodal sentiment reasoning generation and classification.\n",
      "Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B\n",
      "parameters achieves strong performance on JMSRC, while exhibiting robust\n",
      "generalization and enhanced interpretability.\n",
      "\n",
      "Published: 2024-12-29\n",
      "Title: Diff4MMLiTS: Advanced Multimodal Liver Tumor Segmentation via Diffusion-Based Image Synthesis and Alignment\n",
      "Authors: Shiyun Chen, Li Lin, Pujin Cheng, ZhiCheng Jin, JianJian Chen, HaiDong Zhu, Kenneth K. Y. Wong, Xiaoying Tang\n",
      "Summary: Multimodal learning has been demonstrated to enhance performance across\n",
      "various clinical tasks, owing to the diverse perspectives offered by different\n",
      "modalities of data. However, existing multimodal segmentation methods rely on\n",
      "well-registered multimodal data, which is unrealistic for real-world clinical\n",
      "images, particularly for indistinct and diffuse regions such as liver tumors.\n",
      "In this paper, we introduce Diff4MMLiTS, a four-stage multimodal liver tumor\n",
      "segmentation pipeline: pre-registration of the target organs in multimodal CTs;\n",
      "dilation of the annotated modality's mask and followed by its use in inpainting\n",
      "to obtain multimodal normal CTs without tumors; synthesis of strictly aligned\n",
      "multimodal CTs with tumors using the latent diffusion model based on multimodal\n",
      "CT features and randomly generated tumor masks; and finally, training the\n",
      "segmentation model, thus eliminating the need for strictly aligned multimodal\n",
      "data. Extensive experiments on public and internal datasets demonstrate the\n",
      "superiority of Diff4MMLiTS over other state-of-the-art multimodal segmentation\n",
      "methods.\n",
      "\n",
      "Published: 2025-08-08\n",
      "Title: M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation\n",
      "Authors: Zhiyou Xiao, Qinhan Yu, Binghui Li, Geng Chen, Chong Chen, Wentao Zhang\n",
      "Summary: Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables\n",
      "diverse multimodal inputs but remains limited to single-modality outputs,\n",
      "restricting expressive capacity and practical utility. In contrast, real-world\n",
      "applications often demand both multimodal inputs and multimodal outputs for\n",
      "effective communication and grounded reasoning. Motivated by the recent success\n",
      "of Reinforcement Learning (RL) in complex reasoning tasks for Large Language\n",
      "Models (LLMs), we adopt RL as a principled and effective paradigm to address\n",
      "the multi-step, outcome-driven challenges inherent in multimodal output\n",
      "generation. Here, we introduce M2IO-R1, a novel framework for Multimodal\n",
      "Retrieval-Augmented\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2025-06-10\n",
      "Title: Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment\n",
      "Authors: Tianyu Chen, Jian Lou, Wenjie Wang\n",
      "Summary: As Retrieval-Augmented Generation (RAG) evolves into service-oriented\n",
      "platforms (Rag-as-a-Service) with shared knowledge bases, protecting the\n",
      "copyright of contributed data becomes essential. Existing watermarking methods\n",
      "in RAG focus solely on textual knowledge, leaving image knowledge unprotected.\n",
      "In this work, we propose AQUA, the first watermark framework for image\n",
      "knowledge protection in Multimodal RAG systems. AQUA embeds semantic signals\n",
      "into synthetic images using two complementary methods: acronym-based triggers\n",
      "and spatial relationship cues. These techniques ensure watermark signals\n",
      "survive indirect watermark propagation from image retriever to textual\n",
      "generator, being efficient, effective and imperceptible. Experiments across\n",
      "diverse models and datasets show that AQUA enables robust, stealthy, and\n",
      "reliable copyright tracing, filling a key gap in multimodal RAG protection.\n",
      "\n",
      "Published: 2024-10-29\n",
      "Title: Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications\n",
      "Authors: Monica Riedler, Stefan Langer\n",
      "Summary: Large Language Models (LLMs) have demonstrated impressive capabilities in\n",
      "answering questions, but they lack domain-specific knowledge and are prone to\n",
      "hallucinations. Retrieval Augmented Generation (RAG) is one approach to address\n",
      "these challenges, while multimodal models are emerging as promising AI\n",
      "assistants for processing both text and images. In this paper we describe a\n",
      "series of experiments aimed at determining how to best integrate multimodal\n",
      "models into RAG systems for the industrial domain. The purpose of the\n",
      "experiments is to determine whether including images alongside text from\n",
      "documents within the industrial domain increases RAG performance and to find\n",
      "the optimal configuration for such a multimodal RAG system. Our experiments\n",
      "include two approaches for image processing and retrieval, as well as two LLMs\n",
      "(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies\n",
      "involve the use of multimodal embeddings and the generation of textual\n",
      "summaries from images. We evaluate our experiments with an LLM-as-a-Judge\n",
      "approach. Our results reveal that multimodal RAG can outperform single-modality\n",
      "RAG settings, although image retrieval poses a greater challenge than text\n",
      "retrieval. Additionally, leveraging textual summaries from images presents a\n",
      "more promising approach compared to the use of multimodal embeddings, providing\n",
      "more opportunities for future advancements.\n",
      "\n",
      "Published: 2024-10-15\n",
      "Title: Self-adaptive Multimodal Retrieval-Augmented Generation\n",
      "Authors: Wenjia Zhai\n",
      "Summary: Traditional Retrieval-Augmented Generation (RAG) methods are limited by their\n",
      "reliance on a fixed number of retrieved documents, often resulting in\n",
      "incomplete or noisy information that undermines task performance. Although\n",
      "recent adaptive approaches alleviated these problems, their application in\n",
      "intricate and real-world multimodal tasks remains limited. To address these, we\n",
      "propose a new approach called Self-adaptive Multimodal Retrieval-Augmented\n",
      "Generation (SAM-RAG), tailored specifically for multimodal contexts. SAM-RAG\n",
      "not only dynamically filters relevant documents based on the input query,\n",
      "including image captions when needed, but also verifies the quality of both the\n",
      "retrieved documents and the output. Extensive experimental results show that\n",
      "SAM-RAG surpasses existing state-of-the-art methods in both retrieval accuracy\n",
      "and response generation. By further ablation experiments and effectiveness\n",
      "analysis, SAM-RAG maintains high recall quality while improving overall task\n",
      "performance in multimodal RAG task. Our codes are available at\n",
      "https://github.com/SAM-RAG/SAM_RAG.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv_search\n",
      "\n",
      "Published: 2025-06-04\n",
      "Title: CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG\n",
      "Authors: Yang Tian, Fan Liu, Jingyuan Zhang, Victoria W., Yupeng Hu, Liqiang Nie\n",
      "Summary: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\n",
      "enhance Multimodal Large Language Models by incorporating externally retrieved\n",
      "multimodal knowledge, but it introduces two challenges: Parametric-Retrieved\n",
      "Knowledge Inconsistency (PRKI), where discrepancies between parametric and\n",
      "retrieved knowledge create uncertainty in determining reliability, and\n",
      "Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between\n",
      "visual and textual sources disrupts entity representation. To address these\n",
      "challenges, we propose Cross-source knowledge \\textbf{Re}conciliation for\n",
      "Multimodal RAG (CoRe-MMRAG), a novel end-to-end framework that effectively\n",
      "reconciles inconsistencies across knowledge sources. CoRe-MMRAG follows a\n",
      "four-stage pipeline: it first generates an internal response from parametric\n",
      "knowledge, then selects the most relevant multimodal evidence via joint\n",
      "similarity assessment, generates an external response, and finally integrates\n",
      "both to produce a reliable answer. Additionally, a specialized training\n",
      "paradigm enhances knowledge source discrimination, multimodal integration, and\n",
      "unified answer generation. Experiments on KB-VQA benchmarks show that\n",
      "CoRe-MMRAG achieves substantial improvements over baseline methods, achieving\n",
      "5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA, respectively.\n",
      "\n",
      "Published: 2024-10-03\n",
      "Title: Synthetic Multimodal Question Generation\n",
      "Authors: Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosenberg, Sina Pakazad, Tongshuang Wu, Graham Neubig\n",
      "Summary: Multimodal Retrieval Augmented Generation (MMRAG) is a powerful approach to\n",
      "question-answering over multimodal documents. A key challenge with evaluating\n",
      "MMRAG is the paucity of high-quality datasets matching the question styles and\n",
      "modalities of interest. In light of this, we propose SMMQG, a synthetic data\n",
      "generation framework. SMMQG leverages interplay between a retriever, large\n",
      "language model (LLM) and large multimodal model (LMM) to generate question and\n",
      "answer pairs directly from multimodal documents, with the questions conforming\n",
      "to specified styles and modalities. We use SMMQG to generate an MMRAG dataset\n",
      "of 1024 questions over Wikipedia documents and evaluate state-of-the-art models\n",
      "using it, revealing insights into model performance that are attainable only\n",
      "through style- and modality-specific evaluation data. Next, we measure the\n",
      "quality of data produced by SMMQG via a human study. We find that the quality\n",
      "of SMMQG-generated synthetic data is on par with the quality of the\n",
      "crowdsourced benchmark MMQA and that downstream evaluation results using both\n",
      "datasets strongly concur.\n",
      "\n",
      "Published: 2025-02-21\n",
      "Title: MMRAG: Multi-Mode Retrieval-Augmented Generation with Large Language Models for Biomedical In-Context Learning\n",
      "Authors: Zaifu Zhan, Jun Wang, Shuang Zhou, Jiawen Deng, Rui Zhang\n",
      "Summary: Objective: To optimize in-context learning in biomedical natural language\n",
      "processing by improving example selection. Methods: We introduce a novel\n",
      "multi-mode retrieval-augmented generation (MMRAG) framework, which integrates\n",
      "four retrieval strategies: (1) Random Mode, selecting examples arbitrarily; (2)\n",
      "Top Mode, retrieving the most relevant examples based on similarity; (3)\n",
      "Diversity Mode, ensuring variation in selected examples; and (4) Class Mode,\n",
      "selecting category-representative examples. This study evaluates MMRAG on three\n",
      "core biomedical NLP tasks: Named Entity Recognition (NER), Relation Extraction\n",
      "(RE), and Text Classification (TC). The datasets used include BC2GM for gene\n",
      "and protein mention recognition (NER), DDI for drug-drug interaction extraction\n",
      "(RE), GIT for general biomedical information extraction (RE), and HealthAdvice\n",
      "for health-related text classification (TC). The framework is tested with two\n",
      "large language mo\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"The future of multimodal RAG systems: transforming AI's capabilities\", \"url\": \"https://superlinear.eu/insights/articles/the-future-of-multimodal-rag-systems-transforming-ai-capabilities\", \"content\": \"# The future of multimodal RAG systems: transforming AIâs capabilities\\n\\n# The future of multimodal RAG systems: transforming AIâs capabilities\\n\\nLast updated on:\\n\\n30 Apr 2025\\n\\nPublished on:\\n\\n21 Feb 2025\\n\\nExplore the next evolution of Retrieval-Augmented Generation (RAG), where AI goes beyond text to integrate images, video, and audio. Multimodal RAG unlocks richer, more precise insights, but merging diverse data comes with challenges. [...] Explore the next evolution of Retrieval-Augmented Generation (RAG), where AI goes beyond text to integrate images, video, and audio. Multimodal RAG unlocks richer, more precise insights, but merging diverse data comes with challenges.\\n\\nExplore the next evolution of Retrieval-Augmented Generation (RAG), where AI goes beyond text to integrate images, video, and audio. Multimodal RAG unlocks richer, more precise insights, but merging diverse data comes with challenges. [...] Multimodal Retrieval-Augmented Generation (RAG) is transforming how AI systems handle complex queries, leveraging diverse data types to provide richer, contextually accurate responses. While text-grounding dominates today, advancements in multimodal embedding could soon simplify this process, enabling more seamless integration of non-text data.\", \"score\": 0.77589405}, {\"title\": \"Trends in Active Retrieval Augmented Generation: 2025 and Beyond\", \"url\": \"https://www.signitysolutions.com/blog/trends-in-active-retrieval-augmented-generation\", \"content\": \"Beyond text-based AI models, retrieval-augmented generation has a bright future. Multimodal RAG will include a variety of data formats, such as audio, video, and image, into AI-powered systems. By adding picture retrieval, speech-to-text conversion, and video analysis to their retrieval pipelines, AI assistants, chatbots, and content production models will be able to offer more comprehensive responses. [...] At Signity, we help businesses stay ahead in the evolving landscape of Retrieval-Augmented Generation (RAG). Our expertise in real-time retrieval, multimodal RAG, and hybrid retrieval techniques ensures that your AI solutions are contextually aware and industry-specific.\\n\\n### Maximize Business Efficiency With Active RAG\\n\\nExplore how AI-driven retrieval can enhance productivity, accuracy, and customer experience. [...] #### Careers\\n\\nJoin a forward-thinking team that is building the future of software solutions.\\n\\n# Trends in Active Retrieval Augmented Generation: 2025 and Beyond\\n\\nRAG is advancing AI with real-time retrieval, hybrid search, and multimodal capabilities. Trends like personalized RAG, on-device AI, and scalable solutions will impact industries. This blog explores RAG’s future and its business potential in 2025 and beyond.\\n\\nTrends in Active Retrieval Augmented Generation\", \"score\": 0.76888984}, {\"title\": \"Multi-RAG: A Multimodal Retrieval-Augmented Generation System ...\", \"url\": \"https://arxiv.org/html/2505.23990v1\", \"content\": \"To fill this critical need, we present Multi-RAG, a multimodal retrieval-augmented generation (RAG) system designed to provide adaptive assistance to humans in information-intensive circumstances. Our system aims to improve situational understanding and reduce cognitive load by integrating and reasoning over multi-source information streams, including video, audio, and text. As an enabling step toward long-term human-robot partnerships, Multi-RAG explores how multimodal information [...] As robots increasingly integrate into the fabric of human society, enabling them to learn continually from human interaction is both a technical imperative and a societal opportunity. Our Multi-RAG system demonstrates how multi-modal retrieval-augmented generation can serve as a scalable, adaptive foundation for robotic systems that support human cognition in dynamic, information-rich environments. By grounding perception, audio, and image in real-time, human-centered inputs, this work bridges [...] However, despite their growing popularity and capabilities, LLMs still face notable limitations in technical reliability and practical applicability. One major constraint is hallucinations, as LLMs are known to occasionally generate plausible-sounding but incorrect information . Additionally, LLMs often lack the latest information because their training corpus is typically outdated by several months or years . Therefore, we propose a human-centered system that helps the agent understand the\", \"score\": 0.72207695}, {\"title\": \"A Survey on Multimodal Retrieval-Augmented Generation\", \"url\": \"https://github.com/llm-lab-org/Multimodal-RAG-Survey\", \"content\": \"# llm-lab-org/Multimodal-RAG-Survey\\n\\n## Folders and files\\n\\n| Name | | Name | Last commit message | Last commit date |\\n ---  --- \\n| Latest commit   History133 Commits | | |\\n| .gitignore | | .gitignore |  |  |\\n| Multimodal\\\\_RAG\\\\_ACL2025.pdf | | Multimodal\\\\_RAG\\\\_ACL2025.pdf |  |  |\\n| Multimodal\\\\_RAG\\\\_Slides.pdf | | Multimodal\\\\_RAG\\\\_Slides.pdf |  |  |\\n| README.md | | README.md |  |  |\\n| View all files | | |\\n\\n## Latest commit\\n\\n## History\\n\\n## Repository files navigation [...] `@misc{abootorabi2025askmodalitycomprehensivesurvey,\\ntitle={Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation},\\nauthor={Mohammad Mahdi Abootorabi and Amirhosein Zobeiri and Mahdi Dehghani and Mohammadali Mohammadkhani and Bardia Mohammadi and Omid Ghahroodi and Mahdieh Soleymani Baghshah and Ehsaneddin Asgari},\\nyear={2025},\\neprint={2502.08826},\\narchivePrefix={arXiv},\\nprimaryClass={cs.CL},\\nurl={\\n}`\\n\\n## 📧 Contact [...] This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, benchmarks, metrics, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, loss functions, and agent-based approaches, while also exploring the diverse Multimodal RAG scenarios. In addition, we outline open challenges and future directions to guide research in this evolving field. This\", \"score\": 0.7140107}, {\"title\": \"Multimodal Retrieval-Augmented Generation: Advances and ...\", \"url\": \"https://denemlabs.blog/2025/04/16/multimodal-retrieval-augmented-generation-advances-and-remaining-challenges/\", \"content\": \"Multimodal Retrieval-Augmented Generation (RAG) enhances the reasoning capabilities of language models by retrieving and incorporating heterogeneous data sources—such as images, diagrams, and structured data—into the generative process. This review surveys recent advances in the field, focusing on applications in healthcare and education. It also identifies key technological bottlenecks, particularly in scenarios requiring the interpretation of graphical information like clinical decision trees [...] ### 2.3 Retrieval Infrastructure and Hybrid Search\\n\\nSystems like LlamaIndex and Haystack now support hybrid retrieval pipelines that combine dense vector search with symbolic constraints, allowing more precise access to visual documents like charts, decision trees, or multi-panel illustrations (Miller et al., 2023). Such infrastructure is critical for real-time applications in high-stakes domains like clinical diagnostics or adaptive learning platforms. [...] Recent models such as CLIP-RAG and FaRL leverage shared embedding spaces for text and image modalities, allowing systems to retrieve relevant non-textual content in response to textual queries. This is particularly impactful in education, where learners may query concepts and receive diagrams or annotated visuals to support their understanding (Smith and Jones, 2024).\", \"score\": 0.7040996}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"RAG at the Crossroads - Mid-2025 Reflections on AI's Incremental ...\", \"url\": \"https://ragflow.io/blog/rag-at-the-crossroads-mid-2025-reflections-on-ai-evolution\", \"content\": \"Throughout our events from late 2024 to early 2025, we consistently highlighted three key features for RAG in 2025: reasoning, memory, and multimodality. The first two are inherently linked to Agents. In our initial blog this year, we offered a comprehensive overview of implementation of reasoning. A recent survey [Ref 3] further synthesizes reasoning and RAG, and we have adapted and condensed its framework as shown below: [...] Turning to the second aspect: multimodal data. In our year-end review, we highlighted Multimodal RAG (MM-RAG) as a key trend for 2025. Yet, by mid-year, this trend has failed to gain momentum. The primary obstacle remains the immaturity of the supporting infrastructure. As noted, late interaction models continue to dominate MM-RAG pipelines, meaning embedding models produce Tensors, or multi-vectors. For instance, a single image may be represented by 1,024 vectors, each comprising [...] In summary, our analysis shows that core RAG technology saw little significant progress in 2025. Meanwhile, the interdependence between RAG and Agents has deepened considerably—whether as the foundation of Agent Memory or enabling DeepResearch capabilities. From an Agent’s perspective, RAG may be just one Tool among many, but by managing unstructured data and Memory, it stands as one of the most fundamental and critical Tools. It is fair to say that without robust RAG, practical enterprise\", \"score\": 0.8763069}, {\"title\": \"Ask in Any Modality A Comprehensive Survey on ...\", \"url\": \"https://aclanthology.org/2025.findings-acl.861.pdf\", \"content\": \"Agentic Generation and Interaction Agent-driven multimodal RAG uses versatile autonomous/semi-autonomous systems across diverse interaction paradigms and specialized domains, often generat-ing complex outputs. For user interaction, AppAgent v2 (Li et al., 2024c) enables mobile GUI naviga-tion while USER-LLM R1 (Rahimi et al., 2025) creates personalized conversational agents via dy-namic profiling, particularly for elderly users. In specialized applications, MMAD (Jiang et al., 2025) addresses [...] Source Attribution and Evidence Transparency Ensuring source attribution in multimodal RAG sys-tems is a significant research focus. OMG-QA (Nan et al., 2024b) prompts LLMs for explicit evidence citation in generated responses. MuRAR (Zhu et al., 2025) refines an LLM’s initial response by integrat-ing multimodal information from a source-based retriever to improve informativeness. However, its recall is constrained, as the retriever may miss evi-dence spanning different sections or web [...] In parallel, P2PCAP (Changin et al., 2024) improves retrieval precision by regenerating captions as dy-namic queries. Further innovations address error correction and efficiency. LA-RAG (Li et al., 2024b) utilizes fine-grained speech-to-speech retrieval and forced alignment to enhance ASR accuracy through LLM in-context learning. Meanwhile, hybrid sys-tems, such as Xiao et al. (2025), integrate LLMs to correct errors in noisy environments using retrieved text/audio context.\", \"score\": 0.8058924}, {\"title\": \"The Rise and Evolution of RAG in 2024 A Year in Review\", \"url\": \"https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review\", \"content\": \"In the upcoming year of 2025, research based on Encoder-Decoder architectures is expected to advance further. We can anticipate the potential development of a unified multi-modal document parsing model capable of accurately converting various unstructured documents into text content. [...] promote multimodal RAG. It is anticipated that with more models becoming available, tensor-based reranking will see widespread application in 2025. [...] Whether for multimodal or textual data, the results of chunking significantly impact final outcomes. In 2025, we can expect more high-quality work in this area that will ultimately resolve issues related to data entry quality.\\n\\n## Hybrid Search​\", \"score\": 0.7598145}, {\"title\": \"[2504.08748] A Survey of Multimodal Retrieval-Augmented Generation\", \"url\": \"https://arxiv.org/abs/2504.08748\", \"content\": \"Published Time: Tue, 15 Apr 2025 00:00:26 GMT\\n\\n[2504.08748] A Survey of Multimodal Retrieval-Augmented Generation\\n\\nSkip to main content\\n\\nImage 1: Cornell University Logo\\n\\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\\n\\n \\n\\n [Submitted on 26 Mar 2025]\\n\\nTitle:A Survey of Multimodal Retrieval-Augmented Generation\\n\\nAuthors:Lang Mei, Siyu Mo, Zhihan Yang, Chong Chen\", \"score\": 0.7570233}, {\"title\": \"Multimodal RAG Survey\", \"url\": \"https://multimodalrag.github.io/\", \"content\": \"Asgari, Ehsaneddin\\\",\\n                editor = \\\"Che, Wanxiang  and\\n                  Nabende, Joyce  and\\n                  Shutova, Ekaterina  and\\n                  Pilehvar, Mohammad Taher\\\",\\n                booktitle = \\\"Findings of the Association for Computational Linguistics: ACL 2025\\\",\\n                month = jul,\\n                year = \\\"2025\\\",\\n                address = \\\"Vienna, Austria\\\",\\n                publisher = \\\"Association for Computational Linguistics\\\", [...] url = \\\"\\n                doi = \\\"10.18653/v1/2025.findings-acl.861\\\",\\n                pages = \\\"16776--16809\\\",\\n                ISBN = \\\"979-8-89176-256-5\\\", [...] ```\\n@inproceedings{abootorabi-etal-2025-ask,\\n                title = \\\"Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation\\\",\\n                author = \\\"Abootorabi, Mohammad Mahdi  and\\n                  Zobeiri, Amirhosein  and\\n                  Dehghani, Mahdi  and\\n                  Mohammadkhani, Mohammadali  and\\n                  Mohammadi, Bardia  and\\n                  Ghahroodi, Omid  and\\n                  Baghshah, Mahdieh Soleymani  and\", \"score\": 0.7065353}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"title\": \"Advanced Embedding Techniques in Multimodal Retrieval ...\", \"url\": \"https://drpress.org/ojs/index.php/jceim/article/view/24094\", \"content\": \"Our study presents significant advancements in the handling of multimodal data through an extended Retrieval-Augmented Generation (RAG) model. By integrating advanced embedding techniques, efficient retrieval mechanisms, and robust generative capabilities, our model demonstrates notable improvements in retrieval accuracy, real-time efficiency, generative quality, and scalability. The retrieval accuracy of our model reached 85%, showing a 10% improvement over existing benchmarks. Furthermore, [...] the retrieval time was reduced by 40%, enhancing real-time application performance. The model's generative quality was also significantly improved, with BLEU and ROUGE scores increasing by 15% and 12%, respectively. These results validate the effectiveness of our approach and its applicability to various AI applications, including information retrieval, recommendation systems, and content creation. Future research directions include the integration of additional modalities and further [...] Salemi, A., & Zamani, H. (2024, July). Evaluating Retrieval Quality in Retrieval-Augmented Generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 2395-2400).\\n\\nQiu, L., & Liu, M. (2024). Innovative Design of Cultural Souvenirs Based on Deep Learning and CAD.\\n\\nZhang, Y., Zhang, C., Tang, Y., & He, Z. (2024). Cross-modal concept learning and inference for vision-language models. Neurocomputing, 583, 127530.\", \"score\": 0.82560414}, {\"title\": \"Enhancing foundation models for scientific discovery via ...\", \"url\": \"https://www.sciencedirect.com/science/article/pii/S1570826824000313\", \"content\": \"We discuss the opportunities to advance life science discovery by incorporating domain knowledge into generative multimodal models to enhance predictive capabilities, enable conditional generation, and support knowledge-enhanced Retrieval-Augmented Generation (RAG) with multimodal retrieval. Such capabilities are especially valuable for discovery tasks where single-modal methods fall short. [...] Conditional multimodal generation. A unified objective function for training GNNs typically includes tasks such as link prediction, classification and regression (e.g., for numerical attributes like toxicity). However, MKGs encompass a diverse range of data attributes beyond numerical ones, such as textual properties and images. This diversity may require implementing multimodal imputation models and generative heads that can accommodate varied modalities, extending beyond assigning predefined [...] ### 3.1. Proposed end-to-end discovery workflow\\n\\nAn end-to-end Multimodal Knowledge Discovery workflow is illustrated in Fig. 1. We discuss open challenges from data ingestion and knowledge extraction to learning multimodal, knowledge-enhanced representations, ultimately improving downstream tasks and supporting evidence-based hypothesis generation.\", \"score\": 0.81770587}, {\"title\": \"Multimodal Knowledge Extraction and Retrieval System for ...\", \"url\": \"https://techcommunity.microsoft.com/blog/educatordeveloperblog/enhancing-retrieval-augmented-generation-with-a-multimodal-knowledge-extraction-/4241375\", \"content\": \"The rapid evolution of AI has led to powerful tools for knowledge retrieval and question-answering systems, particularly with the rise of Retrieval-Augmented Generation (RAG) systems. This blog post introduces my capstone project, created as part of the IXN program at UCL in collaboration with Microsoft, aimed at enhancing RAG systems by integrating multimodal knowledge extraction and retrieval capabilities. The system enables AI agents to process both textual and visual data, offering more [...] Retrieval-Augmented Generation with a Multimodal Knowledge Extraction and Retrieval [...] ## Implementation Details\\n\\n#### Multimodal Knowledge Extraction\\n\\nThe system starts by fetching both textual and visual data from Wikimedia, using the Wikimedia API and web scraping techniques. Then the key steps in knowledge extraction implementation are:\\n\\nData Preprocessing: Text is cleaned, images are classified into categories such as plots or images for appropriate handling during later transformations, and tables are structured for easier processing.\", \"score\": 0.7811474}, {\"title\": \"Generative Multi-Modal Knowledge Retrieval with Large Language ...\", \"url\": \"https://arxiv.org/abs/2401.08206\", \"content\": \"# Code, Data and Media Associated with this Article\\n\\n# Demos\\n\\n# Recommenders and Search Tools\\n\\n# arXivLabs: experimental projects with community collaborators\\n\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. [...] Cornell University\\narxiv logo\\n\\nHelp | Advanced Search\\n\\narXiv logo\\nCornell University Logo\\n\\n## quick links\\n\\n# Computer Science > Information Retrieval\\n\\n# Title:Generative Multi-Modal Knowledge Retrieval with Large Language Models [...] |  |  |\\n --- |\\n| Comments: | Accepted to AAAI 2024 |\\n| Subjects: | Information Retrieval (cs.IR); Computation and Language (cs.CL) |\\n| Cite as: | arXiv:2401.08206 [cs.IR] |\\n|  | (or  arXiv:2401.08206v1 [cs.IR] for this version) |\\n|  |  Focus to learn more  arXiv-issued DOI via DataCite |\\n\\n## Submission history\\n\\n## Access Paper:\\n\\nlicense icon\\n\\n### References & Citations\\n\\n## BibTeX formatted citation\\n\\n### Bookmark\\n\\nBibSonomy logo\\nReddit logo\\n\\n# Bibliographic and Citation Tools\", \"score\": 0.73505294}, {\"title\": \"Using generative AI to do multimodal information retrieval\", \"url\": \"https://www.amazon.science/blog/using-generative-ai-to-do-multimodal-information-retrieval\", \"content\": \"Unlike prior approaches that are limited to single-modality tasks or specific benchmarks, GENIUS generalizes across retrieval of texts, images, and image-text pairs, maintaining high speed and competitive accuracy. Its advantages over prior generation-based models are based on two key innovations: [...] ### Conference\\n\\n   CVPR 2025\\n\\n### Related publications\\n\\n   GENIUS: A generative framework for universal multimodal search\\n\\nAbout the Author\\n\\n Sungyeon Kim \\n\\n Sungyeon Kim is a postdoctoral researcher at the Computer Vision Lab at the Pohang University of Science and Technology (POSTECH). He was an intern at Amazon when the work was done. \\n\\nXiaofan Lin\\n\\n Xiaofan Lin is a senior applied scientist with Amazon's search organization. \\n\\nRelated content [...] Image 6: GENIUS framework.png\", \"score\": 0.6633424}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: wikipedia_search\n",
      "\n",
      "Page: Large language model\n",
      "Summary: A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\n",
      "The largest and most capable LLMs are generative pretrained transformers (GPTs), based on a transformer architecture, which are largely used in generative chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.\n",
      "\n",
      "\n",
      "\n",
      "Page: Multimodal learning\n",
      "Summary: Multimodal learning is a type of deep learning that integrates and processes multiple types of data, referred to as  modalities, such as text, audio, images, or video. This integration allows for a more holistic understanding of complex data, improving model performance in tasks like visual question answering, cross-modal retrieval, text-to-image generation, aesthetic ranking, and image captioning.\n",
      "Large multimodal models, such as Google Gemini and GPT-4o, have become increasingly popular since 2023, enabling increased versatility and a broader understanding of real-world phenomena.\n",
      "\n",
      "Page: Music and artificial intelligence\n",
      "Summary: Music and artificial intelligence (music and AI) is the development of music software programs which use AI to generate music. As with applications in other fields, AI in music also simulates mental tasks. A prominent feature is the capability of an AI algorithm to learn based on past data, such as in computer accompaniment technology, wherein the AI is capable of listening to a human performer and performing accompaniment. Artificial intelligence also drives interactive composition technology, wherein a computer composes music in response to a live performance. There are other AI applications in music that cover not only music composition, production, and performance but also how music is marketed and consumed. Several music player programs have also been developed to use voice recognition and natural language processing technology for music voice control. Current research includes the application of AI in music composition, performance, theory and digital sound processing. Composers/artists like Jennifer Walshe or Holly Herndon have been exploring aspects of music AI for years in their performances and musical works. Another original approach of humans “imitating AI” can be found in the 43-hour sound installation String Quartet(s) by Georges Lentz (see interview with ChatGPT-4 on music and AI).\n",
      "20th century art historian Erwin Panofsky proposed that in all art, there existed three levels of meaning: primary meaning, or the natural subject; secondary meaning, or the conventional subject; and tertiary meaning, the intrinsic content of the subject. AI music explores the foremost of these, creating music without the \"intention\" which is usually behind it, leaving composers who listen to machine-generated pieces feeling unsettled by the lack of apparent meaning.\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here’s a concise, up-to-date digest of the state of multimodal Retrieval-Augmented Generation (MM-RAG) as of mid-2025.\n",
      "\n",
      "What’s happening now\n",
      "- MM-RAG is maturing from a niche capability into a broader set of architectures and benchmarks that handle text + images (and increasingly audio/video/other modalities) in retrieval-augmented workflows.\n",
      "- The field is being heavily fueled by comprehensive surveys, standardized benchmarks, synthetic data for evaluation, and end-to-end systems that address cross-modal evidence and consistency.\n",
      "- Practical issues being tackled include: robust multimodal fusion, reliable cross-modal retrieval, alignment between parametric and retrieved content, evaluation protocols, and safety/copyright concerns.\n",
      "\n",
      "Notable recent papers and resources (mid-2025)\n",
      "- Surveys and overviews\n",
      "  - A Survey of Multimodal Retrieval-Augmented Generation (Mei, Mo, Yang, et al., 2025) – arXiv:2504.08748\n",
      "  - Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation (Findings of ACL 2025) – notes and survey coverage across modalities and evaluation\n",
      "  - Multimodal RAG Survey (ACL Findings 2025) – consolidated survey with datasets, benchmarks, and methods\n",
      "\n",
      "- System and method innovations\n",
      "  - Multi-RAG (arXiv:2505.23990) – a multimodal retrieval-augmented system for adaptive information in contexts with video, audio, and text\n",
      "  - M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval-Augmented Multimodal Generation (2025) – introduces reinforcement-learning-based control for multimodal outputs\n",
      "  - SAM-RAG: Self-adaptive Multimodal Retrieval-Augmented Generation (2025) – dynamically filters and validates retrieved content and outputs\n",
      "  - CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG (2024) – reconciles inconsistencies between parametric and retrieved knowledge, and between visual/textual sources\n",
      "  - SMMQG: Synthetic Multimodal Question Generation (2025) – a data-generation framework to create evaluation datasets tailored to MM-RAG styles and modalities\n",
      "  - MMRAG for Biomedical In-Context Learning (2025) – domain-specific MM-RAG approaches for biomedicine\n",
      "\n",
      "- Applications, data, and safety\n",
      "  - Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications (2024) – empirical findings on using images alongside text in industry domains\n",
      "  - AQUA: Watermarking for image knowledge in Multimodal RAG (2024) – protecting copyright in MM-RAG pipelines\n",
      "  - The RAG landscape and future directions (industry blogs and opinion pieces, 2024–2025) – syntheses and roadmaps from researchers and practitioners\n",
      "\n",
      "What’s driving the field\n",
      "- Strong emphasis on surveys and benchmarks to reduce duplication and standardize evaluation across modalities.\n",
      "- Emergence of knowledge-grounded multimodal graphs and structured representations (e.g., multimodal knowledge graphs) to improve reasoning and factuality.\n",
      "- Increased attention to data quality and evaluation: synthetic data frameworks (SMMQG) and cross-domain benchmarks (KB-VQA, Encyclopedic-VQA, biomedical tasks).\n",
      "- Safety, provenance, and copyright concerns in MM-RAG workflows (e.g., watermarking, evidence citation in responses).\n",
      "\n",
      "What this means for you (quick take)\n",
      "- If you want a broad, current understanding: start with the two to three survey papers (Mei et al. arXiv 2504.08748; ACL Findings 2025 survey; Multimodal RAG Survey website).\n",
      "- If you want to build or compare systems: look at Multi-RAG (for a full MM-RAG system), SAM-RAG (adaptive retrieval and validation), and M2IO-R1 (RL-based reasoning for multimodal outputs).\n",
      "- If you care about evaluation and datasets: check SMMQG for synthetic MM-RAG data and the biomedical MMRAG paper for domain-specific in-context learning.\n",
      "- If you’re concerned with safety/copyright: read AQUA’s watermarking approach and related discussion on source attribution and evidence transparency (OMG-QA/MuRAR-style ideas are discussed in the surveys).\n",
      "\n",
      "Would you like links or PDFs for any of these papers, or a domain-specific reading list (e.g., healthcare, industry, robotics, education)? I can tailor a short reading plan and extract key takeaways from 2–3 papers to get you started.\n"
     ]
    }
   ],
   "source": [
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "# print(response[\"messages\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceadf3b5",
   "metadata": {},
   "source": [
    "### Create Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f16f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
